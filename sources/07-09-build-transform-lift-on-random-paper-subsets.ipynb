{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run lib.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(edgeitems=10, linewidth=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_df = pd.read_csv('data/text_to_topic_scores.csv', index_col=0)\n",
    "\n",
    "texts_pr = pd.read_csv('texts_preproc_relevant.csv', index_col=0)\n",
    "\n",
    "texts = pd.read_csv('papers_parsed_relevant.csv', index_col=0)\\\n",
    "    .loc[:, ['journal', 'date_parsed']]\\\n",
    "    .reindex(texts_pr.index)\\\n",
    "    .reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1.1.1.1. -- Sample complexity and generalization bounds</th>\n",
       "      <th>1.1.1.2. -- Boolean function learning</th>\n",
       "      <th>1.1.1.3. -- Unsupervised learning and clustering</th>\n",
       "      <th>1.1.1.4.1. -- Support vector machines</th>\n",
       "      <th>1.1.1.4.2. -- Gaussian processes</th>\n",
       "      <th>1.1.1.4.3. -- Modelling</th>\n",
       "      <th>1.1.1.5. -- Boosting</th>\n",
       "      <th>1.1.1.6. -- Bayesian analysis</th>\n",
       "      <th>1.1.1.7. -- Inductive inference</th>\n",
       "      <th>1.1.1.8. -- Online learning theory</th>\n",
       "      <th>...</th>\n",
       "      <th>5.2.4.1.3. -- Policy iteration</th>\n",
       "      <th>5.2.4.1.4. -- Temporal difference learning</th>\n",
       "      <th>5.2.4.1.5. -- Approximate dynamic programming methods</th>\n",
       "      <th>5.2.4.2.1. -- Boosting</th>\n",
       "      <th>5.2.4.2.2. -- Bagging</th>\n",
       "      <th>5.2.4.2.3. -- Fusion of classifiers</th>\n",
       "      <th>5.2.4.3.1 -- Spectral clustering</th>\n",
       "      <th>5.2.4.4. -- Feature selection</th>\n",
       "      <th>5.2.4.5.1 -- Generalized eigenvalue</th>\n",
       "      <th>5.2.5. -- Cross-validation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.146779</td>\n",
       "      <td>0.148864</td>\n",
       "      <td>0.317016</td>\n",
       "      <td>0.136573</td>\n",
       "      <td>0.155241</td>\n",
       "      <td>0.137437</td>\n",
       "      <td>0.160465</td>\n",
       "      <td>0.281274</td>\n",
       "      <td>0.246067</td>\n",
       "      <td>0.158224</td>\n",
       "      <td>...</td>\n",
       "      <td>0.189453</td>\n",
       "      <td>0.199818</td>\n",
       "      <td>0.207490</td>\n",
       "      <td>0.160465</td>\n",
       "      <td>0.178204</td>\n",
       "      <td>0.119531</td>\n",
       "      <td>0.329988</td>\n",
       "      <td>0.319550</td>\n",
       "      <td>0.130482</td>\n",
       "      <td>0.217381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.213833</td>\n",
       "      <td>0.135648</td>\n",
       "      <td>0.308386</td>\n",
       "      <td>0.097235</td>\n",
       "      <td>0.163988</td>\n",
       "      <td>0.111039</td>\n",
       "      <td>0.176288</td>\n",
       "      <td>0.231717</td>\n",
       "      <td>0.216278</td>\n",
       "      <td>0.109494</td>\n",
       "      <td>...</td>\n",
       "      <td>0.218900</td>\n",
       "      <td>0.210582</td>\n",
       "      <td>0.158816</td>\n",
       "      <td>0.176288</td>\n",
       "      <td>0.106835</td>\n",
       "      <td>0.148433</td>\n",
       "      <td>0.318810</td>\n",
       "      <td>0.238902</td>\n",
       "      <td>0.113895</td>\n",
       "      <td>0.153735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.130861</td>\n",
       "      <td>0.212880</td>\n",
       "      <td>0.342364</td>\n",
       "      <td>0.228501</td>\n",
       "      <td>0.131850</td>\n",
       "      <td>0.151590</td>\n",
       "      <td>0.157596</td>\n",
       "      <td>0.081035</td>\n",
       "      <td>0.145126</td>\n",
       "      <td>0.226929</td>\n",
       "      <td>...</td>\n",
       "      <td>0.208125</td>\n",
       "      <td>0.223977</td>\n",
       "      <td>0.127286</td>\n",
       "      <td>0.157596</td>\n",
       "      <td>0.155013</td>\n",
       "      <td>0.269969</td>\n",
       "      <td>0.433328</td>\n",
       "      <td>0.199925</td>\n",
       "      <td>0.142234</td>\n",
       "      <td>0.241214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.120147</td>\n",
       "      <td>0.253489</td>\n",
       "      <td>0.352651</td>\n",
       "      <td>0.149955</td>\n",
       "      <td>0.232027</td>\n",
       "      <td>0.246658</td>\n",
       "      <td>0.161025</td>\n",
       "      <td>0.102643</td>\n",
       "      <td>0.221068</td>\n",
       "      <td>0.288539</td>\n",
       "      <td>...</td>\n",
       "      <td>0.171006</td>\n",
       "      <td>0.247210</td>\n",
       "      <td>0.117086</td>\n",
       "      <td>0.161025</td>\n",
       "      <td>0.101487</td>\n",
       "      <td>0.139135</td>\n",
       "      <td>0.357909</td>\n",
       "      <td>0.198067</td>\n",
       "      <td>0.102722</td>\n",
       "      <td>0.186526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.249352</td>\n",
       "      <td>0.133184</td>\n",
       "      <td>0.307590</td>\n",
       "      <td>0.101416</td>\n",
       "      <td>0.140609</td>\n",
       "      <td>0.099105</td>\n",
       "      <td>0.104585</td>\n",
       "      <td>0.148708</td>\n",
       "      <td>0.282816</td>\n",
       "      <td>0.104701</td>\n",
       "      <td>...</td>\n",
       "      <td>0.176652</td>\n",
       "      <td>0.323895</td>\n",
       "      <td>0.161467</td>\n",
       "      <td>0.104585</td>\n",
       "      <td>0.110777</td>\n",
       "      <td>0.119197</td>\n",
       "      <td>0.403607</td>\n",
       "      <td>0.133150</td>\n",
       "      <td>0.226799</td>\n",
       "      <td>0.203539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.201564</td>\n",
       "      <td>0.227974</td>\n",
       "      <td>0.317781</td>\n",
       "      <td>0.208838</td>\n",
       "      <td>0.115182</td>\n",
       "      <td>0.236364</td>\n",
       "      <td>0.123501</td>\n",
       "      <td>0.087960</td>\n",
       "      <td>0.198322</td>\n",
       "      <td>0.236425</td>\n",
       "      <td>...</td>\n",
       "      <td>0.182536</td>\n",
       "      <td>0.223695</td>\n",
       "      <td>0.160219</td>\n",
       "      <td>0.123501</td>\n",
       "      <td>0.088216</td>\n",
       "      <td>0.163189</td>\n",
       "      <td>0.325475</td>\n",
       "      <td>0.228125</td>\n",
       "      <td>0.101625</td>\n",
       "      <td>0.179478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.177392</td>\n",
       "      <td>0.223245</td>\n",
       "      <td>0.611876</td>\n",
       "      <td>0.151325</td>\n",
       "      <td>0.138115</td>\n",
       "      <td>0.114294</td>\n",
       "      <td>0.165034</td>\n",
       "      <td>0.117744</td>\n",
       "      <td>0.157261</td>\n",
       "      <td>0.228703</td>\n",
       "      <td>...</td>\n",
       "      <td>0.203409</td>\n",
       "      <td>0.350850</td>\n",
       "      <td>0.235056</td>\n",
       "      <td>0.165034</td>\n",
       "      <td>0.168134</td>\n",
       "      <td>0.135912</td>\n",
       "      <td>0.407228</td>\n",
       "      <td>0.138567</td>\n",
       "      <td>0.122096</td>\n",
       "      <td>0.201846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.166423</td>\n",
       "      <td>0.139857</td>\n",
       "      <td>0.381959</td>\n",
       "      <td>0.176999</td>\n",
       "      <td>0.107430</td>\n",
       "      <td>0.117128</td>\n",
       "      <td>0.183612</td>\n",
       "      <td>0.122425</td>\n",
       "      <td>0.245228</td>\n",
       "      <td>0.167854</td>\n",
       "      <td>...</td>\n",
       "      <td>0.161591</td>\n",
       "      <td>0.252487</td>\n",
       "      <td>0.162493</td>\n",
       "      <td>0.183612</td>\n",
       "      <td>0.111283</td>\n",
       "      <td>0.139458</td>\n",
       "      <td>0.451132</td>\n",
       "      <td>0.277890</td>\n",
       "      <td>0.145228</td>\n",
       "      <td>0.142365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.246698</td>\n",
       "      <td>0.250485</td>\n",
       "      <td>0.566290</td>\n",
       "      <td>0.218441</td>\n",
       "      <td>0.134450</td>\n",
       "      <td>0.118486</td>\n",
       "      <td>0.096192</td>\n",
       "      <td>0.256354</td>\n",
       "      <td>0.206387</td>\n",
       "      <td>0.369564</td>\n",
       "      <td>...</td>\n",
       "      <td>0.178881</td>\n",
       "      <td>0.322083</td>\n",
       "      <td>0.165166</td>\n",
       "      <td>0.096192</td>\n",
       "      <td>0.085929</td>\n",
       "      <td>0.268450</td>\n",
       "      <td>0.303687</td>\n",
       "      <td>0.322109</td>\n",
       "      <td>0.287601</td>\n",
       "      <td>0.245244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.192792</td>\n",
       "      <td>0.153208</td>\n",
       "      <td>0.415652</td>\n",
       "      <td>0.111682</td>\n",
       "      <td>0.206287</td>\n",
       "      <td>0.107320</td>\n",
       "      <td>0.187159</td>\n",
       "      <td>0.109417</td>\n",
       "      <td>0.204001</td>\n",
       "      <td>0.116916</td>\n",
       "      <td>...</td>\n",
       "      <td>0.198469</td>\n",
       "      <td>0.238937</td>\n",
       "      <td>0.211841</td>\n",
       "      <td>0.187159</td>\n",
       "      <td>0.164249</td>\n",
       "      <td>0.286287</td>\n",
       "      <td>0.417164</td>\n",
       "      <td>0.331842</td>\n",
       "      <td>0.220369</td>\n",
       "      <td>0.227139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.135986</td>\n",
       "      <td>0.105450</td>\n",
       "      <td>0.226756</td>\n",
       "      <td>0.369367</td>\n",
       "      <td>0.139522</td>\n",
       "      <td>0.086130</td>\n",
       "      <td>0.168194</td>\n",
       "      <td>0.102082</td>\n",
       "      <td>0.180081</td>\n",
       "      <td>0.102798</td>\n",
       "      <td>...</td>\n",
       "      <td>0.215746</td>\n",
       "      <td>0.152046</td>\n",
       "      <td>0.133940</td>\n",
       "      <td>0.168194</td>\n",
       "      <td>0.086762</td>\n",
       "      <td>0.163617</td>\n",
       "      <td>0.346939</td>\n",
       "      <td>0.244318</td>\n",
       "      <td>0.140443</td>\n",
       "      <td>0.224745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.168721</td>\n",
       "      <td>0.174705</td>\n",
       "      <td>0.263490</td>\n",
       "      <td>0.132206</td>\n",
       "      <td>0.158296</td>\n",
       "      <td>0.274150</td>\n",
       "      <td>0.176654</td>\n",
       "      <td>0.070442</td>\n",
       "      <td>0.166324</td>\n",
       "      <td>0.124020</td>\n",
       "      <td>...</td>\n",
       "      <td>0.216061</td>\n",
       "      <td>0.219450</td>\n",
       "      <td>0.148728</td>\n",
       "      <td>0.176654</td>\n",
       "      <td>0.115428</td>\n",
       "      <td>0.138635</td>\n",
       "      <td>0.329766</td>\n",
       "      <td>0.211878</td>\n",
       "      <td>0.091399</td>\n",
       "      <td>0.197672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.195358</td>\n",
       "      <td>0.233843</td>\n",
       "      <td>0.325127</td>\n",
       "      <td>0.111762</td>\n",
       "      <td>0.094158</td>\n",
       "      <td>0.182654</td>\n",
       "      <td>0.162446</td>\n",
       "      <td>0.119589</td>\n",
       "      <td>0.186321</td>\n",
       "      <td>0.100320</td>\n",
       "      <td>...</td>\n",
       "      <td>0.308007</td>\n",
       "      <td>0.109550</td>\n",
       "      <td>0.201779</td>\n",
       "      <td>0.162446</td>\n",
       "      <td>0.119663</td>\n",
       "      <td>0.157321</td>\n",
       "      <td>0.309173</td>\n",
       "      <td>0.245649</td>\n",
       "      <td>0.131959</td>\n",
       "      <td>0.161398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.163220</td>\n",
       "      <td>0.118858</td>\n",
       "      <td>0.245648</td>\n",
       "      <td>0.097568</td>\n",
       "      <td>0.137857</td>\n",
       "      <td>0.112043</td>\n",
       "      <td>0.125598</td>\n",
       "      <td>0.097123</td>\n",
       "      <td>0.203642</td>\n",
       "      <td>0.108509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.166184</td>\n",
       "      <td>0.177200</td>\n",
       "      <td>0.195906</td>\n",
       "      <td>0.125598</td>\n",
       "      <td>0.095763</td>\n",
       "      <td>0.286757</td>\n",
       "      <td>0.311718</td>\n",
       "      <td>0.343359</td>\n",
       "      <td>0.115858</td>\n",
       "      <td>0.160748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.175418</td>\n",
       "      <td>0.246436</td>\n",
       "      <td>0.525437</td>\n",
       "      <td>0.208777</td>\n",
       "      <td>0.223494</td>\n",
       "      <td>0.135055</td>\n",
       "      <td>0.130399</td>\n",
       "      <td>0.074453</td>\n",
       "      <td>0.204030</td>\n",
       "      <td>0.329863</td>\n",
       "      <td>...</td>\n",
       "      <td>0.176521</td>\n",
       "      <td>0.338880</td>\n",
       "      <td>0.192734</td>\n",
       "      <td>0.130399</td>\n",
       "      <td>0.187886</td>\n",
       "      <td>0.123700</td>\n",
       "      <td>0.361486</td>\n",
       "      <td>0.314665</td>\n",
       "      <td>0.127648</td>\n",
       "      <td>0.300291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.151399</td>\n",
       "      <td>0.122786</td>\n",
       "      <td>0.215473</td>\n",
       "      <td>0.139142</td>\n",
       "      <td>0.136476</td>\n",
       "      <td>0.118576</td>\n",
       "      <td>0.103336</td>\n",
       "      <td>0.129034</td>\n",
       "      <td>0.212740</td>\n",
       "      <td>0.109596</td>\n",
       "      <td>...</td>\n",
       "      <td>0.245126</td>\n",
       "      <td>0.140396</td>\n",
       "      <td>0.137815</td>\n",
       "      <td>0.103336</td>\n",
       "      <td>0.082643</td>\n",
       "      <td>0.112091</td>\n",
       "      <td>0.295773</td>\n",
       "      <td>0.257353</td>\n",
       "      <td>0.109432</td>\n",
       "      <td>0.211686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.191404</td>\n",
       "      <td>0.344913</td>\n",
       "      <td>0.510264</td>\n",
       "      <td>0.193075</td>\n",
       "      <td>0.102839</td>\n",
       "      <td>0.153148</td>\n",
       "      <td>0.153108</td>\n",
       "      <td>0.075301</td>\n",
       "      <td>0.206735</td>\n",
       "      <td>0.296324</td>\n",
       "      <td>...</td>\n",
       "      <td>0.178410</td>\n",
       "      <td>0.241845</td>\n",
       "      <td>0.187689</td>\n",
       "      <td>0.153108</td>\n",
       "      <td>0.176220</td>\n",
       "      <td>0.136876</td>\n",
       "      <td>0.471974</td>\n",
       "      <td>0.270732</td>\n",
       "      <td>0.151336</td>\n",
       "      <td>0.176503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.195649</td>\n",
       "      <td>0.207489</td>\n",
       "      <td>0.311234</td>\n",
       "      <td>0.129356</td>\n",
       "      <td>0.330538</td>\n",
       "      <td>0.134019</td>\n",
       "      <td>0.164722</td>\n",
       "      <td>0.238944</td>\n",
       "      <td>0.213387</td>\n",
       "      <td>0.132623</td>\n",
       "      <td>...</td>\n",
       "      <td>0.290108</td>\n",
       "      <td>0.214655</td>\n",
       "      <td>0.135003</td>\n",
       "      <td>0.164722</td>\n",
       "      <td>0.136314</td>\n",
       "      <td>0.131519</td>\n",
       "      <td>0.331154</td>\n",
       "      <td>0.315660</td>\n",
       "      <td>0.145988</td>\n",
       "      <td>0.165849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.135867</td>\n",
       "      <td>0.153368</td>\n",
       "      <td>0.402756</td>\n",
       "      <td>0.170614</td>\n",
       "      <td>0.207534</td>\n",
       "      <td>0.134563</td>\n",
       "      <td>0.168743</td>\n",
       "      <td>0.111045</td>\n",
       "      <td>0.214334</td>\n",
       "      <td>0.136066</td>\n",
       "      <td>...</td>\n",
       "      <td>0.156906</td>\n",
       "      <td>0.212365</td>\n",
       "      <td>0.136634</td>\n",
       "      <td>0.168743</td>\n",
       "      <td>0.187027</td>\n",
       "      <td>0.153390</td>\n",
       "      <td>0.353548</td>\n",
       "      <td>0.378191</td>\n",
       "      <td>0.085466</td>\n",
       "      <td>0.184077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.122168</td>\n",
       "      <td>0.230001</td>\n",
       "      <td>0.236107</td>\n",
       "      <td>0.150027</td>\n",
       "      <td>0.131785</td>\n",
       "      <td>0.126220</td>\n",
       "      <td>0.129434</td>\n",
       "      <td>0.092483</td>\n",
       "      <td>0.147118</td>\n",
       "      <td>0.119895</td>\n",
       "      <td>...</td>\n",
       "      <td>0.168294</td>\n",
       "      <td>0.117355</td>\n",
       "      <td>0.162667</td>\n",
       "      <td>0.129434</td>\n",
       "      <td>0.099680</td>\n",
       "      <td>0.149951</td>\n",
       "      <td>0.287257</td>\n",
       "      <td>0.178085</td>\n",
       "      <td>0.161595</td>\n",
       "      <td>0.186693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.134351</td>\n",
       "      <td>0.139021</td>\n",
       "      <td>0.169989</td>\n",
       "      <td>0.110179</td>\n",
       "      <td>0.188611</td>\n",
       "      <td>0.144647</td>\n",
       "      <td>0.143418</td>\n",
       "      <td>0.261870</td>\n",
       "      <td>0.236809</td>\n",
       "      <td>0.124528</td>\n",
       "      <td>...</td>\n",
       "      <td>0.232531</td>\n",
       "      <td>0.228731</td>\n",
       "      <td>0.177223</td>\n",
       "      <td>0.143418</td>\n",
       "      <td>0.084541</td>\n",
       "      <td>0.164480</td>\n",
       "      <td>0.247303</td>\n",
       "      <td>0.231142</td>\n",
       "      <td>0.160416</td>\n",
       "      <td>0.184462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.134957</td>\n",
       "      <td>0.151309</td>\n",
       "      <td>0.253615</td>\n",
       "      <td>0.126806</td>\n",
       "      <td>0.125641</td>\n",
       "      <td>0.155071</td>\n",
       "      <td>0.161641</td>\n",
       "      <td>0.235585</td>\n",
       "      <td>0.221897</td>\n",
       "      <td>0.144211</td>\n",
       "      <td>...</td>\n",
       "      <td>0.167089</td>\n",
       "      <td>0.150369</td>\n",
       "      <td>0.228402</td>\n",
       "      <td>0.161641</td>\n",
       "      <td>0.191361</td>\n",
       "      <td>0.253681</td>\n",
       "      <td>0.411708</td>\n",
       "      <td>0.328589</td>\n",
       "      <td>0.128894</td>\n",
       "      <td>0.207635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.213908</td>\n",
       "      <td>0.254911</td>\n",
       "      <td>0.325243</td>\n",
       "      <td>0.190596</td>\n",
       "      <td>0.212038</td>\n",
       "      <td>0.083775</td>\n",
       "      <td>0.107922</td>\n",
       "      <td>0.140835</td>\n",
       "      <td>0.147311</td>\n",
       "      <td>0.112402</td>\n",
       "      <td>...</td>\n",
       "      <td>0.209780</td>\n",
       "      <td>0.168944</td>\n",
       "      <td>0.164101</td>\n",
       "      <td>0.107922</td>\n",
       "      <td>0.131834</td>\n",
       "      <td>0.178038</td>\n",
       "      <td>0.310287</td>\n",
       "      <td>0.155588</td>\n",
       "      <td>0.178428</td>\n",
       "      <td>0.173146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.220279</td>\n",
       "      <td>0.117350</td>\n",
       "      <td>0.214311</td>\n",
       "      <td>0.149279</td>\n",
       "      <td>0.198892</td>\n",
       "      <td>0.188208</td>\n",
       "      <td>0.141191</td>\n",
       "      <td>0.150937</td>\n",
       "      <td>0.115100</td>\n",
       "      <td>0.093102</td>\n",
       "      <td>...</td>\n",
       "      <td>0.204554</td>\n",
       "      <td>0.108339</td>\n",
       "      <td>0.177862</td>\n",
       "      <td>0.141191</td>\n",
       "      <td>0.112971</td>\n",
       "      <td>0.133581</td>\n",
       "      <td>0.380284</td>\n",
       "      <td>0.266844</td>\n",
       "      <td>0.099760</td>\n",
       "      <td>0.183641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.228879</td>\n",
       "      <td>0.259057</td>\n",
       "      <td>0.680968</td>\n",
       "      <td>0.123696</td>\n",
       "      <td>0.187983</td>\n",
       "      <td>0.236526</td>\n",
       "      <td>0.181752</td>\n",
       "      <td>0.080494</td>\n",
       "      <td>0.206372</td>\n",
       "      <td>0.237183</td>\n",
       "      <td>...</td>\n",
       "      <td>0.214547</td>\n",
       "      <td>0.304053</td>\n",
       "      <td>0.111496</td>\n",
       "      <td>0.181752</td>\n",
       "      <td>0.111095</td>\n",
       "      <td>0.117582</td>\n",
       "      <td>0.463031</td>\n",
       "      <td>0.224619</td>\n",
       "      <td>0.210738</td>\n",
       "      <td>0.172524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.193618</td>\n",
       "      <td>0.221824</td>\n",
       "      <td>0.600851</td>\n",
       "      <td>0.115657</td>\n",
       "      <td>0.297298</td>\n",
       "      <td>0.219365</td>\n",
       "      <td>0.117906</td>\n",
       "      <td>0.255824</td>\n",
       "      <td>0.147003</td>\n",
       "      <td>0.227354</td>\n",
       "      <td>...</td>\n",
       "      <td>0.218256</td>\n",
       "      <td>0.303102</td>\n",
       "      <td>0.211689</td>\n",
       "      <td>0.117906</td>\n",
       "      <td>0.116746</td>\n",
       "      <td>0.150339</td>\n",
       "      <td>0.446511</td>\n",
       "      <td>0.189706</td>\n",
       "      <td>0.300093</td>\n",
       "      <td>0.177621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.133686</td>\n",
       "      <td>0.231909</td>\n",
       "      <td>0.471001</td>\n",
       "      <td>0.119695</td>\n",
       "      <td>0.136614</td>\n",
       "      <td>0.111284</td>\n",
       "      <td>0.114040</td>\n",
       "      <td>0.248690</td>\n",
       "      <td>0.265539</td>\n",
       "      <td>0.179151</td>\n",
       "      <td>...</td>\n",
       "      <td>0.197023</td>\n",
       "      <td>0.215691</td>\n",
       "      <td>0.160045</td>\n",
       "      <td>0.114040</td>\n",
       "      <td>0.194472</td>\n",
       "      <td>0.130905</td>\n",
       "      <td>0.388730</td>\n",
       "      <td>0.202541</td>\n",
       "      <td>0.131863</td>\n",
       "      <td>0.220331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.141602</td>\n",
       "      <td>0.134178</td>\n",
       "      <td>0.298836</td>\n",
       "      <td>0.107665</td>\n",
       "      <td>0.267763</td>\n",
       "      <td>0.137925</td>\n",
       "      <td>0.178498</td>\n",
       "      <td>0.111945</td>\n",
       "      <td>0.225159</td>\n",
       "      <td>0.127565</td>\n",
       "      <td>...</td>\n",
       "      <td>0.162192</td>\n",
       "      <td>0.221036</td>\n",
       "      <td>0.106670</td>\n",
       "      <td>0.178498</td>\n",
       "      <td>0.112945</td>\n",
       "      <td>0.175774</td>\n",
       "      <td>0.351796</td>\n",
       "      <td>0.459176</td>\n",
       "      <td>0.098925</td>\n",
       "      <td>0.176254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.247660</td>\n",
       "      <td>0.229422</td>\n",
       "      <td>0.602581</td>\n",
       "      <td>0.088050</td>\n",
       "      <td>0.137708</td>\n",
       "      <td>0.259136</td>\n",
       "      <td>0.198557</td>\n",
       "      <td>0.124551</td>\n",
       "      <td>0.167423</td>\n",
       "      <td>0.230288</td>\n",
       "      <td>...</td>\n",
       "      <td>0.176187</td>\n",
       "      <td>0.217299</td>\n",
       "      <td>0.136309</td>\n",
       "      <td>0.198557</td>\n",
       "      <td>0.121793</td>\n",
       "      <td>0.135931</td>\n",
       "      <td>0.413161</td>\n",
       "      <td>0.163374</td>\n",
       "      <td>0.111685</td>\n",
       "      <td>0.177859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.203364</td>\n",
       "      <td>0.111225</td>\n",
       "      <td>0.248625</td>\n",
       "      <td>0.139806</td>\n",
       "      <td>0.130718</td>\n",
       "      <td>0.109459</td>\n",
       "      <td>0.129832</td>\n",
       "      <td>0.076968</td>\n",
       "      <td>0.256682</td>\n",
       "      <td>0.108679</td>\n",
       "      <td>...</td>\n",
       "      <td>0.215523</td>\n",
       "      <td>0.232570</td>\n",
       "      <td>0.112012</td>\n",
       "      <td>0.129832</td>\n",
       "      <td>0.093949</td>\n",
       "      <td>0.111635</td>\n",
       "      <td>0.316733</td>\n",
       "      <td>0.149886</td>\n",
       "      <td>0.137854</td>\n",
       "      <td>0.195088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.117803</td>\n",
       "      <td>0.132331</td>\n",
       "      <td>0.441256</td>\n",
       "      <td>0.106516</td>\n",
       "      <td>0.112609</td>\n",
       "      <td>0.095599</td>\n",
       "      <td>0.130195</td>\n",
       "      <td>0.082792</td>\n",
       "      <td>0.246457</td>\n",
       "      <td>0.092820</td>\n",
       "      <td>...</td>\n",
       "      <td>0.204261</td>\n",
       "      <td>0.128964</td>\n",
       "      <td>0.196279</td>\n",
       "      <td>0.130195</td>\n",
       "      <td>0.149092</td>\n",
       "      <td>0.169246</td>\n",
       "      <td>0.318336</td>\n",
       "      <td>0.232269</td>\n",
       "      <td>0.185783</td>\n",
       "      <td>0.197351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.160430</td>\n",
       "      <td>0.152927</td>\n",
       "      <td>0.252626</td>\n",
       "      <td>0.185120</td>\n",
       "      <td>0.172182</td>\n",
       "      <td>0.177898</td>\n",
       "      <td>0.176718</td>\n",
       "      <td>0.234021</td>\n",
       "      <td>0.239307</td>\n",
       "      <td>0.115349</td>\n",
       "      <td>...</td>\n",
       "      <td>0.196683</td>\n",
       "      <td>0.223627</td>\n",
       "      <td>0.156862</td>\n",
       "      <td>0.176718</td>\n",
       "      <td>0.166922</td>\n",
       "      <td>0.136539</td>\n",
       "      <td>0.424173</td>\n",
       "      <td>0.230212</td>\n",
       "      <td>0.185834</td>\n",
       "      <td>0.218898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.142067</td>\n",
       "      <td>0.123032</td>\n",
       "      <td>0.287412</td>\n",
       "      <td>0.108177</td>\n",
       "      <td>0.110873</td>\n",
       "      <td>0.168238</td>\n",
       "      <td>0.114671</td>\n",
       "      <td>0.088035</td>\n",
       "      <td>0.166622</td>\n",
       "      <td>0.119443</td>\n",
       "      <td>...</td>\n",
       "      <td>0.178207</td>\n",
       "      <td>0.211274</td>\n",
       "      <td>0.127121</td>\n",
       "      <td>0.114671</td>\n",
       "      <td>0.109226</td>\n",
       "      <td>0.155241</td>\n",
       "      <td>0.326918</td>\n",
       "      <td>0.251955</td>\n",
       "      <td>0.102415</td>\n",
       "      <td>0.175681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.157845</td>\n",
       "      <td>0.119515</td>\n",
       "      <td>0.308348</td>\n",
       "      <td>0.129062</td>\n",
       "      <td>0.122348</td>\n",
       "      <td>0.137723</td>\n",
       "      <td>0.151636</td>\n",
       "      <td>0.115925</td>\n",
       "      <td>0.155132</td>\n",
       "      <td>0.103822</td>\n",
       "      <td>...</td>\n",
       "      <td>0.234224</td>\n",
       "      <td>0.119027</td>\n",
       "      <td>0.181260</td>\n",
       "      <td>0.151636</td>\n",
       "      <td>0.153913</td>\n",
       "      <td>0.244443</td>\n",
       "      <td>0.388604</td>\n",
       "      <td>0.275169</td>\n",
       "      <td>0.091829</td>\n",
       "      <td>0.156603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.188661</td>\n",
       "      <td>0.265782</td>\n",
       "      <td>0.284563</td>\n",
       "      <td>0.116114</td>\n",
       "      <td>0.103378</td>\n",
       "      <td>0.128445</td>\n",
       "      <td>0.180198</td>\n",
       "      <td>0.124389</td>\n",
       "      <td>0.179644</td>\n",
       "      <td>0.232692</td>\n",
       "      <td>...</td>\n",
       "      <td>0.249551</td>\n",
       "      <td>0.165761</td>\n",
       "      <td>0.198941</td>\n",
       "      <td>0.180198</td>\n",
       "      <td>0.119450</td>\n",
       "      <td>0.131190</td>\n",
       "      <td>0.362583</td>\n",
       "      <td>0.211162</td>\n",
       "      <td>0.100926</td>\n",
       "      <td>0.160466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.185877</td>\n",
       "      <td>0.211686</td>\n",
       "      <td>0.320170</td>\n",
       "      <td>0.116547</td>\n",
       "      <td>0.114868</td>\n",
       "      <td>0.118110</td>\n",
       "      <td>0.150766</td>\n",
       "      <td>0.088832</td>\n",
       "      <td>0.202079</td>\n",
       "      <td>0.109146</td>\n",
       "      <td>...</td>\n",
       "      <td>0.238250</td>\n",
       "      <td>0.200088</td>\n",
       "      <td>0.199106</td>\n",
       "      <td>0.150766</td>\n",
       "      <td>0.182286</td>\n",
       "      <td>0.164073</td>\n",
       "      <td>0.661338</td>\n",
       "      <td>0.183341</td>\n",
       "      <td>0.144085</td>\n",
       "      <td>0.280492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.238132</td>\n",
       "      <td>0.233418</td>\n",
       "      <td>0.244176</td>\n",
       "      <td>0.136379</td>\n",
       "      <td>0.199970</td>\n",
       "      <td>0.248987</td>\n",
       "      <td>0.184922</td>\n",
       "      <td>0.280055</td>\n",
       "      <td>0.322700</td>\n",
       "      <td>0.119790</td>\n",
       "      <td>...</td>\n",
       "      <td>0.184627</td>\n",
       "      <td>0.187121</td>\n",
       "      <td>0.268068</td>\n",
       "      <td>0.184922</td>\n",
       "      <td>0.137070</td>\n",
       "      <td>0.126538</td>\n",
       "      <td>0.317065</td>\n",
       "      <td>0.223282</td>\n",
       "      <td>0.176036</td>\n",
       "      <td>0.191981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.147577</td>\n",
       "      <td>0.115959</td>\n",
       "      <td>0.214349</td>\n",
       "      <td>0.118533</td>\n",
       "      <td>0.119846</td>\n",
       "      <td>0.166594</td>\n",
       "      <td>0.089290</td>\n",
       "      <td>0.077847</td>\n",
       "      <td>0.155134</td>\n",
       "      <td>0.130066</td>\n",
       "      <td>...</td>\n",
       "      <td>0.167720</td>\n",
       "      <td>0.099432</td>\n",
       "      <td>0.166651</td>\n",
       "      <td>0.089290</td>\n",
       "      <td>0.080125</td>\n",
       "      <td>0.109560</td>\n",
       "      <td>0.298032</td>\n",
       "      <td>0.180229</td>\n",
       "      <td>0.118343</td>\n",
       "      <td>0.167386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.157992</td>\n",
       "      <td>0.304693</td>\n",
       "      <td>0.355942</td>\n",
       "      <td>0.122546</td>\n",
       "      <td>0.105168</td>\n",
       "      <td>0.183691</td>\n",
       "      <td>0.130373</td>\n",
       "      <td>0.131000</td>\n",
       "      <td>0.221999</td>\n",
       "      <td>0.215578</td>\n",
       "      <td>...</td>\n",
       "      <td>0.242434</td>\n",
       "      <td>0.323302</td>\n",
       "      <td>0.166690</td>\n",
       "      <td>0.130373</td>\n",
       "      <td>0.093667</td>\n",
       "      <td>0.141407</td>\n",
       "      <td>0.324945</td>\n",
       "      <td>0.184456</td>\n",
       "      <td>0.105682</td>\n",
       "      <td>0.169282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.235340</td>\n",
       "      <td>0.110541</td>\n",
       "      <td>0.450521</td>\n",
       "      <td>0.134559</td>\n",
       "      <td>0.097764</td>\n",
       "      <td>0.256432</td>\n",
       "      <td>0.168198</td>\n",
       "      <td>0.118216</td>\n",
       "      <td>0.210438</td>\n",
       "      <td>0.153485</td>\n",
       "      <td>...</td>\n",
       "      <td>0.172566</td>\n",
       "      <td>0.197608</td>\n",
       "      <td>0.309507</td>\n",
       "      <td>0.168198</td>\n",
       "      <td>0.180388</td>\n",
       "      <td>0.184139</td>\n",
       "      <td>0.422355</td>\n",
       "      <td>0.171045</td>\n",
       "      <td>0.212293</td>\n",
       "      <td>0.261631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.132646</td>\n",
       "      <td>0.241493</td>\n",
       "      <td>0.575941</td>\n",
       "      <td>0.232110</td>\n",
       "      <td>0.128954</td>\n",
       "      <td>0.192955</td>\n",
       "      <td>0.146430</td>\n",
       "      <td>0.220790</td>\n",
       "      <td>0.205782</td>\n",
       "      <td>0.329546</td>\n",
       "      <td>...</td>\n",
       "      <td>0.175176</td>\n",
       "      <td>0.347181</td>\n",
       "      <td>0.125304</td>\n",
       "      <td>0.146430</td>\n",
       "      <td>0.107308</td>\n",
       "      <td>0.162873</td>\n",
       "      <td>0.670893</td>\n",
       "      <td>0.307623</td>\n",
       "      <td>0.141114</td>\n",
       "      <td>0.172004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.180093</td>\n",
       "      <td>0.135327</td>\n",
       "      <td>0.255467</td>\n",
       "      <td>0.127227</td>\n",
       "      <td>0.129886</td>\n",
       "      <td>0.122549</td>\n",
       "      <td>0.129057</td>\n",
       "      <td>0.094189</td>\n",
       "      <td>0.210291</td>\n",
       "      <td>0.170511</td>\n",
       "      <td>...</td>\n",
       "      <td>0.189844</td>\n",
       "      <td>0.148207</td>\n",
       "      <td>0.175219</td>\n",
       "      <td>0.129057</td>\n",
       "      <td>0.108649</td>\n",
       "      <td>0.104487</td>\n",
       "      <td>0.335359</td>\n",
       "      <td>0.311732</td>\n",
       "      <td>0.135854</td>\n",
       "      <td>0.205047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.201452</td>\n",
       "      <td>0.134947</td>\n",
       "      <td>0.241145</td>\n",
       "      <td>0.140372</td>\n",
       "      <td>0.139743</td>\n",
       "      <td>0.092075</td>\n",
       "      <td>0.087806</td>\n",
       "      <td>0.239577</td>\n",
       "      <td>0.176349</td>\n",
       "      <td>0.114198</td>\n",
       "      <td>...</td>\n",
       "      <td>0.166476</td>\n",
       "      <td>0.117420</td>\n",
       "      <td>0.162774</td>\n",
       "      <td>0.087806</td>\n",
       "      <td>0.082716</td>\n",
       "      <td>0.130095</td>\n",
       "      <td>0.443038</td>\n",
       "      <td>0.156910</td>\n",
       "      <td>0.143777</td>\n",
       "      <td>0.165756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.128410</td>\n",
       "      <td>0.166555</td>\n",
       "      <td>0.266935</td>\n",
       "      <td>0.130848</td>\n",
       "      <td>0.121671</td>\n",
       "      <td>0.100950</td>\n",
       "      <td>0.150879</td>\n",
       "      <td>0.107978</td>\n",
       "      <td>0.196925</td>\n",
       "      <td>0.144598</td>\n",
       "      <td>...</td>\n",
       "      <td>0.180315</td>\n",
       "      <td>0.234844</td>\n",
       "      <td>0.226106</td>\n",
       "      <td>0.150879</td>\n",
       "      <td>0.162903</td>\n",
       "      <td>0.452549</td>\n",
       "      <td>0.369756</td>\n",
       "      <td>0.444511</td>\n",
       "      <td>0.109759</td>\n",
       "      <td>0.221136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.110213</td>\n",
       "      <td>0.131392</td>\n",
       "      <td>0.261214</td>\n",
       "      <td>0.106889</td>\n",
       "      <td>0.109817</td>\n",
       "      <td>0.112219</td>\n",
       "      <td>0.125396</td>\n",
       "      <td>0.264874</td>\n",
       "      <td>0.156368</td>\n",
       "      <td>0.175625</td>\n",
       "      <td>...</td>\n",
       "      <td>0.157375</td>\n",
       "      <td>0.143812</td>\n",
       "      <td>0.167725</td>\n",
       "      <td>0.125396</td>\n",
       "      <td>0.096824</td>\n",
       "      <td>0.115083</td>\n",
       "      <td>0.382636</td>\n",
       "      <td>0.128476</td>\n",
       "      <td>0.155283</td>\n",
       "      <td>0.190141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.246438</td>\n",
       "      <td>0.232479</td>\n",
       "      <td>0.535876</td>\n",
       "      <td>0.181813</td>\n",
       "      <td>0.183480</td>\n",
       "      <td>0.121158</td>\n",
       "      <td>0.117238</td>\n",
       "      <td>0.091852</td>\n",
       "      <td>0.168169</td>\n",
       "      <td>0.215581</td>\n",
       "      <td>...</td>\n",
       "      <td>0.306549</td>\n",
       "      <td>0.208302</td>\n",
       "      <td>0.177766</td>\n",
       "      <td>0.117238</td>\n",
       "      <td>0.185085</td>\n",
       "      <td>0.174043</td>\n",
       "      <td>0.448848</td>\n",
       "      <td>0.303456</td>\n",
       "      <td>0.097363</td>\n",
       "      <td>0.351441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.149130</td>\n",
       "      <td>0.235019</td>\n",
       "      <td>0.323453</td>\n",
       "      <td>0.091682</td>\n",
       "      <td>0.122775</td>\n",
       "      <td>0.088954</td>\n",
       "      <td>0.121894</td>\n",
       "      <td>0.091215</td>\n",
       "      <td>0.155372</td>\n",
       "      <td>0.239031</td>\n",
       "      <td>...</td>\n",
       "      <td>0.186062</td>\n",
       "      <td>0.234409</td>\n",
       "      <td>0.197827</td>\n",
       "      <td>0.121894</td>\n",
       "      <td>0.078992</td>\n",
       "      <td>0.128192</td>\n",
       "      <td>0.322891</td>\n",
       "      <td>0.337812</td>\n",
       "      <td>0.279166</td>\n",
       "      <td>0.192140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.132723</td>\n",
       "      <td>0.235959</td>\n",
       "      <td>0.377313</td>\n",
       "      <td>0.180984</td>\n",
       "      <td>0.185398</td>\n",
       "      <td>0.103797</td>\n",
       "      <td>0.111535</td>\n",
       "      <td>0.101712</td>\n",
       "      <td>0.228005</td>\n",
       "      <td>0.311384</td>\n",
       "      <td>...</td>\n",
       "      <td>0.174160</td>\n",
       "      <td>0.315993</td>\n",
       "      <td>0.277180</td>\n",
       "      <td>0.111535</td>\n",
       "      <td>0.120086</td>\n",
       "      <td>0.176352</td>\n",
       "      <td>0.385293</td>\n",
       "      <td>0.216331</td>\n",
       "      <td>0.214585</td>\n",
       "      <td>0.194260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.147260</td>\n",
       "      <td>0.127488</td>\n",
       "      <td>0.181270</td>\n",
       "      <td>0.079728</td>\n",
       "      <td>0.098014</td>\n",
       "      <td>0.164212</td>\n",
       "      <td>0.151197</td>\n",
       "      <td>0.129921</td>\n",
       "      <td>0.174140</td>\n",
       "      <td>0.121081</td>\n",
       "      <td>...</td>\n",
       "      <td>0.255022</td>\n",
       "      <td>0.186838</td>\n",
       "      <td>0.228311</td>\n",
       "      <td>0.151197</td>\n",
       "      <td>0.078762</td>\n",
       "      <td>0.182663</td>\n",
       "      <td>0.282476</td>\n",
       "      <td>0.181725</td>\n",
       "      <td>0.143418</td>\n",
       "      <td>0.191033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.181051</td>\n",
       "      <td>0.153261</td>\n",
       "      <td>0.236841</td>\n",
       "      <td>0.178641</td>\n",
       "      <td>0.092876</td>\n",
       "      <td>0.136989</td>\n",
       "      <td>0.130698</td>\n",
       "      <td>0.101730</td>\n",
       "      <td>0.183492</td>\n",
       "      <td>0.125405</td>\n",
       "      <td>...</td>\n",
       "      <td>0.258043</td>\n",
       "      <td>0.110183</td>\n",
       "      <td>0.127197</td>\n",
       "      <td>0.130698</td>\n",
       "      <td>0.128632</td>\n",
       "      <td>0.157205</td>\n",
       "      <td>0.343907</td>\n",
       "      <td>0.255538</td>\n",
       "      <td>0.136574</td>\n",
       "      <td>0.153329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26749</th>\n",
       "      <td>0.160436</td>\n",
       "      <td>0.140607</td>\n",
       "      <td>0.123564</td>\n",
       "      <td>0.121316</td>\n",
       "      <td>0.147534</td>\n",
       "      <td>0.149459</td>\n",
       "      <td>0.122764</td>\n",
       "      <td>0.085552</td>\n",
       "      <td>0.177913</td>\n",
       "      <td>0.192557</td>\n",
       "      <td>...</td>\n",
       "      <td>0.176951</td>\n",
       "      <td>0.134045</td>\n",
       "      <td>0.141297</td>\n",
       "      <td>0.122764</td>\n",
       "      <td>0.130524</td>\n",
       "      <td>0.136203</td>\n",
       "      <td>0.116855</td>\n",
       "      <td>0.330210</td>\n",
       "      <td>0.155101</td>\n",
       "      <td>0.184109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26750</th>\n",
       "      <td>0.173017</td>\n",
       "      <td>0.124357</td>\n",
       "      <td>0.121314</td>\n",
       "      <td>0.153231</td>\n",
       "      <td>0.137259</td>\n",
       "      <td>0.079218</td>\n",
       "      <td>0.110055</td>\n",
       "      <td>0.086907</td>\n",
       "      <td>0.192796</td>\n",
       "      <td>0.095866</td>\n",
       "      <td>...</td>\n",
       "      <td>0.178551</td>\n",
       "      <td>0.220209</td>\n",
       "      <td>0.121238</td>\n",
       "      <td>0.110055</td>\n",
       "      <td>0.274031</td>\n",
       "      <td>0.136400</td>\n",
       "      <td>0.122675</td>\n",
       "      <td>0.166302</td>\n",
       "      <td>0.119934</td>\n",
       "      <td>0.254717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26751</th>\n",
       "      <td>0.141669</td>\n",
       "      <td>0.127956</td>\n",
       "      <td>0.122265</td>\n",
       "      <td>0.205780</td>\n",
       "      <td>0.162651</td>\n",
       "      <td>0.382500</td>\n",
       "      <td>0.094335</td>\n",
       "      <td>0.335057</td>\n",
       "      <td>0.176286</td>\n",
       "      <td>0.163318</td>\n",
       "      <td>...</td>\n",
       "      <td>0.222313</td>\n",
       "      <td>0.133544</td>\n",
       "      <td>0.155535</td>\n",
       "      <td>0.094335</td>\n",
       "      <td>0.107299</td>\n",
       "      <td>0.134289</td>\n",
       "      <td>0.123596</td>\n",
       "      <td>0.235690</td>\n",
       "      <td>0.152807</td>\n",
       "      <td>0.188513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26752</th>\n",
       "      <td>0.143975</td>\n",
       "      <td>0.186471</td>\n",
       "      <td>0.136822</td>\n",
       "      <td>0.124764</td>\n",
       "      <td>0.139127</td>\n",
       "      <td>0.191201</td>\n",
       "      <td>0.123898</td>\n",
       "      <td>0.105100</td>\n",
       "      <td>0.215217</td>\n",
       "      <td>0.188477</td>\n",
       "      <td>...</td>\n",
       "      <td>0.175541</td>\n",
       "      <td>0.120678</td>\n",
       "      <td>0.246534</td>\n",
       "      <td>0.123898</td>\n",
       "      <td>0.138415</td>\n",
       "      <td>0.141216</td>\n",
       "      <td>0.138498</td>\n",
       "      <td>0.181066</td>\n",
       "      <td>0.086517</td>\n",
       "      <td>0.164061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26753</th>\n",
       "      <td>0.267161</td>\n",
       "      <td>0.260282</td>\n",
       "      <td>0.355809</td>\n",
       "      <td>0.161382</td>\n",
       "      <td>0.129316</td>\n",
       "      <td>0.182350</td>\n",
       "      <td>0.139541</td>\n",
       "      <td>0.237930</td>\n",
       "      <td>0.329874</td>\n",
       "      <td>0.230868</td>\n",
       "      <td>...</td>\n",
       "      <td>0.210642</td>\n",
       "      <td>0.275226</td>\n",
       "      <td>0.161843</td>\n",
       "      <td>0.139541</td>\n",
       "      <td>0.142964</td>\n",
       "      <td>0.424154</td>\n",
       "      <td>0.320345</td>\n",
       "      <td>0.316677</td>\n",
       "      <td>0.164455</td>\n",
       "      <td>0.168505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26754</th>\n",
       "      <td>0.212230</td>\n",
       "      <td>0.148520</td>\n",
       "      <td>0.138231</td>\n",
       "      <td>0.208016</td>\n",
       "      <td>0.124658</td>\n",
       "      <td>0.246744</td>\n",
       "      <td>0.159450</td>\n",
       "      <td>0.131943</td>\n",
       "      <td>0.375505</td>\n",
       "      <td>0.134976</td>\n",
       "      <td>...</td>\n",
       "      <td>0.159908</td>\n",
       "      <td>0.251483</td>\n",
       "      <td>0.169436</td>\n",
       "      <td>0.159450</td>\n",
       "      <td>0.132638</td>\n",
       "      <td>0.165062</td>\n",
       "      <td>0.156353</td>\n",
       "      <td>0.332878</td>\n",
       "      <td>0.209145</td>\n",
       "      <td>0.204932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26755</th>\n",
       "      <td>0.190822</td>\n",
       "      <td>0.273574</td>\n",
       "      <td>0.203921</td>\n",
       "      <td>0.136178</td>\n",
       "      <td>0.238640</td>\n",
       "      <td>0.244811</td>\n",
       "      <td>0.134228</td>\n",
       "      <td>0.271883</td>\n",
       "      <td>0.207566</td>\n",
       "      <td>0.243639</td>\n",
       "      <td>...</td>\n",
       "      <td>0.202985</td>\n",
       "      <td>0.406173</td>\n",
       "      <td>0.140143</td>\n",
       "      <td>0.134228</td>\n",
       "      <td>0.110770</td>\n",
       "      <td>0.117924</td>\n",
       "      <td>0.123119</td>\n",
       "      <td>0.194684</td>\n",
       "      <td>0.336912</td>\n",
       "      <td>0.175510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26756</th>\n",
       "      <td>0.222275</td>\n",
       "      <td>0.223805</td>\n",
       "      <td>0.229527</td>\n",
       "      <td>0.102681</td>\n",
       "      <td>0.177806</td>\n",
       "      <td>0.121785</td>\n",
       "      <td>0.138506</td>\n",
       "      <td>0.435683</td>\n",
       "      <td>0.220892</td>\n",
       "      <td>0.162628</td>\n",
       "      <td>...</td>\n",
       "      <td>0.181194</td>\n",
       "      <td>0.120527</td>\n",
       "      <td>0.225839</td>\n",
       "      <td>0.138506</td>\n",
       "      <td>0.111494</td>\n",
       "      <td>0.172912</td>\n",
       "      <td>0.314851</td>\n",
       "      <td>0.309226</td>\n",
       "      <td>0.194424</td>\n",
       "      <td>0.366699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26757</th>\n",
       "      <td>0.256011</td>\n",
       "      <td>0.245265</td>\n",
       "      <td>0.127546</td>\n",
       "      <td>0.207967</td>\n",
       "      <td>0.263836</td>\n",
       "      <td>0.400749</td>\n",
       "      <td>0.135215</td>\n",
       "      <td>0.550912</td>\n",
       "      <td>0.299036</td>\n",
       "      <td>0.190023</td>\n",
       "      <td>...</td>\n",
       "      <td>0.219098</td>\n",
       "      <td>0.263495</td>\n",
       "      <td>0.277726</td>\n",
       "      <td>0.135215</td>\n",
       "      <td>0.128291</td>\n",
       "      <td>0.128863</td>\n",
       "      <td>0.152228</td>\n",
       "      <td>0.269798</td>\n",
       "      <td>0.161638</td>\n",
       "      <td>0.186051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26758</th>\n",
       "      <td>0.187088</td>\n",
       "      <td>0.131898</td>\n",
       "      <td>0.132892</td>\n",
       "      <td>0.138632</td>\n",
       "      <td>0.163746</td>\n",
       "      <td>0.252380</td>\n",
       "      <td>0.124024</td>\n",
       "      <td>0.394997</td>\n",
       "      <td>0.139384</td>\n",
       "      <td>0.111958</td>\n",
       "      <td>...</td>\n",
       "      <td>0.224619</td>\n",
       "      <td>0.120316</td>\n",
       "      <td>0.207390</td>\n",
       "      <td>0.124024</td>\n",
       "      <td>0.093529</td>\n",
       "      <td>0.141381</td>\n",
       "      <td>0.219787</td>\n",
       "      <td>0.147827</td>\n",
       "      <td>0.140212</td>\n",
       "      <td>0.176858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26759</th>\n",
       "      <td>0.232753</td>\n",
       "      <td>0.131475</td>\n",
       "      <td>0.127788</td>\n",
       "      <td>0.112111</td>\n",
       "      <td>0.114041</td>\n",
       "      <td>0.147279</td>\n",
       "      <td>0.126277</td>\n",
       "      <td>0.224112</td>\n",
       "      <td>0.155178</td>\n",
       "      <td>0.156875</td>\n",
       "      <td>...</td>\n",
       "      <td>0.225331</td>\n",
       "      <td>0.153588</td>\n",
       "      <td>0.140628</td>\n",
       "      <td>0.126277</td>\n",
       "      <td>0.139262</td>\n",
       "      <td>0.117411</td>\n",
       "      <td>0.145548</td>\n",
       "      <td>0.194617</td>\n",
       "      <td>0.210859</td>\n",
       "      <td>0.178710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26760</th>\n",
       "      <td>0.154928</td>\n",
       "      <td>0.228180</td>\n",
       "      <td>0.118117</td>\n",
       "      <td>0.157885</td>\n",
       "      <td>0.117910</td>\n",
       "      <td>0.127771</td>\n",
       "      <td>0.120376</td>\n",
       "      <td>0.126281</td>\n",
       "      <td>0.193876</td>\n",
       "      <td>0.193420</td>\n",
       "      <td>...</td>\n",
       "      <td>0.159401</td>\n",
       "      <td>0.217892</td>\n",
       "      <td>0.163767</td>\n",
       "      <td>0.120376</td>\n",
       "      <td>0.276424</td>\n",
       "      <td>0.107929</td>\n",
       "      <td>0.114855</td>\n",
       "      <td>0.298452</td>\n",
       "      <td>0.170757</td>\n",
       "      <td>0.224565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26761</th>\n",
       "      <td>0.259607</td>\n",
       "      <td>0.127876</td>\n",
       "      <td>0.125805</td>\n",
       "      <td>0.099223</td>\n",
       "      <td>0.198447</td>\n",
       "      <td>0.124315</td>\n",
       "      <td>0.134269</td>\n",
       "      <td>0.119608</td>\n",
       "      <td>0.137111</td>\n",
       "      <td>0.122757</td>\n",
       "      <td>...</td>\n",
       "      <td>0.225439</td>\n",
       "      <td>0.119203</td>\n",
       "      <td>0.132112</td>\n",
       "      <td>0.134269</td>\n",
       "      <td>0.099086</td>\n",
       "      <td>0.130971</td>\n",
       "      <td>0.125084</td>\n",
       "      <td>0.158476</td>\n",
       "      <td>0.177354</td>\n",
       "      <td>0.185583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26762</th>\n",
       "      <td>0.219341</td>\n",
       "      <td>0.161623</td>\n",
       "      <td>0.150086</td>\n",
       "      <td>0.182597</td>\n",
       "      <td>0.176313</td>\n",
       "      <td>0.403695</td>\n",
       "      <td>0.130704</td>\n",
       "      <td>0.296908</td>\n",
       "      <td>0.488085</td>\n",
       "      <td>0.114246</td>\n",
       "      <td>...</td>\n",
       "      <td>0.201703</td>\n",
       "      <td>0.203187</td>\n",
       "      <td>0.180501</td>\n",
       "      <td>0.130704</td>\n",
       "      <td>0.119369</td>\n",
       "      <td>0.173095</td>\n",
       "      <td>0.182169</td>\n",
       "      <td>0.211351</td>\n",
       "      <td>0.130189</td>\n",
       "      <td>0.191372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26763</th>\n",
       "      <td>0.177317</td>\n",
       "      <td>0.135271</td>\n",
       "      <td>0.130985</td>\n",
       "      <td>0.152720</td>\n",
       "      <td>0.123628</td>\n",
       "      <td>0.154642</td>\n",
       "      <td>0.112467</td>\n",
       "      <td>0.264784</td>\n",
       "      <td>0.369404</td>\n",
       "      <td>0.142279</td>\n",
       "      <td>...</td>\n",
       "      <td>0.188764</td>\n",
       "      <td>0.223480</td>\n",
       "      <td>0.210016</td>\n",
       "      <td>0.112467</td>\n",
       "      <td>0.153925</td>\n",
       "      <td>0.131980</td>\n",
       "      <td>0.128372</td>\n",
       "      <td>0.172259</td>\n",
       "      <td>0.169881</td>\n",
       "      <td>0.284737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26764</th>\n",
       "      <td>0.124515</td>\n",
       "      <td>0.134084</td>\n",
       "      <td>0.140478</td>\n",
       "      <td>0.154302</td>\n",
       "      <td>0.185171</td>\n",
       "      <td>0.112114</td>\n",
       "      <td>0.136347</td>\n",
       "      <td>0.105497</td>\n",
       "      <td>0.301475</td>\n",
       "      <td>0.104429</td>\n",
       "      <td>...</td>\n",
       "      <td>0.192671</td>\n",
       "      <td>0.125509</td>\n",
       "      <td>0.147539</td>\n",
       "      <td>0.136347</td>\n",
       "      <td>0.153267</td>\n",
       "      <td>0.128259</td>\n",
       "      <td>0.180827</td>\n",
       "      <td>0.334200</td>\n",
       "      <td>0.185228</td>\n",
       "      <td>0.201373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26765</th>\n",
       "      <td>0.220783</td>\n",
       "      <td>0.123174</td>\n",
       "      <td>0.125267</td>\n",
       "      <td>0.146882</td>\n",
       "      <td>0.107390</td>\n",
       "      <td>0.256031</td>\n",
       "      <td>0.183569</td>\n",
       "      <td>0.377140</td>\n",
       "      <td>0.153990</td>\n",
       "      <td>0.104049</td>\n",
       "      <td>...</td>\n",
       "      <td>0.161963</td>\n",
       "      <td>0.116511</td>\n",
       "      <td>0.170236</td>\n",
       "      <td>0.183569</td>\n",
       "      <td>0.095002</td>\n",
       "      <td>0.129926</td>\n",
       "      <td>0.161320</td>\n",
       "      <td>0.172026</td>\n",
       "      <td>0.095402</td>\n",
       "      <td>0.138390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26766</th>\n",
       "      <td>0.136396</td>\n",
       "      <td>0.152512</td>\n",
       "      <td>0.143159</td>\n",
       "      <td>0.142415</td>\n",
       "      <td>0.136935</td>\n",
       "      <td>0.126615</td>\n",
       "      <td>0.098275</td>\n",
       "      <td>0.111090</td>\n",
       "      <td>0.138746</td>\n",
       "      <td>0.118680</td>\n",
       "      <td>...</td>\n",
       "      <td>0.212482</td>\n",
       "      <td>0.117832</td>\n",
       "      <td>0.107394</td>\n",
       "      <td>0.098275</td>\n",
       "      <td>0.098371</td>\n",
       "      <td>0.119111</td>\n",
       "      <td>0.132522</td>\n",
       "      <td>0.312086</td>\n",
       "      <td>0.189465</td>\n",
       "      <td>0.241916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26767</th>\n",
       "      <td>0.318088</td>\n",
       "      <td>0.214981</td>\n",
       "      <td>0.129173</td>\n",
       "      <td>0.122056</td>\n",
       "      <td>0.138520</td>\n",
       "      <td>0.221515</td>\n",
       "      <td>0.096435</td>\n",
       "      <td>0.297090</td>\n",
       "      <td>0.303401</td>\n",
       "      <td>0.170058</td>\n",
       "      <td>...</td>\n",
       "      <td>0.177720</td>\n",
       "      <td>0.187417</td>\n",
       "      <td>0.163681</td>\n",
       "      <td>0.096435</td>\n",
       "      <td>0.122765</td>\n",
       "      <td>0.187320</td>\n",
       "      <td>0.167180</td>\n",
       "      <td>0.186513</td>\n",
       "      <td>0.098886</td>\n",
       "      <td>0.167779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26768</th>\n",
       "      <td>0.166141</td>\n",
       "      <td>0.156828</td>\n",
       "      <td>0.165081</td>\n",
       "      <td>0.210254</td>\n",
       "      <td>0.197398</td>\n",
       "      <td>0.273639</td>\n",
       "      <td>0.156441</td>\n",
       "      <td>0.088113</td>\n",
       "      <td>0.274875</td>\n",
       "      <td>0.139165</td>\n",
       "      <td>...</td>\n",
       "      <td>0.229152</td>\n",
       "      <td>0.256804</td>\n",
       "      <td>0.322030</td>\n",
       "      <td>0.156441</td>\n",
       "      <td>0.152301</td>\n",
       "      <td>0.092746</td>\n",
       "      <td>0.159485</td>\n",
       "      <td>0.190567</td>\n",
       "      <td>0.092827</td>\n",
       "      <td>0.153309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26769</th>\n",
       "      <td>0.121907</td>\n",
       "      <td>0.127220</td>\n",
       "      <td>0.123193</td>\n",
       "      <td>0.131297</td>\n",
       "      <td>0.171650</td>\n",
       "      <td>0.081270</td>\n",
       "      <td>0.131754</td>\n",
       "      <td>0.259684</td>\n",
       "      <td>0.252153</td>\n",
       "      <td>0.098927</td>\n",
       "      <td>...</td>\n",
       "      <td>0.204814</td>\n",
       "      <td>0.285868</td>\n",
       "      <td>0.176790</td>\n",
       "      <td>0.131754</td>\n",
       "      <td>0.092409</td>\n",
       "      <td>0.134009</td>\n",
       "      <td>0.162578</td>\n",
       "      <td>0.262863</td>\n",
       "      <td>0.099636</td>\n",
       "      <td>0.155270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26770</th>\n",
       "      <td>0.187508</td>\n",
       "      <td>0.204138</td>\n",
       "      <td>0.125430</td>\n",
       "      <td>0.126233</td>\n",
       "      <td>0.391373</td>\n",
       "      <td>0.256350</td>\n",
       "      <td>0.183450</td>\n",
       "      <td>0.322342</td>\n",
       "      <td>0.348512</td>\n",
       "      <td>0.106761</td>\n",
       "      <td>...</td>\n",
       "      <td>0.277271</td>\n",
       "      <td>0.261232</td>\n",
       "      <td>0.141170</td>\n",
       "      <td>0.183450</td>\n",
       "      <td>0.133196</td>\n",
       "      <td>0.097647</td>\n",
       "      <td>0.150018</td>\n",
       "      <td>0.222733</td>\n",
       "      <td>0.206094</td>\n",
       "      <td>0.192797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26771</th>\n",
       "      <td>0.139937</td>\n",
       "      <td>0.131325</td>\n",
       "      <td>0.142922</td>\n",
       "      <td>0.144010</td>\n",
       "      <td>0.269953</td>\n",
       "      <td>0.255807</td>\n",
       "      <td>0.163349</td>\n",
       "      <td>0.295996</td>\n",
       "      <td>0.167834</td>\n",
       "      <td>0.112172</td>\n",
       "      <td>...</td>\n",
       "      <td>0.154729</td>\n",
       "      <td>0.127456</td>\n",
       "      <td>0.120069</td>\n",
       "      <td>0.163349</td>\n",
       "      <td>0.140542</td>\n",
       "      <td>0.147564</td>\n",
       "      <td>0.190648</td>\n",
       "      <td>0.231170</td>\n",
       "      <td>0.138187</td>\n",
       "      <td>0.173128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26772</th>\n",
       "      <td>0.130880</td>\n",
       "      <td>0.144202</td>\n",
       "      <td>0.093616</td>\n",
       "      <td>0.107273</td>\n",
       "      <td>0.156020</td>\n",
       "      <td>0.218315</td>\n",
       "      <td>0.098107</td>\n",
       "      <td>0.278263</td>\n",
       "      <td>0.166644</td>\n",
       "      <td>0.108896</td>\n",
       "      <td>...</td>\n",
       "      <td>0.194466</td>\n",
       "      <td>0.113732</td>\n",
       "      <td>0.151458</td>\n",
       "      <td>0.098107</td>\n",
       "      <td>0.092334</td>\n",
       "      <td>0.189712</td>\n",
       "      <td>0.136244</td>\n",
       "      <td>0.191860</td>\n",
       "      <td>0.150964</td>\n",
       "      <td>0.206547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26773</th>\n",
       "      <td>0.216040</td>\n",
       "      <td>0.111038</td>\n",
       "      <td>0.127131</td>\n",
       "      <td>0.137784</td>\n",
       "      <td>0.198300</td>\n",
       "      <td>0.273922</td>\n",
       "      <td>0.110547</td>\n",
       "      <td>0.275026</td>\n",
       "      <td>0.169254</td>\n",
       "      <td>0.158967</td>\n",
       "      <td>...</td>\n",
       "      <td>0.144336</td>\n",
       "      <td>0.110356</td>\n",
       "      <td>0.149517</td>\n",
       "      <td>0.110547</td>\n",
       "      <td>0.085981</td>\n",
       "      <td>0.201461</td>\n",
       "      <td>0.102778</td>\n",
       "      <td>0.145592</td>\n",
       "      <td>0.242391</td>\n",
       "      <td>0.131224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26774</th>\n",
       "      <td>0.172681</td>\n",
       "      <td>0.377526</td>\n",
       "      <td>0.205346</td>\n",
       "      <td>0.146418</td>\n",
       "      <td>0.142063</td>\n",
       "      <td>0.213606</td>\n",
       "      <td>0.099553</td>\n",
       "      <td>0.267805</td>\n",
       "      <td>0.186847</td>\n",
       "      <td>0.256838</td>\n",
       "      <td>...</td>\n",
       "      <td>0.186749</td>\n",
       "      <td>0.234117</td>\n",
       "      <td>0.128084</td>\n",
       "      <td>0.099553</td>\n",
       "      <td>0.109536</td>\n",
       "      <td>0.384339</td>\n",
       "      <td>0.132466</td>\n",
       "      <td>0.246919</td>\n",
       "      <td>0.112786</td>\n",
       "      <td>0.196690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26775</th>\n",
       "      <td>0.193710</td>\n",
       "      <td>0.121627</td>\n",
       "      <td>0.137331</td>\n",
       "      <td>0.267369</td>\n",
       "      <td>0.182466</td>\n",
       "      <td>0.158179</td>\n",
       "      <td>0.132073</td>\n",
       "      <td>0.082947</td>\n",
       "      <td>0.222631</td>\n",
       "      <td>0.125362</td>\n",
       "      <td>...</td>\n",
       "      <td>0.210515</td>\n",
       "      <td>0.144695</td>\n",
       "      <td>0.152873</td>\n",
       "      <td>0.132073</td>\n",
       "      <td>0.111285</td>\n",
       "      <td>0.187336</td>\n",
       "      <td>0.124931</td>\n",
       "      <td>0.185453</td>\n",
       "      <td>0.132280</td>\n",
       "      <td>0.191615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26776</th>\n",
       "      <td>0.182415</td>\n",
       "      <td>0.168940</td>\n",
       "      <td>0.162624</td>\n",
       "      <td>0.093713</td>\n",
       "      <td>0.146890</td>\n",
       "      <td>0.099770</td>\n",
       "      <td>0.133666</td>\n",
       "      <td>0.294795</td>\n",
       "      <td>0.287017</td>\n",
       "      <td>0.128317</td>\n",
       "      <td>...</td>\n",
       "      <td>0.199943</td>\n",
       "      <td>0.198973</td>\n",
       "      <td>0.088016</td>\n",
       "      <td>0.133666</td>\n",
       "      <td>0.097419</td>\n",
       "      <td>0.117212</td>\n",
       "      <td>0.112917</td>\n",
       "      <td>0.254260</td>\n",
       "      <td>0.087882</td>\n",
       "      <td>0.222596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26777</th>\n",
       "      <td>0.242739</td>\n",
       "      <td>0.200490</td>\n",
       "      <td>0.112401</td>\n",
       "      <td>0.162118</td>\n",
       "      <td>0.100955</td>\n",
       "      <td>0.131281</td>\n",
       "      <td>0.084282</td>\n",
       "      <td>0.091667</td>\n",
       "      <td>0.190928</td>\n",
       "      <td>0.137669</td>\n",
       "      <td>...</td>\n",
       "      <td>0.161906</td>\n",
       "      <td>0.167596</td>\n",
       "      <td>0.114626</td>\n",
       "      <td>0.084282</td>\n",
       "      <td>0.062644</td>\n",
       "      <td>0.106065</td>\n",
       "      <td>0.145623</td>\n",
       "      <td>0.235039</td>\n",
       "      <td>0.140581</td>\n",
       "      <td>0.166122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26778</th>\n",
       "      <td>0.176992</td>\n",
       "      <td>0.233213</td>\n",
       "      <td>0.258102</td>\n",
       "      <td>0.172943</td>\n",
       "      <td>0.284136</td>\n",
       "      <td>0.241065</td>\n",
       "      <td>0.129154</td>\n",
       "      <td>0.242416</td>\n",
       "      <td>0.180258</td>\n",
       "      <td>0.266003</td>\n",
       "      <td>...</td>\n",
       "      <td>0.295705</td>\n",
       "      <td>0.238098</td>\n",
       "      <td>0.252016</td>\n",
       "      <td>0.129154</td>\n",
       "      <td>0.158971</td>\n",
       "      <td>0.111937</td>\n",
       "      <td>0.182483</td>\n",
       "      <td>0.265337</td>\n",
       "      <td>0.251558</td>\n",
       "      <td>0.180560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26779</th>\n",
       "      <td>0.211940</td>\n",
       "      <td>0.127137</td>\n",
       "      <td>0.133443</td>\n",
       "      <td>0.109251</td>\n",
       "      <td>0.223102</td>\n",
       "      <td>0.164927</td>\n",
       "      <td>0.138301</td>\n",
       "      <td>0.170618</td>\n",
       "      <td>0.165602</td>\n",
       "      <td>0.105173</td>\n",
       "      <td>...</td>\n",
       "      <td>0.259832</td>\n",
       "      <td>0.100004</td>\n",
       "      <td>0.335955</td>\n",
       "      <td>0.138301</td>\n",
       "      <td>0.109524</td>\n",
       "      <td>0.113895</td>\n",
       "      <td>0.138413</td>\n",
       "      <td>0.234475</td>\n",
       "      <td>0.138440</td>\n",
       "      <td>0.197979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26780</th>\n",
       "      <td>0.113273</td>\n",
       "      <td>0.140323</td>\n",
       "      <td>0.135302</td>\n",
       "      <td>0.103194</td>\n",
       "      <td>0.508130</td>\n",
       "      <td>0.395414</td>\n",
       "      <td>0.130022</td>\n",
       "      <td>0.374518</td>\n",
       "      <td>0.294936</td>\n",
       "      <td>0.160086</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247176</td>\n",
       "      <td>0.229998</td>\n",
       "      <td>0.177006</td>\n",
       "      <td>0.130022</td>\n",
       "      <td>0.119029</td>\n",
       "      <td>0.159864</td>\n",
       "      <td>0.167771</td>\n",
       "      <td>0.239734</td>\n",
       "      <td>0.110314</td>\n",
       "      <td>0.188731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26781</th>\n",
       "      <td>0.163496</td>\n",
       "      <td>0.222262</td>\n",
       "      <td>0.163853</td>\n",
       "      <td>0.150330</td>\n",
       "      <td>0.219140</td>\n",
       "      <td>0.216911</td>\n",
       "      <td>0.174080</td>\n",
       "      <td>0.567885</td>\n",
       "      <td>0.343381</td>\n",
       "      <td>0.119055</td>\n",
       "      <td>...</td>\n",
       "      <td>0.158221</td>\n",
       "      <td>0.213885</td>\n",
       "      <td>0.192420</td>\n",
       "      <td>0.174080</td>\n",
       "      <td>0.096193</td>\n",
       "      <td>0.101285</td>\n",
       "      <td>0.171966</td>\n",
       "      <td>0.219360</td>\n",
       "      <td>0.170101</td>\n",
       "      <td>0.134241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26782</th>\n",
       "      <td>0.217029</td>\n",
       "      <td>0.124122</td>\n",
       "      <td>0.110521</td>\n",
       "      <td>0.214731</td>\n",
       "      <td>0.137925</td>\n",
       "      <td>0.253825</td>\n",
       "      <td>0.170114</td>\n",
       "      <td>0.378621</td>\n",
       "      <td>0.307349</td>\n",
       "      <td>0.100551</td>\n",
       "      <td>...</td>\n",
       "      <td>0.213889</td>\n",
       "      <td>0.242607</td>\n",
       "      <td>0.098356</td>\n",
       "      <td>0.170114</td>\n",
       "      <td>0.111423</td>\n",
       "      <td>0.112589</td>\n",
       "      <td>0.149096</td>\n",
       "      <td>0.160068</td>\n",
       "      <td>0.177614</td>\n",
       "      <td>0.234714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26783</th>\n",
       "      <td>0.151500</td>\n",
       "      <td>0.175011</td>\n",
       "      <td>0.148327</td>\n",
       "      <td>0.163581</td>\n",
       "      <td>0.347919</td>\n",
       "      <td>0.233945</td>\n",
       "      <td>0.131125</td>\n",
       "      <td>0.434251</td>\n",
       "      <td>0.217858</td>\n",
       "      <td>0.213223</td>\n",
       "      <td>...</td>\n",
       "      <td>0.177949</td>\n",
       "      <td>0.129530</td>\n",
       "      <td>0.127927</td>\n",
       "      <td>0.131125</td>\n",
       "      <td>0.162611</td>\n",
       "      <td>0.137478</td>\n",
       "      <td>0.166814</td>\n",
       "      <td>0.214821</td>\n",
       "      <td>0.097165</td>\n",
       "      <td>0.176508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26784</th>\n",
       "      <td>0.181854</td>\n",
       "      <td>0.130128</td>\n",
       "      <td>0.101643</td>\n",
       "      <td>0.123532</td>\n",
       "      <td>0.166406</td>\n",
       "      <td>0.217472</td>\n",
       "      <td>0.143757</td>\n",
       "      <td>0.440973</td>\n",
       "      <td>0.389462</td>\n",
       "      <td>0.085107</td>\n",
       "      <td>...</td>\n",
       "      <td>0.152678</td>\n",
       "      <td>0.171483</td>\n",
       "      <td>0.129954</td>\n",
       "      <td>0.143757</td>\n",
       "      <td>0.097627</td>\n",
       "      <td>0.203974</td>\n",
       "      <td>0.138768</td>\n",
       "      <td>0.309690</td>\n",
       "      <td>0.098975</td>\n",
       "      <td>0.158423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26785</th>\n",
       "      <td>0.248903</td>\n",
       "      <td>0.145039</td>\n",
       "      <td>0.130425</td>\n",
       "      <td>0.209926</td>\n",
       "      <td>0.163252</td>\n",
       "      <td>0.135203</td>\n",
       "      <td>0.133940</td>\n",
       "      <td>0.542126</td>\n",
       "      <td>0.162357</td>\n",
       "      <td>0.105070</td>\n",
       "      <td>...</td>\n",
       "      <td>0.169298</td>\n",
       "      <td>0.140332</td>\n",
       "      <td>0.197174</td>\n",
       "      <td>0.133940</td>\n",
       "      <td>0.173553</td>\n",
       "      <td>0.115387</td>\n",
       "      <td>0.181295</td>\n",
       "      <td>0.214133</td>\n",
       "      <td>0.232146</td>\n",
       "      <td>0.166705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26786</th>\n",
       "      <td>0.251184</td>\n",
       "      <td>0.219879</td>\n",
       "      <td>0.180528</td>\n",
       "      <td>0.310471</td>\n",
       "      <td>0.119011</td>\n",
       "      <td>0.097076</td>\n",
       "      <td>0.146399</td>\n",
       "      <td>0.219075</td>\n",
       "      <td>0.216761</td>\n",
       "      <td>0.270904</td>\n",
       "      <td>...</td>\n",
       "      <td>0.265979</td>\n",
       "      <td>0.211008</td>\n",
       "      <td>0.121309</td>\n",
       "      <td>0.146399</td>\n",
       "      <td>0.096288</td>\n",
       "      <td>0.165396</td>\n",
       "      <td>0.154588</td>\n",
       "      <td>0.225575</td>\n",
       "      <td>0.244475</td>\n",
       "      <td>0.241712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26787</th>\n",
       "      <td>0.196152</td>\n",
       "      <td>0.158969</td>\n",
       "      <td>0.107197</td>\n",
       "      <td>0.113264</td>\n",
       "      <td>0.220958</td>\n",
       "      <td>0.181141</td>\n",
       "      <td>0.143052</td>\n",
       "      <td>0.252064</td>\n",
       "      <td>0.184105</td>\n",
       "      <td>0.122627</td>\n",
       "      <td>...</td>\n",
       "      <td>0.218689</td>\n",
       "      <td>0.259427</td>\n",
       "      <td>0.233994</td>\n",
       "      <td>0.143052</td>\n",
       "      <td>0.133685</td>\n",
       "      <td>0.120321</td>\n",
       "      <td>0.101498</td>\n",
       "      <td>0.319427</td>\n",
       "      <td>0.145873</td>\n",
       "      <td>0.210100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26788</th>\n",
       "      <td>0.199358</td>\n",
       "      <td>0.142819</td>\n",
       "      <td>0.108297</td>\n",
       "      <td>0.179327</td>\n",
       "      <td>0.208955</td>\n",
       "      <td>0.195545</td>\n",
       "      <td>0.120023</td>\n",
       "      <td>0.304650</td>\n",
       "      <td>0.174369</td>\n",
       "      <td>0.134595</td>\n",
       "      <td>...</td>\n",
       "      <td>0.263371</td>\n",
       "      <td>0.135871</td>\n",
       "      <td>0.350634</td>\n",
       "      <td>0.120023</td>\n",
       "      <td>0.092221</td>\n",
       "      <td>0.117225</td>\n",
       "      <td>0.121786</td>\n",
       "      <td>0.162540</td>\n",
       "      <td>0.173952</td>\n",
       "      <td>0.171131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26789</th>\n",
       "      <td>0.153271</td>\n",
       "      <td>0.170773</td>\n",
       "      <td>0.156040</td>\n",
       "      <td>0.165188</td>\n",
       "      <td>0.220638</td>\n",
       "      <td>0.210336</td>\n",
       "      <td>0.141243</td>\n",
       "      <td>0.082579</td>\n",
       "      <td>0.239969</td>\n",
       "      <td>0.146290</td>\n",
       "      <td>...</td>\n",
       "      <td>0.167905</td>\n",
       "      <td>0.265258</td>\n",
       "      <td>0.176826</td>\n",
       "      <td>0.141243</td>\n",
       "      <td>0.145448</td>\n",
       "      <td>0.113801</td>\n",
       "      <td>0.156869</td>\n",
       "      <td>0.323403</td>\n",
       "      <td>0.094136</td>\n",
       "      <td>0.163735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26790</th>\n",
       "      <td>0.175984</td>\n",
       "      <td>0.147149</td>\n",
       "      <td>0.123759</td>\n",
       "      <td>0.091272</td>\n",
       "      <td>0.162588</td>\n",
       "      <td>0.114619</td>\n",
       "      <td>0.204330</td>\n",
       "      <td>0.303209</td>\n",
       "      <td>0.133574</td>\n",
       "      <td>0.117232</td>\n",
       "      <td>...</td>\n",
       "      <td>0.147172</td>\n",
       "      <td>0.138259</td>\n",
       "      <td>0.094863</td>\n",
       "      <td>0.204330</td>\n",
       "      <td>0.174593</td>\n",
       "      <td>0.178001</td>\n",
       "      <td>0.105386</td>\n",
       "      <td>0.154212</td>\n",
       "      <td>0.130698</td>\n",
       "      <td>0.162569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26791</th>\n",
       "      <td>0.200187</td>\n",
       "      <td>0.118299</td>\n",
       "      <td>0.135443</td>\n",
       "      <td>0.134407</td>\n",
       "      <td>0.299837</td>\n",
       "      <td>0.437250</td>\n",
       "      <td>0.130165</td>\n",
       "      <td>0.268333</td>\n",
       "      <td>0.118058</td>\n",
       "      <td>0.103653</td>\n",
       "      <td>...</td>\n",
       "      <td>0.179858</td>\n",
       "      <td>0.112467</td>\n",
       "      <td>0.326238</td>\n",
       "      <td>0.130165</td>\n",
       "      <td>0.138908</td>\n",
       "      <td>0.094136</td>\n",
       "      <td>0.160554</td>\n",
       "      <td>0.162852</td>\n",
       "      <td>0.134673</td>\n",
       "      <td>0.208555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26792</th>\n",
       "      <td>0.193623</td>\n",
       "      <td>0.125700</td>\n",
       "      <td>0.114667</td>\n",
       "      <td>0.123992</td>\n",
       "      <td>0.242272</td>\n",
       "      <td>0.171599</td>\n",
       "      <td>0.146382</td>\n",
       "      <td>0.290992</td>\n",
       "      <td>0.203500</td>\n",
       "      <td>0.170135</td>\n",
       "      <td>...</td>\n",
       "      <td>0.183174</td>\n",
       "      <td>0.136447</td>\n",
       "      <td>0.220173</td>\n",
       "      <td>0.146382</td>\n",
       "      <td>0.116545</td>\n",
       "      <td>0.134595</td>\n",
       "      <td>0.130791</td>\n",
       "      <td>0.330328</td>\n",
       "      <td>0.213370</td>\n",
       "      <td>0.183436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26793</th>\n",
       "      <td>0.217560</td>\n",
       "      <td>0.141631</td>\n",
       "      <td>0.110605</td>\n",
       "      <td>0.102667</td>\n",
       "      <td>0.332425</td>\n",
       "      <td>0.109325</td>\n",
       "      <td>0.141239</td>\n",
       "      <td>0.263233</td>\n",
       "      <td>0.216021</td>\n",
       "      <td>0.096883</td>\n",
       "      <td>...</td>\n",
       "      <td>0.251774</td>\n",
       "      <td>0.156645</td>\n",
       "      <td>0.175162</td>\n",
       "      <td>0.141239</td>\n",
       "      <td>0.082604</td>\n",
       "      <td>0.195705</td>\n",
       "      <td>0.124839</td>\n",
       "      <td>0.216916</td>\n",
       "      <td>0.314152</td>\n",
       "      <td>0.190215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26794</th>\n",
       "      <td>0.192752</td>\n",
       "      <td>0.179001</td>\n",
       "      <td>0.107359</td>\n",
       "      <td>0.076345</td>\n",
       "      <td>0.233856</td>\n",
       "      <td>0.165697</td>\n",
       "      <td>0.150676</td>\n",
       "      <td>0.293141</td>\n",
       "      <td>0.144555</td>\n",
       "      <td>0.092808</td>\n",
       "      <td>...</td>\n",
       "      <td>0.197206</td>\n",
       "      <td>0.120111</td>\n",
       "      <td>0.098157</td>\n",
       "      <td>0.150676</td>\n",
       "      <td>0.146476</td>\n",
       "      <td>0.145468</td>\n",
       "      <td>0.166837</td>\n",
       "      <td>0.220492</td>\n",
       "      <td>0.113413</td>\n",
       "      <td>0.211762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26795</th>\n",
       "      <td>0.145941</td>\n",
       "      <td>0.116410</td>\n",
       "      <td>0.121482</td>\n",
       "      <td>0.116305</td>\n",
       "      <td>0.416403</td>\n",
       "      <td>0.239787</td>\n",
       "      <td>0.163990</td>\n",
       "      <td>0.383152</td>\n",
       "      <td>0.228864</td>\n",
       "      <td>0.174561</td>\n",
       "      <td>...</td>\n",
       "      <td>0.173453</td>\n",
       "      <td>0.190797</td>\n",
       "      <td>0.286749</td>\n",
       "      <td>0.163990</td>\n",
       "      <td>0.100538</td>\n",
       "      <td>0.150160</td>\n",
       "      <td>0.192854</td>\n",
       "      <td>0.218293</td>\n",
       "      <td>0.114975</td>\n",
       "      <td>0.175831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26796</th>\n",
       "      <td>0.176643</td>\n",
       "      <td>0.153974</td>\n",
       "      <td>0.128861</td>\n",
       "      <td>0.135111</td>\n",
       "      <td>0.132457</td>\n",
       "      <td>0.211169</td>\n",
       "      <td>0.138486</td>\n",
       "      <td>0.245646</td>\n",
       "      <td>0.115449</td>\n",
       "      <td>0.122427</td>\n",
       "      <td>...</td>\n",
       "      <td>0.180270</td>\n",
       "      <td>0.110076</td>\n",
       "      <td>0.265345</td>\n",
       "      <td>0.138486</td>\n",
       "      <td>0.070314</td>\n",
       "      <td>0.166039</td>\n",
       "      <td>0.154844</td>\n",
       "      <td>0.401948</td>\n",
       "      <td>0.136355</td>\n",
       "      <td>0.226379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26797</th>\n",
       "      <td>0.246680</td>\n",
       "      <td>0.103898</td>\n",
       "      <td>0.131978</td>\n",
       "      <td>0.091032</td>\n",
       "      <td>0.187170</td>\n",
       "      <td>0.246636</td>\n",
       "      <td>0.118944</td>\n",
       "      <td>0.272182</td>\n",
       "      <td>0.087259</td>\n",
       "      <td>0.101034</td>\n",
       "      <td>...</td>\n",
       "      <td>0.196924</td>\n",
       "      <td>0.094490</td>\n",
       "      <td>0.271729</td>\n",
       "      <td>0.118944</td>\n",
       "      <td>0.074871</td>\n",
       "      <td>0.132817</td>\n",
       "      <td>0.174018</td>\n",
       "      <td>0.139583</td>\n",
       "      <td>0.235373</td>\n",
       "      <td>0.179491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26798</th>\n",
       "      <td>0.129300</td>\n",
       "      <td>0.279789</td>\n",
       "      <td>0.200870</td>\n",
       "      <td>0.135411</td>\n",
       "      <td>0.164267</td>\n",
       "      <td>0.103101</td>\n",
       "      <td>0.127574</td>\n",
       "      <td>0.266297</td>\n",
       "      <td>0.138196</td>\n",
       "      <td>0.225427</td>\n",
       "      <td>...</td>\n",
       "      <td>0.157610</td>\n",
       "      <td>0.190376</td>\n",
       "      <td>0.144032</td>\n",
       "      <td>0.127574</td>\n",
       "      <td>0.106472</td>\n",
       "      <td>0.177645</td>\n",
       "      <td>0.135624</td>\n",
       "      <td>0.198115</td>\n",
       "      <td>0.173613</td>\n",
       "      <td>0.183496</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26799 rows × 353 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       1.1.1.1. -- Sample complexity and generalization bounds  \\\n",
       "0      0.146779                                                  \n",
       "1      0.213833                                                  \n",
       "2      0.130861                                                  \n",
       "3      0.120147                                                  \n",
       "4      0.249352                                                  \n",
       "5      0.201564                                                  \n",
       "6      0.177392                                                  \n",
       "7      0.166423                                                  \n",
       "8      0.246698                                                  \n",
       "9      0.192792                                                  \n",
       "10     0.135986                                                  \n",
       "11     0.168721                                                  \n",
       "12     0.195358                                                  \n",
       "13     0.163220                                                  \n",
       "14     0.175418                                                  \n",
       "15     0.151399                                                  \n",
       "16     0.191404                                                  \n",
       "17     0.195649                                                  \n",
       "18     0.135867                                                  \n",
       "19     0.122168                                                  \n",
       "20     0.134351                                                  \n",
       "21     0.134957                                                  \n",
       "22     0.213908                                                  \n",
       "23     0.220279                                                  \n",
       "24     0.228879                                                  \n",
       "25     0.193618                                                  \n",
       "26     0.133686                                                  \n",
       "27     0.141602                                                  \n",
       "28     0.247660                                                  \n",
       "29     0.203364                                                  \n",
       "30     0.117803                                                  \n",
       "31     0.160430                                                  \n",
       "32     0.142067                                                  \n",
       "33     0.157845                                                  \n",
       "34     0.188661                                                  \n",
       "35     0.185877                                                  \n",
       "36     0.238132                                                  \n",
       "37     0.147577                                                  \n",
       "38     0.157992                                                  \n",
       "39     0.235340                                                  \n",
       "40     0.132646                                                  \n",
       "41     0.180093                                                  \n",
       "42     0.201452                                                  \n",
       "43     0.128410                                                  \n",
       "44     0.110213                                                  \n",
       "45     0.246438                                                  \n",
       "46     0.149130                                                  \n",
       "47     0.132723                                                  \n",
       "48     0.147260                                                  \n",
       "49     0.181051                                                  \n",
       "...         ...                                                  \n",
       "26749  0.160436                                                  \n",
       "26750  0.173017                                                  \n",
       "26751  0.141669                                                  \n",
       "26752  0.143975                                                  \n",
       "26753  0.267161                                                  \n",
       "26754  0.212230                                                  \n",
       "26755  0.190822                                                  \n",
       "26756  0.222275                                                  \n",
       "26757  0.256011                                                  \n",
       "26758  0.187088                                                  \n",
       "26759  0.232753                                                  \n",
       "26760  0.154928                                                  \n",
       "26761  0.259607                                                  \n",
       "26762  0.219341                                                  \n",
       "26763  0.177317                                                  \n",
       "26764  0.124515                                                  \n",
       "26765  0.220783                                                  \n",
       "26766  0.136396                                                  \n",
       "26767  0.318088                                                  \n",
       "26768  0.166141                                                  \n",
       "26769  0.121907                                                  \n",
       "26770  0.187508                                                  \n",
       "26771  0.139937                                                  \n",
       "26772  0.130880                                                  \n",
       "26773  0.216040                                                  \n",
       "26774  0.172681                                                  \n",
       "26775  0.193710                                                  \n",
       "26776  0.182415                                                  \n",
       "26777  0.242739                                                  \n",
       "26778  0.176992                                                  \n",
       "26779  0.211940                                                  \n",
       "26780  0.113273                                                  \n",
       "26781  0.163496                                                  \n",
       "26782  0.217029                                                  \n",
       "26783  0.151500                                                  \n",
       "26784  0.181854                                                  \n",
       "26785  0.248903                                                  \n",
       "26786  0.251184                                                  \n",
       "26787  0.196152                                                  \n",
       "26788  0.199358                                                  \n",
       "26789  0.153271                                                  \n",
       "26790  0.175984                                                  \n",
       "26791  0.200187                                                  \n",
       "26792  0.193623                                                  \n",
       "26793  0.217560                                                  \n",
       "26794  0.192752                                                  \n",
       "26795  0.145941                                                  \n",
       "26796  0.176643                                                  \n",
       "26797  0.246680                                                  \n",
       "26798  0.129300                                                  \n",
       "\n",
       "       1.1.1.2. -- Boolean function learning  \\\n",
       "0      0.148864                                \n",
       "1      0.135648                                \n",
       "2      0.212880                                \n",
       "3      0.253489                                \n",
       "4      0.133184                                \n",
       "5      0.227974                                \n",
       "6      0.223245                                \n",
       "7      0.139857                                \n",
       "8      0.250485                                \n",
       "9      0.153208                                \n",
       "10     0.105450                                \n",
       "11     0.174705                                \n",
       "12     0.233843                                \n",
       "13     0.118858                                \n",
       "14     0.246436                                \n",
       "15     0.122786                                \n",
       "16     0.344913                                \n",
       "17     0.207489                                \n",
       "18     0.153368                                \n",
       "19     0.230001                                \n",
       "20     0.139021                                \n",
       "21     0.151309                                \n",
       "22     0.254911                                \n",
       "23     0.117350                                \n",
       "24     0.259057                                \n",
       "25     0.221824                                \n",
       "26     0.231909                                \n",
       "27     0.134178                                \n",
       "28     0.229422                                \n",
       "29     0.111225                                \n",
       "30     0.132331                                \n",
       "31     0.152927                                \n",
       "32     0.123032                                \n",
       "33     0.119515                                \n",
       "34     0.265782                                \n",
       "35     0.211686                                \n",
       "36     0.233418                                \n",
       "37     0.115959                                \n",
       "38     0.304693                                \n",
       "39     0.110541                                \n",
       "40     0.241493                                \n",
       "41     0.135327                                \n",
       "42     0.134947                                \n",
       "43     0.166555                                \n",
       "44     0.131392                                \n",
       "45     0.232479                                \n",
       "46     0.235019                                \n",
       "47     0.235959                                \n",
       "48     0.127488                                \n",
       "49     0.153261                                \n",
       "...         ...                                \n",
       "26749  0.140607                                \n",
       "26750  0.124357                                \n",
       "26751  0.127956                                \n",
       "26752  0.186471                                \n",
       "26753  0.260282                                \n",
       "26754  0.148520                                \n",
       "26755  0.273574                                \n",
       "26756  0.223805                                \n",
       "26757  0.245265                                \n",
       "26758  0.131898                                \n",
       "26759  0.131475                                \n",
       "26760  0.228180                                \n",
       "26761  0.127876                                \n",
       "26762  0.161623                                \n",
       "26763  0.135271                                \n",
       "26764  0.134084                                \n",
       "26765  0.123174                                \n",
       "26766  0.152512                                \n",
       "26767  0.214981                                \n",
       "26768  0.156828                                \n",
       "26769  0.127220                                \n",
       "26770  0.204138                                \n",
       "26771  0.131325                                \n",
       "26772  0.144202                                \n",
       "26773  0.111038                                \n",
       "26774  0.377526                                \n",
       "26775  0.121627                                \n",
       "26776  0.168940                                \n",
       "26777  0.200490                                \n",
       "26778  0.233213                                \n",
       "26779  0.127137                                \n",
       "26780  0.140323                                \n",
       "26781  0.222262                                \n",
       "26782  0.124122                                \n",
       "26783  0.175011                                \n",
       "26784  0.130128                                \n",
       "26785  0.145039                                \n",
       "26786  0.219879                                \n",
       "26787  0.158969                                \n",
       "26788  0.142819                                \n",
       "26789  0.170773                                \n",
       "26790  0.147149                                \n",
       "26791  0.118299                                \n",
       "26792  0.125700                                \n",
       "26793  0.141631                                \n",
       "26794  0.179001                                \n",
       "26795  0.116410                                \n",
       "26796  0.153974                                \n",
       "26797  0.103898                                \n",
       "26798  0.279789                                \n",
       "\n",
       "       1.1.1.3. -- Unsupervised learning and clustering  \\\n",
       "0      0.317016                                           \n",
       "1      0.308386                                           \n",
       "2      0.342364                                           \n",
       "3      0.352651                                           \n",
       "4      0.307590                                           \n",
       "5      0.317781                                           \n",
       "6      0.611876                                           \n",
       "7      0.381959                                           \n",
       "8      0.566290                                           \n",
       "9      0.415652                                           \n",
       "10     0.226756                                           \n",
       "11     0.263490                                           \n",
       "12     0.325127                                           \n",
       "13     0.245648                                           \n",
       "14     0.525437                                           \n",
       "15     0.215473                                           \n",
       "16     0.510264                                           \n",
       "17     0.311234                                           \n",
       "18     0.402756                                           \n",
       "19     0.236107                                           \n",
       "20     0.169989                                           \n",
       "21     0.253615                                           \n",
       "22     0.325243                                           \n",
       "23     0.214311                                           \n",
       "24     0.680968                                           \n",
       "25     0.600851                                           \n",
       "26     0.471001                                           \n",
       "27     0.298836                                           \n",
       "28     0.602581                                           \n",
       "29     0.248625                                           \n",
       "30     0.441256                                           \n",
       "31     0.252626                                           \n",
       "32     0.287412                                           \n",
       "33     0.308348                                           \n",
       "34     0.284563                                           \n",
       "35     0.320170                                           \n",
       "36     0.244176                                           \n",
       "37     0.214349                                           \n",
       "38     0.355942                                           \n",
       "39     0.450521                                           \n",
       "40     0.575941                                           \n",
       "41     0.255467                                           \n",
       "42     0.241145                                           \n",
       "43     0.266935                                           \n",
       "44     0.261214                                           \n",
       "45     0.535876                                           \n",
       "46     0.323453                                           \n",
       "47     0.377313                                           \n",
       "48     0.181270                                           \n",
       "49     0.236841                                           \n",
       "...         ...                                           \n",
       "26749  0.123564                                           \n",
       "26750  0.121314                                           \n",
       "26751  0.122265                                           \n",
       "26752  0.136822                                           \n",
       "26753  0.355809                                           \n",
       "26754  0.138231                                           \n",
       "26755  0.203921                                           \n",
       "26756  0.229527                                           \n",
       "26757  0.127546                                           \n",
       "26758  0.132892                                           \n",
       "26759  0.127788                                           \n",
       "26760  0.118117                                           \n",
       "26761  0.125805                                           \n",
       "26762  0.150086                                           \n",
       "26763  0.130985                                           \n",
       "26764  0.140478                                           \n",
       "26765  0.125267                                           \n",
       "26766  0.143159                                           \n",
       "26767  0.129173                                           \n",
       "26768  0.165081                                           \n",
       "26769  0.123193                                           \n",
       "26770  0.125430                                           \n",
       "26771  0.142922                                           \n",
       "26772  0.093616                                           \n",
       "26773  0.127131                                           \n",
       "26774  0.205346                                           \n",
       "26775  0.137331                                           \n",
       "26776  0.162624                                           \n",
       "26777  0.112401                                           \n",
       "26778  0.258102                                           \n",
       "26779  0.133443                                           \n",
       "26780  0.135302                                           \n",
       "26781  0.163853                                           \n",
       "26782  0.110521                                           \n",
       "26783  0.148327                                           \n",
       "26784  0.101643                                           \n",
       "26785  0.130425                                           \n",
       "26786  0.180528                                           \n",
       "26787  0.107197                                           \n",
       "26788  0.108297                                           \n",
       "26789  0.156040                                           \n",
       "26790  0.123759                                           \n",
       "26791  0.135443                                           \n",
       "26792  0.114667                                           \n",
       "26793  0.110605                                           \n",
       "26794  0.107359                                           \n",
       "26795  0.121482                                           \n",
       "26796  0.128861                                           \n",
       "26797  0.131978                                           \n",
       "26798  0.200870                                           \n",
       "\n",
       "       1.1.1.4.1. -- Support vector machines  \\\n",
       "0      0.136573                                \n",
       "1      0.097235                                \n",
       "2      0.228501                                \n",
       "3      0.149955                                \n",
       "4      0.101416                                \n",
       "5      0.208838                                \n",
       "6      0.151325                                \n",
       "7      0.176999                                \n",
       "8      0.218441                                \n",
       "9      0.111682                                \n",
       "10     0.369367                                \n",
       "11     0.132206                                \n",
       "12     0.111762                                \n",
       "13     0.097568                                \n",
       "14     0.208777                                \n",
       "15     0.139142                                \n",
       "16     0.193075                                \n",
       "17     0.129356                                \n",
       "18     0.170614                                \n",
       "19     0.150027                                \n",
       "20     0.110179                                \n",
       "21     0.126806                                \n",
       "22     0.190596                                \n",
       "23     0.149279                                \n",
       "24     0.123696                                \n",
       "25     0.115657                                \n",
       "26     0.119695                                \n",
       "27     0.107665                                \n",
       "28     0.088050                                \n",
       "29     0.139806                                \n",
       "30     0.106516                                \n",
       "31     0.185120                                \n",
       "32     0.108177                                \n",
       "33     0.129062                                \n",
       "34     0.116114                                \n",
       "35     0.116547                                \n",
       "36     0.136379                                \n",
       "37     0.118533                                \n",
       "38     0.122546                                \n",
       "39     0.134559                                \n",
       "40     0.232110                                \n",
       "41     0.127227                                \n",
       "42     0.140372                                \n",
       "43     0.130848                                \n",
       "44     0.106889                                \n",
       "45     0.181813                                \n",
       "46     0.091682                                \n",
       "47     0.180984                                \n",
       "48     0.079728                                \n",
       "49     0.178641                                \n",
       "...         ...                                \n",
       "26749  0.121316                                \n",
       "26750  0.153231                                \n",
       "26751  0.205780                                \n",
       "26752  0.124764                                \n",
       "26753  0.161382                                \n",
       "26754  0.208016                                \n",
       "26755  0.136178                                \n",
       "26756  0.102681                                \n",
       "26757  0.207967                                \n",
       "26758  0.138632                                \n",
       "26759  0.112111                                \n",
       "26760  0.157885                                \n",
       "26761  0.099223                                \n",
       "26762  0.182597                                \n",
       "26763  0.152720                                \n",
       "26764  0.154302                                \n",
       "26765  0.146882                                \n",
       "26766  0.142415                                \n",
       "26767  0.122056                                \n",
       "26768  0.210254                                \n",
       "26769  0.131297                                \n",
       "26770  0.126233                                \n",
       "26771  0.144010                                \n",
       "26772  0.107273                                \n",
       "26773  0.137784                                \n",
       "26774  0.146418                                \n",
       "26775  0.267369                                \n",
       "26776  0.093713                                \n",
       "26777  0.162118                                \n",
       "26778  0.172943                                \n",
       "26779  0.109251                                \n",
       "26780  0.103194                                \n",
       "26781  0.150330                                \n",
       "26782  0.214731                                \n",
       "26783  0.163581                                \n",
       "26784  0.123532                                \n",
       "26785  0.209926                                \n",
       "26786  0.310471                                \n",
       "26787  0.113264                                \n",
       "26788  0.179327                                \n",
       "26789  0.165188                                \n",
       "26790  0.091272                                \n",
       "26791  0.134407                                \n",
       "26792  0.123992                                \n",
       "26793  0.102667                                \n",
       "26794  0.076345                                \n",
       "26795  0.116305                                \n",
       "26796  0.135111                                \n",
       "26797  0.091032                                \n",
       "26798  0.135411                                \n",
       "\n",
       "       1.1.1.4.2. -- Gaussian processes  1.1.1.4.3. -- Modelling  \\\n",
       "0      0.155241                          0.137437                  \n",
       "1      0.163988                          0.111039                  \n",
       "2      0.131850                          0.151590                  \n",
       "3      0.232027                          0.246658                  \n",
       "4      0.140609                          0.099105                  \n",
       "5      0.115182                          0.236364                  \n",
       "6      0.138115                          0.114294                  \n",
       "7      0.107430                          0.117128                  \n",
       "8      0.134450                          0.118486                  \n",
       "9      0.206287                          0.107320                  \n",
       "10     0.139522                          0.086130                  \n",
       "11     0.158296                          0.274150                  \n",
       "12     0.094158                          0.182654                  \n",
       "13     0.137857                          0.112043                  \n",
       "14     0.223494                          0.135055                  \n",
       "15     0.136476                          0.118576                  \n",
       "16     0.102839                          0.153148                  \n",
       "17     0.330538                          0.134019                  \n",
       "18     0.207534                          0.134563                  \n",
       "19     0.131785                          0.126220                  \n",
       "20     0.188611                          0.144647                  \n",
       "21     0.125641                          0.155071                  \n",
       "22     0.212038                          0.083775                  \n",
       "23     0.198892                          0.188208                  \n",
       "24     0.187983                          0.236526                  \n",
       "25     0.297298                          0.219365                  \n",
       "26     0.136614                          0.111284                  \n",
       "27     0.267763                          0.137925                  \n",
       "28     0.137708                          0.259136                  \n",
       "29     0.130718                          0.109459                  \n",
       "30     0.112609                          0.095599                  \n",
       "31     0.172182                          0.177898                  \n",
       "32     0.110873                          0.168238                  \n",
       "33     0.122348                          0.137723                  \n",
       "34     0.103378                          0.128445                  \n",
       "35     0.114868                          0.118110                  \n",
       "36     0.199970                          0.248987                  \n",
       "37     0.119846                          0.166594                  \n",
       "38     0.105168                          0.183691                  \n",
       "39     0.097764                          0.256432                  \n",
       "40     0.128954                          0.192955                  \n",
       "41     0.129886                          0.122549                  \n",
       "42     0.139743                          0.092075                  \n",
       "43     0.121671                          0.100950                  \n",
       "44     0.109817                          0.112219                  \n",
       "45     0.183480                          0.121158                  \n",
       "46     0.122775                          0.088954                  \n",
       "47     0.185398                          0.103797                  \n",
       "48     0.098014                          0.164212                  \n",
       "49     0.092876                          0.136989                  \n",
       "...         ...                               ...                  \n",
       "26749  0.147534                          0.149459                  \n",
       "26750  0.137259                          0.079218                  \n",
       "26751  0.162651                          0.382500                  \n",
       "26752  0.139127                          0.191201                  \n",
       "26753  0.129316                          0.182350                  \n",
       "26754  0.124658                          0.246744                  \n",
       "26755  0.238640                          0.244811                  \n",
       "26756  0.177806                          0.121785                  \n",
       "26757  0.263836                          0.400749                  \n",
       "26758  0.163746                          0.252380                  \n",
       "26759  0.114041                          0.147279                  \n",
       "26760  0.117910                          0.127771                  \n",
       "26761  0.198447                          0.124315                  \n",
       "26762  0.176313                          0.403695                  \n",
       "26763  0.123628                          0.154642                  \n",
       "26764  0.185171                          0.112114                  \n",
       "26765  0.107390                          0.256031                  \n",
       "26766  0.136935                          0.126615                  \n",
       "26767  0.138520                          0.221515                  \n",
       "26768  0.197398                          0.273639                  \n",
       "26769  0.171650                          0.081270                  \n",
       "26770  0.391373                          0.256350                  \n",
       "26771  0.269953                          0.255807                  \n",
       "26772  0.156020                          0.218315                  \n",
       "26773  0.198300                          0.273922                  \n",
       "26774  0.142063                          0.213606                  \n",
       "26775  0.182466                          0.158179                  \n",
       "26776  0.146890                          0.099770                  \n",
       "26777  0.100955                          0.131281                  \n",
       "26778  0.284136                          0.241065                  \n",
       "26779  0.223102                          0.164927                  \n",
       "26780  0.508130                          0.395414                  \n",
       "26781  0.219140                          0.216911                  \n",
       "26782  0.137925                          0.253825                  \n",
       "26783  0.347919                          0.233945                  \n",
       "26784  0.166406                          0.217472                  \n",
       "26785  0.163252                          0.135203                  \n",
       "26786  0.119011                          0.097076                  \n",
       "26787  0.220958                          0.181141                  \n",
       "26788  0.208955                          0.195545                  \n",
       "26789  0.220638                          0.210336                  \n",
       "26790  0.162588                          0.114619                  \n",
       "26791  0.299837                          0.437250                  \n",
       "26792  0.242272                          0.171599                  \n",
       "26793  0.332425                          0.109325                  \n",
       "26794  0.233856                          0.165697                  \n",
       "26795  0.416403                          0.239787                  \n",
       "26796  0.132457                          0.211169                  \n",
       "26797  0.187170                          0.246636                  \n",
       "26798  0.164267                          0.103101                  \n",
       "\n",
       "       1.1.1.5. -- Boosting  1.1.1.6. -- Bayesian analysis  \\\n",
       "0      0.160465              0.281274                        \n",
       "1      0.176288              0.231717                        \n",
       "2      0.157596              0.081035                        \n",
       "3      0.161025              0.102643                        \n",
       "4      0.104585              0.148708                        \n",
       "5      0.123501              0.087960                        \n",
       "6      0.165034              0.117744                        \n",
       "7      0.183612              0.122425                        \n",
       "8      0.096192              0.256354                        \n",
       "9      0.187159              0.109417                        \n",
       "10     0.168194              0.102082                        \n",
       "11     0.176654              0.070442                        \n",
       "12     0.162446              0.119589                        \n",
       "13     0.125598              0.097123                        \n",
       "14     0.130399              0.074453                        \n",
       "15     0.103336              0.129034                        \n",
       "16     0.153108              0.075301                        \n",
       "17     0.164722              0.238944                        \n",
       "18     0.168743              0.111045                        \n",
       "19     0.129434              0.092483                        \n",
       "20     0.143418              0.261870                        \n",
       "21     0.161641              0.235585                        \n",
       "22     0.107922              0.140835                        \n",
       "23     0.141191              0.150937                        \n",
       "24     0.181752              0.080494                        \n",
       "25     0.117906              0.255824                        \n",
       "26     0.114040              0.248690                        \n",
       "27     0.178498              0.111945                        \n",
       "28     0.198557              0.124551                        \n",
       "29     0.129832              0.076968                        \n",
       "30     0.130195              0.082792                        \n",
       "31     0.176718              0.234021                        \n",
       "32     0.114671              0.088035                        \n",
       "33     0.151636              0.115925                        \n",
       "34     0.180198              0.124389                        \n",
       "35     0.150766              0.088832                        \n",
       "36     0.184922              0.280055                        \n",
       "37     0.089290              0.077847                        \n",
       "38     0.130373              0.131000                        \n",
       "39     0.168198              0.118216                        \n",
       "40     0.146430              0.220790                        \n",
       "41     0.129057              0.094189                        \n",
       "42     0.087806              0.239577                        \n",
       "43     0.150879              0.107978                        \n",
       "44     0.125396              0.264874                        \n",
       "45     0.117238              0.091852                        \n",
       "46     0.121894              0.091215                        \n",
       "47     0.111535              0.101712                        \n",
       "48     0.151197              0.129921                        \n",
       "49     0.130698              0.101730                        \n",
       "...         ...                   ...                        \n",
       "26749  0.122764              0.085552                        \n",
       "26750  0.110055              0.086907                        \n",
       "26751  0.094335              0.335057                        \n",
       "26752  0.123898              0.105100                        \n",
       "26753  0.139541              0.237930                        \n",
       "26754  0.159450              0.131943                        \n",
       "26755  0.134228              0.271883                        \n",
       "26756  0.138506              0.435683                        \n",
       "26757  0.135215              0.550912                        \n",
       "26758  0.124024              0.394997                        \n",
       "26759  0.126277              0.224112                        \n",
       "26760  0.120376              0.126281                        \n",
       "26761  0.134269              0.119608                        \n",
       "26762  0.130704              0.296908                        \n",
       "26763  0.112467              0.264784                        \n",
       "26764  0.136347              0.105497                        \n",
       "26765  0.183569              0.377140                        \n",
       "26766  0.098275              0.111090                        \n",
       "26767  0.096435              0.297090                        \n",
       "26768  0.156441              0.088113                        \n",
       "26769  0.131754              0.259684                        \n",
       "26770  0.183450              0.322342                        \n",
       "26771  0.163349              0.295996                        \n",
       "26772  0.098107              0.278263                        \n",
       "26773  0.110547              0.275026                        \n",
       "26774  0.099553              0.267805                        \n",
       "26775  0.132073              0.082947                        \n",
       "26776  0.133666              0.294795                        \n",
       "26777  0.084282              0.091667                        \n",
       "26778  0.129154              0.242416                        \n",
       "26779  0.138301              0.170618                        \n",
       "26780  0.130022              0.374518                        \n",
       "26781  0.174080              0.567885                        \n",
       "26782  0.170114              0.378621                        \n",
       "26783  0.131125              0.434251                        \n",
       "26784  0.143757              0.440973                        \n",
       "26785  0.133940              0.542126                        \n",
       "26786  0.146399              0.219075                        \n",
       "26787  0.143052              0.252064                        \n",
       "26788  0.120023              0.304650                        \n",
       "26789  0.141243              0.082579                        \n",
       "26790  0.204330              0.303209                        \n",
       "26791  0.130165              0.268333                        \n",
       "26792  0.146382              0.290992                        \n",
       "26793  0.141239              0.263233                        \n",
       "26794  0.150676              0.293141                        \n",
       "26795  0.163990              0.383152                        \n",
       "26796  0.138486              0.245646                        \n",
       "26797  0.118944              0.272182                        \n",
       "26798  0.127574              0.266297                        \n",
       "\n",
       "       1.1.1.7. -- Inductive inference  1.1.1.8. -- Online learning theory  \\\n",
       "0      0.246067                         0.158224                             \n",
       "1      0.216278                         0.109494                             \n",
       "2      0.145126                         0.226929                             \n",
       "3      0.221068                         0.288539                             \n",
       "4      0.282816                         0.104701                             \n",
       "5      0.198322                         0.236425                             \n",
       "6      0.157261                         0.228703                             \n",
       "7      0.245228                         0.167854                             \n",
       "8      0.206387                         0.369564                             \n",
       "9      0.204001                         0.116916                             \n",
       "10     0.180081                         0.102798                             \n",
       "11     0.166324                         0.124020                             \n",
       "12     0.186321                         0.100320                             \n",
       "13     0.203642                         0.108509                             \n",
       "14     0.204030                         0.329863                             \n",
       "15     0.212740                         0.109596                             \n",
       "16     0.206735                         0.296324                             \n",
       "17     0.213387                         0.132623                             \n",
       "18     0.214334                         0.136066                             \n",
       "19     0.147118                         0.119895                             \n",
       "20     0.236809                         0.124528                             \n",
       "21     0.221897                         0.144211                             \n",
       "22     0.147311                         0.112402                             \n",
       "23     0.115100                         0.093102                             \n",
       "24     0.206372                         0.237183                             \n",
       "25     0.147003                         0.227354                             \n",
       "26     0.265539                         0.179151                             \n",
       "27     0.225159                         0.127565                             \n",
       "28     0.167423                         0.230288                             \n",
       "29     0.256682                         0.108679                             \n",
       "30     0.246457                         0.092820                             \n",
       "31     0.239307                         0.115349                             \n",
       "32     0.166622                         0.119443                             \n",
       "33     0.155132                         0.103822                             \n",
       "34     0.179644                         0.232692                             \n",
       "35     0.202079                         0.109146                             \n",
       "36     0.322700                         0.119790                             \n",
       "37     0.155134                         0.130066                             \n",
       "38     0.221999                         0.215578                             \n",
       "39     0.210438                         0.153485                             \n",
       "40     0.205782                         0.329546                             \n",
       "41     0.210291                         0.170511                             \n",
       "42     0.176349                         0.114198                             \n",
       "43     0.196925                         0.144598                             \n",
       "44     0.156368                         0.175625                             \n",
       "45     0.168169                         0.215581                             \n",
       "46     0.155372                         0.239031                             \n",
       "47     0.228005                         0.311384                             \n",
       "48     0.174140                         0.121081                             \n",
       "49     0.183492                         0.125405                             \n",
       "...         ...                              ...                             \n",
       "26749  0.177913                         0.192557                             \n",
       "26750  0.192796                         0.095866                             \n",
       "26751  0.176286                         0.163318                             \n",
       "26752  0.215217                         0.188477                             \n",
       "26753  0.329874                         0.230868                             \n",
       "26754  0.375505                         0.134976                             \n",
       "26755  0.207566                         0.243639                             \n",
       "26756  0.220892                         0.162628                             \n",
       "26757  0.299036                         0.190023                             \n",
       "26758  0.139384                         0.111958                             \n",
       "26759  0.155178                         0.156875                             \n",
       "26760  0.193876                         0.193420                             \n",
       "26761  0.137111                         0.122757                             \n",
       "26762  0.488085                         0.114246                             \n",
       "26763  0.369404                         0.142279                             \n",
       "26764  0.301475                         0.104429                             \n",
       "26765  0.153990                         0.104049                             \n",
       "26766  0.138746                         0.118680                             \n",
       "26767  0.303401                         0.170058                             \n",
       "26768  0.274875                         0.139165                             \n",
       "26769  0.252153                         0.098927                             \n",
       "26770  0.348512                         0.106761                             \n",
       "26771  0.167834                         0.112172                             \n",
       "26772  0.166644                         0.108896                             \n",
       "26773  0.169254                         0.158967                             \n",
       "26774  0.186847                         0.256838                             \n",
       "26775  0.222631                         0.125362                             \n",
       "26776  0.287017                         0.128317                             \n",
       "26777  0.190928                         0.137669                             \n",
       "26778  0.180258                         0.266003                             \n",
       "26779  0.165602                         0.105173                             \n",
       "26780  0.294936                         0.160086                             \n",
       "26781  0.343381                         0.119055                             \n",
       "26782  0.307349                         0.100551                             \n",
       "26783  0.217858                         0.213223                             \n",
       "26784  0.389462                         0.085107                             \n",
       "26785  0.162357                         0.105070                             \n",
       "26786  0.216761                         0.270904                             \n",
       "26787  0.184105                         0.122627                             \n",
       "26788  0.174369                         0.134595                             \n",
       "26789  0.239969                         0.146290                             \n",
       "26790  0.133574                         0.117232                             \n",
       "26791  0.118058                         0.103653                             \n",
       "26792  0.203500                         0.170135                             \n",
       "26793  0.216021                         0.096883                             \n",
       "26794  0.144555                         0.092808                             \n",
       "26795  0.228864                         0.174561                             \n",
       "26796  0.115449                         0.122427                             \n",
       "26797  0.087259                         0.101034                             \n",
       "26798  0.138196                         0.225427                             \n",
       "\n",
       "       ...  5.2.4.1.3. -- Policy iteration  \\\n",
       "0      ...  0.189453                         \n",
       "1      ...  0.218900                         \n",
       "2      ...  0.208125                         \n",
       "3      ...  0.171006                         \n",
       "4      ...  0.176652                         \n",
       "5      ...  0.182536                         \n",
       "6      ...  0.203409                         \n",
       "7      ...  0.161591                         \n",
       "8      ...  0.178881                         \n",
       "9      ...  0.198469                         \n",
       "10     ...  0.215746                         \n",
       "11     ...  0.216061                         \n",
       "12     ...  0.308007                         \n",
       "13     ...  0.166184                         \n",
       "14     ...  0.176521                         \n",
       "15     ...  0.245126                         \n",
       "16     ...  0.178410                         \n",
       "17     ...  0.290108                         \n",
       "18     ...  0.156906                         \n",
       "19     ...  0.168294                         \n",
       "20     ...  0.232531                         \n",
       "21     ...  0.167089                         \n",
       "22     ...  0.209780                         \n",
       "23     ...  0.204554                         \n",
       "24     ...  0.214547                         \n",
       "25     ...  0.218256                         \n",
       "26     ...  0.197023                         \n",
       "27     ...  0.162192                         \n",
       "28     ...  0.176187                         \n",
       "29     ...  0.215523                         \n",
       "30     ...  0.204261                         \n",
       "31     ...  0.196683                         \n",
       "32     ...  0.178207                         \n",
       "33     ...  0.234224                         \n",
       "34     ...  0.249551                         \n",
       "35     ...  0.238250                         \n",
       "36     ...  0.184627                         \n",
       "37     ...  0.167720                         \n",
       "38     ...  0.242434                         \n",
       "39     ...  0.172566                         \n",
       "40     ...  0.175176                         \n",
       "41     ...  0.189844                         \n",
       "42     ...  0.166476                         \n",
       "43     ...  0.180315                         \n",
       "44     ...  0.157375                         \n",
       "45     ...  0.306549                         \n",
       "46     ...  0.186062                         \n",
       "47     ...  0.174160                         \n",
       "48     ...  0.255022                         \n",
       "49     ...  0.258043                         \n",
       "...    ...       ...                         \n",
       "26749  ...  0.176951                         \n",
       "26750  ...  0.178551                         \n",
       "26751  ...  0.222313                         \n",
       "26752  ...  0.175541                         \n",
       "26753  ...  0.210642                         \n",
       "26754  ...  0.159908                         \n",
       "26755  ...  0.202985                         \n",
       "26756  ...  0.181194                         \n",
       "26757  ...  0.219098                         \n",
       "26758  ...  0.224619                         \n",
       "26759  ...  0.225331                         \n",
       "26760  ...  0.159401                         \n",
       "26761  ...  0.225439                         \n",
       "26762  ...  0.201703                         \n",
       "26763  ...  0.188764                         \n",
       "26764  ...  0.192671                         \n",
       "26765  ...  0.161963                         \n",
       "26766  ...  0.212482                         \n",
       "26767  ...  0.177720                         \n",
       "26768  ...  0.229152                         \n",
       "26769  ...  0.204814                         \n",
       "26770  ...  0.277271                         \n",
       "26771  ...  0.154729                         \n",
       "26772  ...  0.194466                         \n",
       "26773  ...  0.144336                         \n",
       "26774  ...  0.186749                         \n",
       "26775  ...  0.210515                         \n",
       "26776  ...  0.199943                         \n",
       "26777  ...  0.161906                         \n",
       "26778  ...  0.295705                         \n",
       "26779  ...  0.259832                         \n",
       "26780  ...  0.247176                         \n",
       "26781  ...  0.158221                         \n",
       "26782  ...  0.213889                         \n",
       "26783  ...  0.177949                         \n",
       "26784  ...  0.152678                         \n",
       "26785  ...  0.169298                         \n",
       "26786  ...  0.265979                         \n",
       "26787  ...  0.218689                         \n",
       "26788  ...  0.263371                         \n",
       "26789  ...  0.167905                         \n",
       "26790  ...  0.147172                         \n",
       "26791  ...  0.179858                         \n",
       "26792  ...  0.183174                         \n",
       "26793  ...  0.251774                         \n",
       "26794  ...  0.197206                         \n",
       "26795  ...  0.173453                         \n",
       "26796  ...  0.180270                         \n",
       "26797  ...  0.196924                         \n",
       "26798  ...  0.157610                         \n",
       "\n",
       "       5.2.4.1.4. -- Temporal difference learning  \\\n",
       "0      0.199818                                     \n",
       "1      0.210582                                     \n",
       "2      0.223977                                     \n",
       "3      0.247210                                     \n",
       "4      0.323895                                     \n",
       "5      0.223695                                     \n",
       "6      0.350850                                     \n",
       "7      0.252487                                     \n",
       "8      0.322083                                     \n",
       "9      0.238937                                     \n",
       "10     0.152046                                     \n",
       "11     0.219450                                     \n",
       "12     0.109550                                     \n",
       "13     0.177200                                     \n",
       "14     0.338880                                     \n",
       "15     0.140396                                     \n",
       "16     0.241845                                     \n",
       "17     0.214655                                     \n",
       "18     0.212365                                     \n",
       "19     0.117355                                     \n",
       "20     0.228731                                     \n",
       "21     0.150369                                     \n",
       "22     0.168944                                     \n",
       "23     0.108339                                     \n",
       "24     0.304053                                     \n",
       "25     0.303102                                     \n",
       "26     0.215691                                     \n",
       "27     0.221036                                     \n",
       "28     0.217299                                     \n",
       "29     0.232570                                     \n",
       "30     0.128964                                     \n",
       "31     0.223627                                     \n",
       "32     0.211274                                     \n",
       "33     0.119027                                     \n",
       "34     0.165761                                     \n",
       "35     0.200088                                     \n",
       "36     0.187121                                     \n",
       "37     0.099432                                     \n",
       "38     0.323302                                     \n",
       "39     0.197608                                     \n",
       "40     0.347181                                     \n",
       "41     0.148207                                     \n",
       "42     0.117420                                     \n",
       "43     0.234844                                     \n",
       "44     0.143812                                     \n",
       "45     0.208302                                     \n",
       "46     0.234409                                     \n",
       "47     0.315993                                     \n",
       "48     0.186838                                     \n",
       "49     0.110183                                     \n",
       "...         ...                                     \n",
       "26749  0.134045                                     \n",
       "26750  0.220209                                     \n",
       "26751  0.133544                                     \n",
       "26752  0.120678                                     \n",
       "26753  0.275226                                     \n",
       "26754  0.251483                                     \n",
       "26755  0.406173                                     \n",
       "26756  0.120527                                     \n",
       "26757  0.263495                                     \n",
       "26758  0.120316                                     \n",
       "26759  0.153588                                     \n",
       "26760  0.217892                                     \n",
       "26761  0.119203                                     \n",
       "26762  0.203187                                     \n",
       "26763  0.223480                                     \n",
       "26764  0.125509                                     \n",
       "26765  0.116511                                     \n",
       "26766  0.117832                                     \n",
       "26767  0.187417                                     \n",
       "26768  0.256804                                     \n",
       "26769  0.285868                                     \n",
       "26770  0.261232                                     \n",
       "26771  0.127456                                     \n",
       "26772  0.113732                                     \n",
       "26773  0.110356                                     \n",
       "26774  0.234117                                     \n",
       "26775  0.144695                                     \n",
       "26776  0.198973                                     \n",
       "26777  0.167596                                     \n",
       "26778  0.238098                                     \n",
       "26779  0.100004                                     \n",
       "26780  0.229998                                     \n",
       "26781  0.213885                                     \n",
       "26782  0.242607                                     \n",
       "26783  0.129530                                     \n",
       "26784  0.171483                                     \n",
       "26785  0.140332                                     \n",
       "26786  0.211008                                     \n",
       "26787  0.259427                                     \n",
       "26788  0.135871                                     \n",
       "26789  0.265258                                     \n",
       "26790  0.138259                                     \n",
       "26791  0.112467                                     \n",
       "26792  0.136447                                     \n",
       "26793  0.156645                                     \n",
       "26794  0.120111                                     \n",
       "26795  0.190797                                     \n",
       "26796  0.110076                                     \n",
       "26797  0.094490                                     \n",
       "26798  0.190376                                     \n",
       "\n",
       "       5.2.4.1.5. -- Approximate dynamic programming methods  \\\n",
       "0      0.207490                                                \n",
       "1      0.158816                                                \n",
       "2      0.127286                                                \n",
       "3      0.117086                                                \n",
       "4      0.161467                                                \n",
       "5      0.160219                                                \n",
       "6      0.235056                                                \n",
       "7      0.162493                                                \n",
       "8      0.165166                                                \n",
       "9      0.211841                                                \n",
       "10     0.133940                                                \n",
       "11     0.148728                                                \n",
       "12     0.201779                                                \n",
       "13     0.195906                                                \n",
       "14     0.192734                                                \n",
       "15     0.137815                                                \n",
       "16     0.187689                                                \n",
       "17     0.135003                                                \n",
       "18     0.136634                                                \n",
       "19     0.162667                                                \n",
       "20     0.177223                                                \n",
       "21     0.228402                                                \n",
       "22     0.164101                                                \n",
       "23     0.177862                                                \n",
       "24     0.111496                                                \n",
       "25     0.211689                                                \n",
       "26     0.160045                                                \n",
       "27     0.106670                                                \n",
       "28     0.136309                                                \n",
       "29     0.112012                                                \n",
       "30     0.196279                                                \n",
       "31     0.156862                                                \n",
       "32     0.127121                                                \n",
       "33     0.181260                                                \n",
       "34     0.198941                                                \n",
       "35     0.199106                                                \n",
       "36     0.268068                                                \n",
       "37     0.166651                                                \n",
       "38     0.166690                                                \n",
       "39     0.309507                                                \n",
       "40     0.125304                                                \n",
       "41     0.175219                                                \n",
       "42     0.162774                                                \n",
       "43     0.226106                                                \n",
       "44     0.167725                                                \n",
       "45     0.177766                                                \n",
       "46     0.197827                                                \n",
       "47     0.277180                                                \n",
       "48     0.228311                                                \n",
       "49     0.127197                                                \n",
       "...         ...                                                \n",
       "26749  0.141297                                                \n",
       "26750  0.121238                                                \n",
       "26751  0.155535                                                \n",
       "26752  0.246534                                                \n",
       "26753  0.161843                                                \n",
       "26754  0.169436                                                \n",
       "26755  0.140143                                                \n",
       "26756  0.225839                                                \n",
       "26757  0.277726                                                \n",
       "26758  0.207390                                                \n",
       "26759  0.140628                                                \n",
       "26760  0.163767                                                \n",
       "26761  0.132112                                                \n",
       "26762  0.180501                                                \n",
       "26763  0.210016                                                \n",
       "26764  0.147539                                                \n",
       "26765  0.170236                                                \n",
       "26766  0.107394                                                \n",
       "26767  0.163681                                                \n",
       "26768  0.322030                                                \n",
       "26769  0.176790                                                \n",
       "26770  0.141170                                                \n",
       "26771  0.120069                                                \n",
       "26772  0.151458                                                \n",
       "26773  0.149517                                                \n",
       "26774  0.128084                                                \n",
       "26775  0.152873                                                \n",
       "26776  0.088016                                                \n",
       "26777  0.114626                                                \n",
       "26778  0.252016                                                \n",
       "26779  0.335955                                                \n",
       "26780  0.177006                                                \n",
       "26781  0.192420                                                \n",
       "26782  0.098356                                                \n",
       "26783  0.127927                                                \n",
       "26784  0.129954                                                \n",
       "26785  0.197174                                                \n",
       "26786  0.121309                                                \n",
       "26787  0.233994                                                \n",
       "26788  0.350634                                                \n",
       "26789  0.176826                                                \n",
       "26790  0.094863                                                \n",
       "26791  0.326238                                                \n",
       "26792  0.220173                                                \n",
       "26793  0.175162                                                \n",
       "26794  0.098157                                                \n",
       "26795  0.286749                                                \n",
       "26796  0.265345                                                \n",
       "26797  0.271729                                                \n",
       "26798  0.144032                                                \n",
       "\n",
       "       5.2.4.2.1. -- Boosting  5.2.4.2.2. -- Bagging  \\\n",
       "0      0.160465                0.178204                \n",
       "1      0.176288                0.106835                \n",
       "2      0.157596                0.155013                \n",
       "3      0.161025                0.101487                \n",
       "4      0.104585                0.110777                \n",
       "5      0.123501                0.088216                \n",
       "6      0.165034                0.168134                \n",
       "7      0.183612                0.111283                \n",
       "8      0.096192                0.085929                \n",
       "9      0.187159                0.164249                \n",
       "10     0.168194                0.086762                \n",
       "11     0.176654                0.115428                \n",
       "12     0.162446                0.119663                \n",
       "13     0.125598                0.095763                \n",
       "14     0.130399                0.187886                \n",
       "15     0.103336                0.082643                \n",
       "16     0.153108                0.176220                \n",
       "17     0.164722                0.136314                \n",
       "18     0.168743                0.187027                \n",
       "19     0.129434                0.099680                \n",
       "20     0.143418                0.084541                \n",
       "21     0.161641                0.191361                \n",
       "22     0.107922                0.131834                \n",
       "23     0.141191                0.112971                \n",
       "24     0.181752                0.111095                \n",
       "25     0.117906                0.116746                \n",
       "26     0.114040                0.194472                \n",
       "27     0.178498                0.112945                \n",
       "28     0.198557                0.121793                \n",
       "29     0.129832                0.093949                \n",
       "30     0.130195                0.149092                \n",
       "31     0.176718                0.166922                \n",
       "32     0.114671                0.109226                \n",
       "33     0.151636                0.153913                \n",
       "34     0.180198                0.119450                \n",
       "35     0.150766                0.182286                \n",
       "36     0.184922                0.137070                \n",
       "37     0.089290                0.080125                \n",
       "38     0.130373                0.093667                \n",
       "39     0.168198                0.180388                \n",
       "40     0.146430                0.107308                \n",
       "41     0.129057                0.108649                \n",
       "42     0.087806                0.082716                \n",
       "43     0.150879                0.162903                \n",
       "44     0.125396                0.096824                \n",
       "45     0.117238                0.185085                \n",
       "46     0.121894                0.078992                \n",
       "47     0.111535                0.120086                \n",
       "48     0.151197                0.078762                \n",
       "49     0.130698                0.128632                \n",
       "...         ...                     ...                \n",
       "26749  0.122764                0.130524                \n",
       "26750  0.110055                0.274031                \n",
       "26751  0.094335                0.107299                \n",
       "26752  0.123898                0.138415                \n",
       "26753  0.139541                0.142964                \n",
       "26754  0.159450                0.132638                \n",
       "26755  0.134228                0.110770                \n",
       "26756  0.138506                0.111494                \n",
       "26757  0.135215                0.128291                \n",
       "26758  0.124024                0.093529                \n",
       "26759  0.126277                0.139262                \n",
       "26760  0.120376                0.276424                \n",
       "26761  0.134269                0.099086                \n",
       "26762  0.130704                0.119369                \n",
       "26763  0.112467                0.153925                \n",
       "26764  0.136347                0.153267                \n",
       "26765  0.183569                0.095002                \n",
       "26766  0.098275                0.098371                \n",
       "26767  0.096435                0.122765                \n",
       "26768  0.156441                0.152301                \n",
       "26769  0.131754                0.092409                \n",
       "26770  0.183450                0.133196                \n",
       "26771  0.163349                0.140542                \n",
       "26772  0.098107                0.092334                \n",
       "26773  0.110547                0.085981                \n",
       "26774  0.099553                0.109536                \n",
       "26775  0.132073                0.111285                \n",
       "26776  0.133666                0.097419                \n",
       "26777  0.084282                0.062644                \n",
       "26778  0.129154                0.158971                \n",
       "26779  0.138301                0.109524                \n",
       "26780  0.130022                0.119029                \n",
       "26781  0.174080                0.096193                \n",
       "26782  0.170114                0.111423                \n",
       "26783  0.131125                0.162611                \n",
       "26784  0.143757                0.097627                \n",
       "26785  0.133940                0.173553                \n",
       "26786  0.146399                0.096288                \n",
       "26787  0.143052                0.133685                \n",
       "26788  0.120023                0.092221                \n",
       "26789  0.141243                0.145448                \n",
       "26790  0.204330                0.174593                \n",
       "26791  0.130165                0.138908                \n",
       "26792  0.146382                0.116545                \n",
       "26793  0.141239                0.082604                \n",
       "26794  0.150676                0.146476                \n",
       "26795  0.163990                0.100538                \n",
       "26796  0.138486                0.070314                \n",
       "26797  0.118944                0.074871                \n",
       "26798  0.127574                0.106472                \n",
       "\n",
       "       5.2.4.2.3. -- Fusion of classifiers  5.2.4.3.1 -- Spectral clustering  \\\n",
       "0      0.119531                             0.329988                           \n",
       "1      0.148433                             0.318810                           \n",
       "2      0.269969                             0.433328                           \n",
       "3      0.139135                             0.357909                           \n",
       "4      0.119197                             0.403607                           \n",
       "5      0.163189                             0.325475                           \n",
       "6      0.135912                             0.407228                           \n",
       "7      0.139458                             0.451132                           \n",
       "8      0.268450                             0.303687                           \n",
       "9      0.286287                             0.417164                           \n",
       "10     0.163617                             0.346939                           \n",
       "11     0.138635                             0.329766                           \n",
       "12     0.157321                             0.309173                           \n",
       "13     0.286757                             0.311718                           \n",
       "14     0.123700                             0.361486                           \n",
       "15     0.112091                             0.295773                           \n",
       "16     0.136876                             0.471974                           \n",
       "17     0.131519                             0.331154                           \n",
       "18     0.153390                             0.353548                           \n",
       "19     0.149951                             0.287257                           \n",
       "20     0.164480                             0.247303                           \n",
       "21     0.253681                             0.411708                           \n",
       "22     0.178038                             0.310287                           \n",
       "23     0.133581                             0.380284                           \n",
       "24     0.117582                             0.463031                           \n",
       "25     0.150339                             0.446511                           \n",
       "26     0.130905                             0.388730                           \n",
       "27     0.175774                             0.351796                           \n",
       "28     0.135931                             0.413161                           \n",
       "29     0.111635                             0.316733                           \n",
       "30     0.169246                             0.318336                           \n",
       "31     0.136539                             0.424173                           \n",
       "32     0.155241                             0.326918                           \n",
       "33     0.244443                             0.388604                           \n",
       "34     0.131190                             0.362583                           \n",
       "35     0.164073                             0.661338                           \n",
       "36     0.126538                             0.317065                           \n",
       "37     0.109560                             0.298032                           \n",
       "38     0.141407                             0.324945                           \n",
       "39     0.184139                             0.422355                           \n",
       "40     0.162873                             0.670893                           \n",
       "41     0.104487                             0.335359                           \n",
       "42     0.130095                             0.443038                           \n",
       "43     0.452549                             0.369756                           \n",
       "44     0.115083                             0.382636                           \n",
       "45     0.174043                             0.448848                           \n",
       "46     0.128192                             0.322891                           \n",
       "47     0.176352                             0.385293                           \n",
       "48     0.182663                             0.282476                           \n",
       "49     0.157205                             0.343907                           \n",
       "...         ...                                  ...                           \n",
       "26749  0.136203                             0.116855                           \n",
       "26750  0.136400                             0.122675                           \n",
       "26751  0.134289                             0.123596                           \n",
       "26752  0.141216                             0.138498                           \n",
       "26753  0.424154                             0.320345                           \n",
       "26754  0.165062                             0.156353                           \n",
       "26755  0.117924                             0.123119                           \n",
       "26756  0.172912                             0.314851                           \n",
       "26757  0.128863                             0.152228                           \n",
       "26758  0.141381                             0.219787                           \n",
       "26759  0.117411                             0.145548                           \n",
       "26760  0.107929                             0.114855                           \n",
       "26761  0.130971                             0.125084                           \n",
       "26762  0.173095                             0.182169                           \n",
       "26763  0.131980                             0.128372                           \n",
       "26764  0.128259                             0.180827                           \n",
       "26765  0.129926                             0.161320                           \n",
       "26766  0.119111                             0.132522                           \n",
       "26767  0.187320                             0.167180                           \n",
       "26768  0.092746                             0.159485                           \n",
       "26769  0.134009                             0.162578                           \n",
       "26770  0.097647                             0.150018                           \n",
       "26771  0.147564                             0.190648                           \n",
       "26772  0.189712                             0.136244                           \n",
       "26773  0.201461                             0.102778                           \n",
       "26774  0.384339                             0.132466                           \n",
       "26775  0.187336                             0.124931                           \n",
       "26776  0.117212                             0.112917                           \n",
       "26777  0.106065                             0.145623                           \n",
       "26778  0.111937                             0.182483                           \n",
       "26779  0.113895                             0.138413                           \n",
       "26780  0.159864                             0.167771                           \n",
       "26781  0.101285                             0.171966                           \n",
       "26782  0.112589                             0.149096                           \n",
       "26783  0.137478                             0.166814                           \n",
       "26784  0.203974                             0.138768                           \n",
       "26785  0.115387                             0.181295                           \n",
       "26786  0.165396                             0.154588                           \n",
       "26787  0.120321                             0.101498                           \n",
       "26788  0.117225                             0.121786                           \n",
       "26789  0.113801                             0.156869                           \n",
       "26790  0.178001                             0.105386                           \n",
       "26791  0.094136                             0.160554                           \n",
       "26792  0.134595                             0.130791                           \n",
       "26793  0.195705                             0.124839                           \n",
       "26794  0.145468                             0.166837                           \n",
       "26795  0.150160                             0.192854                           \n",
       "26796  0.166039                             0.154844                           \n",
       "26797  0.132817                             0.174018                           \n",
       "26798  0.177645                             0.135624                           \n",
       "\n",
       "       5.2.4.4. -- Feature selection  5.2.4.5.1 -- Generalized eigenvalue  \\\n",
       "0      0.319550                       0.130482                              \n",
       "1      0.238902                       0.113895                              \n",
       "2      0.199925                       0.142234                              \n",
       "3      0.198067                       0.102722                              \n",
       "4      0.133150                       0.226799                              \n",
       "5      0.228125                       0.101625                              \n",
       "6      0.138567                       0.122096                              \n",
       "7      0.277890                       0.145228                              \n",
       "8      0.322109                       0.287601                              \n",
       "9      0.331842                       0.220369                              \n",
       "10     0.244318                       0.140443                              \n",
       "11     0.211878                       0.091399                              \n",
       "12     0.245649                       0.131959                              \n",
       "13     0.343359                       0.115858                              \n",
       "14     0.314665                       0.127648                              \n",
       "15     0.257353                       0.109432                              \n",
       "16     0.270732                       0.151336                              \n",
       "17     0.315660                       0.145988                              \n",
       "18     0.378191                       0.085466                              \n",
       "19     0.178085                       0.161595                              \n",
       "20     0.231142                       0.160416                              \n",
       "21     0.328589                       0.128894                              \n",
       "22     0.155588                       0.178428                              \n",
       "23     0.266844                       0.099760                              \n",
       "24     0.224619                       0.210738                              \n",
       "25     0.189706                       0.300093                              \n",
       "26     0.202541                       0.131863                              \n",
       "27     0.459176                       0.098925                              \n",
       "28     0.163374                       0.111685                              \n",
       "29     0.149886                       0.137854                              \n",
       "30     0.232269                       0.185783                              \n",
       "31     0.230212                       0.185834                              \n",
       "32     0.251955                       0.102415                              \n",
       "33     0.275169                       0.091829                              \n",
       "34     0.211162                       0.100926                              \n",
       "35     0.183341                       0.144085                              \n",
       "36     0.223282                       0.176036                              \n",
       "37     0.180229                       0.118343                              \n",
       "38     0.184456                       0.105682                              \n",
       "39     0.171045                       0.212293                              \n",
       "40     0.307623                       0.141114                              \n",
       "41     0.311732                       0.135854                              \n",
       "42     0.156910                       0.143777                              \n",
       "43     0.444511                       0.109759                              \n",
       "44     0.128476                       0.155283                              \n",
       "45     0.303456                       0.097363                              \n",
       "46     0.337812                       0.279166                              \n",
       "47     0.216331                       0.214585                              \n",
       "48     0.181725                       0.143418                              \n",
       "49     0.255538                       0.136574                              \n",
       "...         ...                            ...                              \n",
       "26749  0.330210                       0.155101                              \n",
       "26750  0.166302                       0.119934                              \n",
       "26751  0.235690                       0.152807                              \n",
       "26752  0.181066                       0.086517                              \n",
       "26753  0.316677                       0.164455                              \n",
       "26754  0.332878                       0.209145                              \n",
       "26755  0.194684                       0.336912                              \n",
       "26756  0.309226                       0.194424                              \n",
       "26757  0.269798                       0.161638                              \n",
       "26758  0.147827                       0.140212                              \n",
       "26759  0.194617                       0.210859                              \n",
       "26760  0.298452                       0.170757                              \n",
       "26761  0.158476                       0.177354                              \n",
       "26762  0.211351                       0.130189                              \n",
       "26763  0.172259                       0.169881                              \n",
       "26764  0.334200                       0.185228                              \n",
       "26765  0.172026                       0.095402                              \n",
       "26766  0.312086                       0.189465                              \n",
       "26767  0.186513                       0.098886                              \n",
       "26768  0.190567                       0.092827                              \n",
       "26769  0.262863                       0.099636                              \n",
       "26770  0.222733                       0.206094                              \n",
       "26771  0.231170                       0.138187                              \n",
       "26772  0.191860                       0.150964                              \n",
       "26773  0.145592                       0.242391                              \n",
       "26774  0.246919                       0.112786                              \n",
       "26775  0.185453                       0.132280                              \n",
       "26776  0.254260                       0.087882                              \n",
       "26777  0.235039                       0.140581                              \n",
       "26778  0.265337                       0.251558                              \n",
       "26779  0.234475                       0.138440                              \n",
       "26780  0.239734                       0.110314                              \n",
       "26781  0.219360                       0.170101                              \n",
       "26782  0.160068                       0.177614                              \n",
       "26783  0.214821                       0.097165                              \n",
       "26784  0.309690                       0.098975                              \n",
       "26785  0.214133                       0.232146                              \n",
       "26786  0.225575                       0.244475                              \n",
       "26787  0.319427                       0.145873                              \n",
       "26788  0.162540                       0.173952                              \n",
       "26789  0.323403                       0.094136                              \n",
       "26790  0.154212                       0.130698                              \n",
       "26791  0.162852                       0.134673                              \n",
       "26792  0.330328                       0.213370                              \n",
       "26793  0.216916                       0.314152                              \n",
       "26794  0.220492                       0.113413                              \n",
       "26795  0.218293                       0.114975                              \n",
       "26796  0.401948                       0.136355                              \n",
       "26797  0.139583                       0.235373                              \n",
       "26798  0.198115                       0.173613                              \n",
       "\n",
       "       5.2.5. -- Cross-validation  \n",
       "0      0.217381                    \n",
       "1      0.153735                    \n",
       "2      0.241214                    \n",
       "3      0.186526                    \n",
       "4      0.203539                    \n",
       "5      0.179478                    \n",
       "6      0.201846                    \n",
       "7      0.142365                    \n",
       "8      0.245244                    \n",
       "9      0.227139                    \n",
       "10     0.224745                    \n",
       "11     0.197672                    \n",
       "12     0.161398                    \n",
       "13     0.160748                    \n",
       "14     0.300291                    \n",
       "15     0.211686                    \n",
       "16     0.176503                    \n",
       "17     0.165849                    \n",
       "18     0.184077                    \n",
       "19     0.186693                    \n",
       "20     0.184462                    \n",
       "21     0.207635                    \n",
       "22     0.173146                    \n",
       "23     0.183641                    \n",
       "24     0.172524                    \n",
       "25     0.177621                    \n",
       "26     0.220331                    \n",
       "27     0.176254                    \n",
       "28     0.177859                    \n",
       "29     0.195088                    \n",
       "30     0.197351                    \n",
       "31     0.218898                    \n",
       "32     0.175681                    \n",
       "33     0.156603                    \n",
       "34     0.160466                    \n",
       "35     0.280492                    \n",
       "36     0.191981                    \n",
       "37     0.167386                    \n",
       "38     0.169282                    \n",
       "39     0.261631                    \n",
       "40     0.172004                    \n",
       "41     0.205047                    \n",
       "42     0.165756                    \n",
       "43     0.221136                    \n",
       "44     0.190141                    \n",
       "45     0.351441                    \n",
       "46     0.192140                    \n",
       "47     0.194260                    \n",
       "48     0.191033                    \n",
       "49     0.153329                    \n",
       "...         ...                    \n",
       "26749  0.184109                    \n",
       "26750  0.254717                    \n",
       "26751  0.188513                    \n",
       "26752  0.164061                    \n",
       "26753  0.168505                    \n",
       "26754  0.204932                    \n",
       "26755  0.175510                    \n",
       "26756  0.366699                    \n",
       "26757  0.186051                    \n",
       "26758  0.176858                    \n",
       "26759  0.178710                    \n",
       "26760  0.224565                    \n",
       "26761  0.185583                    \n",
       "26762  0.191372                    \n",
       "26763  0.284737                    \n",
       "26764  0.201373                    \n",
       "26765  0.138390                    \n",
       "26766  0.241916                    \n",
       "26767  0.167779                    \n",
       "26768  0.153309                    \n",
       "26769  0.155270                    \n",
       "26770  0.192797                    \n",
       "26771  0.173128                    \n",
       "26772  0.206547                    \n",
       "26773  0.131224                    \n",
       "26774  0.196690                    \n",
       "26775  0.191615                    \n",
       "26776  0.222596                    \n",
       "26777  0.166122                    \n",
       "26778  0.180560                    \n",
       "26779  0.197979                    \n",
       "26780  0.188731                    \n",
       "26781  0.134241                    \n",
       "26782  0.234714                    \n",
       "26783  0.176508                    \n",
       "26784  0.158423                    \n",
       "26785  0.166705                    \n",
       "26786  0.241712                    \n",
       "26787  0.210100                    \n",
       "26788  0.171131                    \n",
       "26789  0.163735                    \n",
       "26790  0.162569                    \n",
       "26791  0.208555                    \n",
       "26792  0.183436                    \n",
       "26793  0.190215                    \n",
       "26794  0.211762                    \n",
       "26795  0.175831                    \n",
       "26796  0.226379                    \n",
       "26797  0.179491                    \n",
       "26798  0.183496                    \n",
       "\n",
       "[26799 rows x 353 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Get random sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_subset(df, seed=42, frac=0.2):\n",
    "    return df.sample(frac=frac, random_state=seed)\n",
    "\n",
    "# score_df.pipe(random_subset, seed=43)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Create & transform corelevance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from topic_corelevance import TopicCorelevanceBuilder\n",
    "from lapin import LapinTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_df_subset = score_df.pipe(random_subset, seed=42)\n",
    "\n",
    "subset_corelevance = TopicCorelevanceBuilder(score_df_subset, 0.4).corelevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_corelevance_tr = LapinTransformer(subset_corelevance).Lp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Apply fuzzy clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from faddis import FADDIS\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level='INFO')\n",
    "logging.getLogger('FADDIS').setLevel('INFO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:FADDIS:i = 0, xi = 0.8742, sqrt(xi) = 0.9350\n",
      "INFO:FADDIS:E = 349.2459\n",
      "INFO:FADDIS:i = 1, xi = 0.8978, sqrt(xi) = 0.9475\n",
      "INFO:FADDIS:E = 348.4399\n",
      "INFO:FADDIS:i = 2, xi = 0.8546, sqrt(xi) = 0.9244\n",
      "INFO:FADDIS:E = 347.7096\n",
      "INFO:FADDIS:i = 3, xi = 0.7765, sqrt(xi) = 0.8812\n",
      "INFO:FADDIS:E = 347.1066\n",
      "INFO:FADDIS:i = 4, xi = 0.7284, sqrt(xi) = 0.8535\n",
      "INFO:FADDIS:E = 346.5760\n",
      "INFO:FADDIS:i = 5, xi = 0.6171, sqrt(xi) = 0.7856\n",
      "INFO:FADDIS:E = 346.1952\n",
      "INFO:FADDIS:i = 6, xi = 0.6032, sqrt(xi) = 0.7766\n",
      "INFO:FADDIS:E = 345.8314\n",
      "INFO:FADDIS:i = 7, xi = 0.5278, sqrt(xi) = 0.7265\n",
      "INFO:FADDIS:E = 345.5528\n",
      "INFO:FADDIS:i = 8, xi = 0.6622, sqrt(xi) = 0.8137\n",
      "INFO:FADDIS:E = 345.1144\n",
      "INFO:FADDIS:i = 9, xi = 0.6174, sqrt(xi) = 0.7857\n",
      "INFO:FADDIS:E = 344.7332\n",
      "INFO:FADDIS:i = 10, xi = 0.2897, sqrt(xi) = 0.5383\n",
      "INFO:FADDIS:E = 344.6493\n",
      "INFO:FADDIS:i = 11, xi = 0.3757, sqrt(xi) = 0.6130\n",
      "INFO:FADDIS:E = 344.5081\n",
      "INFO:FADDIS:i = 12, xi = 0.5259, sqrt(xi) = 0.7252\n",
      "INFO:FADDIS:E = 344.2316\n",
      "INFO:FADDIS:i = 13, xi = 0.7174, sqrt(xi) = 0.8470\n",
      "INFO:FADDIS:E = 343.7169\n",
      "INFO:FADDIS:i = 14, xi = 0.5297, sqrt(xi) = 0.7278\n",
      "INFO:FADDIS:E = 343.4363\n",
      "INFO:FADDIS:i = 15, xi = 0.5506, sqrt(xi) = 0.7421\n",
      "INFO:FADDIS:E = 343.1331\n",
      "INFO:FADDIS:i = 16, xi = 0.5074, sqrt(xi) = 0.7123\n",
      "INFO:FADDIS:E = 342.8757\n",
      "INFO:FADDIS:i = 17, xi = 0.4762, sqrt(xi) = 0.6900\n",
      "INFO:FADDIS:E = 342.6490\n",
      "INFO:FADDIS:i = 18, xi = 0.7161, sqrt(xi) = 0.8462\n",
      "INFO:FADDIS:E = 342.1362\n",
      "INFO:FADDIS:i = 19, xi = 0.6264, sqrt(xi) = 0.7914\n",
      "INFO:FADDIS:E = 341.7439\n",
      "INFO:FADDIS:i = 20, xi = 0.5049, sqrt(xi) = 0.7106\n",
      "INFO:FADDIS:E = 341.4889\n",
      "INFO:FADDIS:i = 21, xi = 0.4017, sqrt(xi) = 0.6338\n",
      "INFO:FADDIS:E = 341.3275\n",
      "INFO:FADDIS:i = 22, xi = 0.2516, sqrt(xi) = 0.5016\n",
      "INFO:FADDIS:E = 341.2642\n",
      "INFO:FADDIS:i = 23, xi = 0.3142, sqrt(xi) = 0.5605\n",
      "INFO:FADDIS:E = 341.1655\n",
      "INFO:FADDIS:i = 24, xi = 0.4224, sqrt(xi) = 0.6500\n",
      "INFO:FADDIS:E = 340.9870\n",
      "INFO:FADDIS:i = 25, xi = 0.2594, sqrt(xi) = 0.5093\n",
      "INFO:FADDIS:E = 340.9198\n",
      "INFO:FADDIS:i = 26, xi = 0.3790, sqrt(xi) = 0.6156\n",
      "INFO:FADDIS:E = 340.7761\n",
      "INFO:FADDIS:i = 27, xi = 0.3232, sqrt(xi) = 0.5685\n",
      "INFO:FADDIS:E = 340.6717\n",
      "INFO:FADDIS:i = 28, xi = 0.1788, sqrt(xi) = 0.4228\n",
      "INFO:FADDIS:E = 340.6397\n",
      "INFO:FADDIS:i = 29, xi = 0.5282, sqrt(xi) = 0.7267\n",
      "INFO:FADDIS:E = 340.3608\n",
      "INFO:FADDIS:i = 30, xi = 0.4423, sqrt(xi) = 0.6650\n",
      "INFO:FADDIS:E = 340.1652\n",
      "INFO:FADDIS:i = 31, xi = 0.6133, sqrt(xi) = 0.7831\n",
      "INFO:FADDIS:E = 339.7891\n",
      "INFO:FADDIS:i = 32, xi = 0.2591, sqrt(xi) = 0.5090\n",
      "INFO:FADDIS:E = 339.7219\n",
      "INFO:FADDIS:i = 33, xi = 0.4543, sqrt(xi) = 0.6740\n",
      "INFO:FADDIS:E = 339.5156\n",
      "INFO:FADDIS:i = 34, xi = 0.4374, sqrt(xi) = 0.6614\n",
      "INFO:FADDIS:E = 339.3242\n",
      "INFO:FADDIS:i = 35, xi = 0.2125, sqrt(xi) = 0.4610\n",
      "INFO:FADDIS:E = 339.2790\n",
      "INFO:FADDIS:i = 36, xi = 0.2393, sqrt(xi) = 0.4891\n",
      "INFO:FADDIS:E = 339.2218\n",
      "INFO:FADDIS:i = 37, xi = 0.2508, sqrt(xi) = 0.5008\n",
      "INFO:FADDIS:E = 339.1589\n",
      "INFO:FADDIS:i = 38, xi = 0.2703, sqrt(xi) = 0.5199\n",
      "INFO:FADDIS:E = 339.0859\n",
      "INFO:FADDIS:i = 39, xi = 0.1986, sqrt(xi) = 0.4457\n",
      "INFO:FADDIS:E = 339.0464\n",
      "INFO:FADDIS:i = 40, xi = 0.5035, sqrt(xi) = 0.7096\n",
      "INFO:FADDIS:E = 338.7929\n",
      "INFO:FADDIS:i = 41, xi = 0.2450, sqrt(xi) = 0.4950\n",
      "INFO:FADDIS:E = 338.7329\n",
      "INFO:FADDIS:i = 42, xi = 0.1218, sqrt(xi) = 0.3490\n",
      "INFO:FADDIS:E = 338.7180\n",
      "INFO:FADDIS:i = 43, xi = 0.3307, sqrt(xi) = 0.5751\n",
      "INFO:FADDIS:E = 338.6087\n",
      "INFO:FADDIS:i = 44, xi = 0.3484, sqrt(xi) = 0.5902\n",
      "INFO:FADDIS:E = 338.4873\n",
      "INFO:FADDIS:i = 45, xi = 0.2939, sqrt(xi) = 0.5421\n",
      "INFO:FADDIS:E = 338.4009\n",
      "INFO:FADDIS:i = 46, xi = 0.3262, sqrt(xi) = 0.5711\n",
      "INFO:FADDIS:E = 338.2945\n",
      "INFO:FADDIS:xi < 0 -> break\n"
     ]
    }
   ],
   "source": [
    "clu, intens, contr = FADDIS(max_clusters=100).predict(subset_corelevance_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2160x1584 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(30, 22))\n",
    "plt.imshow(clu, cmap='Greys');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Alltogether"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "logging.getLogger('FADDIS').setLevel('WARNING')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def build_clusters_on_random_subsets(n_trials, frac=0.2):\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for seed in tqdm(range(n_trials)):\n",
    "        score_df_subset = score_df.pipe(random_subset, seed=seed, frac=frac)\n",
    "\n",
    "        subset_corelevance = TopicCorelevanceBuilder(score_df_subset, 0.4).corelevance\n",
    "        subset_corelevance_tr = LapinTransformer(subset_corelevance).Lp\n",
    "        r = FADDIS(max_clusters=100).predict(subset_corelevance_tr)\n",
    "        \n",
    "        results.append(r)\n",
    "        \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "247e90b421c742d59bd175f180b5abee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=150), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lx = build_clusters_on_random_subsets(150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/clusters_on_random_subsets.pkl']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(lx, 'data/clusters_on_random_subsets.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lx = joblib.load('data/clusters_on_random_subsets.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Lift 'em"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'pgfs' from 'C:\\\\Users\\\\Admin\\\\YandexDisk\\\\Study\\\\HSE\\\\КР\\\\research\\\\lifting\\\\pgfs.py'>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from taxonomy import print_tree\n",
    "import pgfs\n",
    "from anytree.importer import JsonImporter\n",
    "importer = JsonImporter()\n",
    "reload(pgfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "U_THRESH = 0.1\n",
    "MIN_OBJECTS = 10\n",
    "\n",
    "TOPICS = score_df.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_is_appropriate(u):\n",
    "    return (u > U_THRESH).sum() > MIN_OBJECTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lift_cluster(u, lifter):\n",
    "    cluster_memberships = (\n",
    "        pd.Series(u, index=TOPICS)\n",
    "        .loc[lambda x: x > U_THRESH] # filter noise\n",
    "        .pipe(lambda x: x / np.sqrt((x ** 2).sum())) # normalize\n",
    "        .to_dict()\n",
    "    )\n",
    "    \n",
    "    lifter.lift(cluster_memberships)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ccc7475d22d41839c279472275baaad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=150), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=36), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=30), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=30), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=38), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=36), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=37), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=33), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=29), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=37), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=39), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=39), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=36), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=43), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=38), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=40), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=33), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=37), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=37), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=38), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=42), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=44), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=35), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=27), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=42), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=41), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=38), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=42), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=37), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=41), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=41), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=34), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=36), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=41), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=39), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=43), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=34), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=42), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=38), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=37), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=43), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=38), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=31), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=47), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=35), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=38), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=38), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=37), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=43), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=41), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=38), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=37), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=39), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=38), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=48), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=36), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=37), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=35), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=41), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=49), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=37), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=39), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=41), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=33), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=36), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=44), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=42), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=38), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=38), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=35), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=37), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=35), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=27), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=44), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=44), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=46), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=36), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=35), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=30), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=26), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=36), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=35), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=38), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=42), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=38), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=38), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=38), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=43), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=34), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=35), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=35), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=32), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=23), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=41), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=34), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=39), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=49), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=42), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=26), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=34), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=38), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=41), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=40), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=38), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=38), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=35), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=48), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=35), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=37), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=37), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=41), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=37), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=33), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=41), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=42), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=38), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=34), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=35), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=38), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=37), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=35), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=43), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=38), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=35), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=46), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=44), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=40), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=30), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=38), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=38), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=37), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=27), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=38), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=38), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=21), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=38), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=43), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=41), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=38), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=37), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=38), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=37), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=41), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=46), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=41), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=37), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=27), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=42), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=41), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=33), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=26), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open('ds_taxonomy.json', 'r') as f:\n",
    "    tax = importer.read(f)\n",
    "    \n",
    "lifter = pgfs.PGFS(tax, lmbda=0.1, gamma=0.9)\n",
    "\n",
    "for clusters, _, _ in tqdm(lx):\n",
    "    for u in tqdm(clusters, leave=False):\n",
    "        if cluster_is_appropriate(u):\n",
    "            lift_cluster(u, lifter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnyNode(n_gains=25, n_losses=0, n_samples=4191, name='root', p_gain=0.005965163445478406, p_loss=0.0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lifter.estimate_ml()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root: p_gain=0.005965163445478406, p_loss=0.0\n",
      "├── 1. + 0 -- Theory of computation: p_gain=0.0, p_loss=0.0\n",
      "│   └── 1.1. -- Theory and algorithms for application domains: p_gain=0.0, p_loss=0.0\n",
      "│       ├── 1.1.1. -- Machine learning theory: p_gain=0.03579098067287044, p_loss=0.0014316392269148174\n",
      "│       │   ├── 1.1.1.1. -- Sample complexity and generalization bounds: p_gain=0.01765688379861608, p_loss=0.04008589835361489\n",
      "│       │   ├── 1.1.1.2. -- Boolean function learning: p_gain=0.0, p_loss=0.004533524218563589\n",
      "│       │   ├── 1.1.1.3. -- Unsupervised learning and clustering: p_gain=0.03579098067287044, p_loss=0.004533524218563589\n",
      "│       │   ├── 1.1.1.4. -- Kernel methods: p_gain=0.011691720353137676, p_loss=0.03626819374850871\n",
      "│       │   │   ├── 1.1.1.4.1. -- Support vector machines: p_gain=0.10188499164877117, p_loss=0.0035790980672870437\n",
      "│       │   │   ├── 1.1.1.4.2. -- Gaussian processes: p_gain=0.08518253400143164, p_loss=0.006919589596754951\n",
      "│       │   │   └── 1.1.1.4.3. -- Modelling: p_gain=0.044858029109997616, p_loss=0.008589835361488905\n",
      "│       │   ├── 1.1.1.5. -- Boosting: p_gain=0.03316630875685994, p_loss=0.04008589835361489\n",
      "│       │   ├── 1.1.1.6. -- Bayesian analysis: p_gain=0.08637556669052732, p_loss=0.04032450489143403\n",
      "│       │   ├── 1.1.1.7. -- Inductive inference: p_gain=0.0, p_loss=0.04032450489143403\n",
      "│       │   ├── 1.1.1.8. -- Online learning theory: p_gain=0.0, p_loss=0.004533524218563589\n",
      "│       │   ├── 1.1.1.9. -- Multi-agent learning: p_gain=0.0, p_loss=0.004533524218563589\n",
      "│       │   ├── 1.1.1.10. -- Models of learning: p_gain=0.00023860653781913624, p_loss=0.004533524218563589\n",
      "│       │   ├── 1.1.1.11. -- Query learning: p_gain=0.0, p_loss=0.004533524218563589\n",
      "│       │   ├── 1.1.1.12. -- Structured prediction: p_gain=0.08208064900978287, p_loss=0.03913147220233834\n",
      "│       │   ├── 1.1.1.13. -- Reinforcement learning: p_gain=0.0, p_loss=0.0026246719160104987\n",
      "│       │   │   ├── 1.1.1.13.1. -- Sequential decision making: p_gain=0.03626819374850871, p_loss=0.03579098067287044\n",
      "│       │   │   ├── 1.1.1.13.2. -- Inverse reinforcement learning: p_gain=0.0, p_loss=0.002863278453829635\n",
      "│       │   │   ├── 1.1.1.13.3. -- Apprenticeship learning: p_gain=0.0, p_loss=0.00190885230255309\n",
      "│       │   │   ├── 1.1.1.13.4. -- Multi-agent reinforcement learning: p_gain=0.0, p_loss=0.03698401336196612\n",
      "│       │   │   └── 1.1.1.13.5. -- Adversarial learning: p_gain=0.0, p_loss=0.00190885230255309\n",
      "│       │   ├── 1.1.1.14. -- Active learning: p_gain=0.0, p_loss=0.004533524218563589\n",
      "│       │   ├── 1.1.1.15. -- Semi-supervised learning: p_gain=0.0, p_loss=0.004533524218563589\n",
      "│       │   ├── 1.1.1.16. -- Markov decision processes: p_gain=0.08828441899308041, p_loss=0.03698401336196612\n",
      "│       │   └── 1.1.1.17. -- Regret bounds: p_gain=0.1448341684562157, p_loss=0.04008589835361489\n",
      "│       └── 1.1.2. -- Database theory: p_gain=0.009305654974946313, p_loss=0.0011930326890956812\n",
      "│           ├── 1.1.2.1. -- Data exchange: p_gain=0.12407539966595085, p_loss=0.0011930326890956812\n",
      "│           ├── 1.1.2.2. -- Data provenance: p_gain=0.0, p_loss=0.014077785731329038\n",
      "│           ├── 1.1.2.3. -- Data modeling: p_gain=0.04271057026962539, p_loss=0.014077785731329038\n",
      "│           ├── 1.1.2.4. -- Database query languages (principles): p_gain=0.046051061799093296, p_loss=0.007396802672393224\n",
      "│           ├── 1.1.2.5. -- Database constraints theory: p_gain=0.01145311381531854, p_loss=0.005010737294201861\n",
      "│           ├── 1.1.2.6. -- Database interoperability: p_gain=0.0011930326890956812, p_loss=0.0071581961345740875\n",
      "│           ├── 1.1.2.7. -- Data structures and algorithms for data management: p_gain=0.07683130517776188, p_loss=0.007396802672393224\n",
      "│           ├── 1.1.2.8. -- Database query processing and optimization (theory): p_gain=0.0071581961345740875, p_loss=0.012646146504414221\n",
      "│           ├── 1.1.2.9. -- Data integration: p_gain=0.0, p_loss=0.014077785731329038\n",
      "│           ├── 1.1.2.10. -- Logic and databases: p_gain=0.09210212359818659, p_loss=0.000954426151276545\n",
      "│           ├── 1.1.2.11. -- Theory of database privacy and security: p_gain=0.0691958959675495, p_loss=0.0026246719160104987\n",
      "│           └── 1.1.2.12. -- Incomplete, inconsistent, and uncertain databases: p_gain=0.0, p_loss=0.012646146504414221\n",
      "├── 2. + 0 -- Mathematics of computing: p_gain=0.00381770460510618, p_loss=0.0007158196134574087\n",
      "│   └── 2.1. -- Probability and statistics: p_gain=0.0, p_loss=0.0\n",
      "│       ├── 2.1.1. -- Probabilistic representations: p_gain=0.07492245287520878, p_loss=0.0035790980672870437\n",
      "│       │   ├── 2.1.1.1. -- Bayesian networks: p_gain=0.010021474588403722, p_loss=0.00572655690765927\n",
      "│       │   ├── 2.1.1.2. -- Markov networks: p_gain=0.005965163445478406, p_loss=0.006203769983297542\n",
      "│       │   ├── 2.1.1.3. -- Factor graphs: p_gain=0.19899785254115962, p_loss=0.07659269863994274\n",
      "│       │   ├── 2.1.1.4. -- Decision diagrams: p_gain=0.046766881412550705, p_loss=0.0796945836315915\n",
      "│       │   ├── 2.1.1.5. -- Equational models: p_gain=0.0, p_loss=0.08041040324504892\n",
      "│       │   ├── 2.1.1.6. -- Causal networks: p_gain=0.0021474588403722263, p_loss=0.006203769983297542\n",
      "│       │   ├── 2.1.1.7. -- Stochastic differential equations: p_gain=0.0, p_loss=0.08041040324504892\n",
      "│       │   └── 2.1.1.8. -- Nonparametric representations: p_gain=0.016702457647339537, p_loss=0.03984729181579575\n",
      "│       │       ├── 2.1.1.8.1. -- Kernel density estimators: p_gain=0.023622047244094488, p_loss=0.03602958721068957\n",
      "│       │       ├── 2.1.1.8.2. -- Spline models: p_gain=0.026962538773562396, p_loss=0.020758768790264854\n",
      "│       │       └── 2.1.1.8.3. -- Bayesian nonparametric models: p_gain=0.012407539966595084, p_loss=0.000954426151276545\n",
      "│       ├── 2.1.2. -- Probabilistic inference problems: p_gain=0.0, p_loss=0.005249343832020997\n",
      "│       │   ├── 2.1.2.1. -- Maximum likelihood estimation: p_gain=0.01145311381531854, p_loss=0.00023860653781913624\n",
      "│       │   ├── 2.1.2.2. -- Bayesian computation: p_gain=0.0004772130756382725, p_loss=0.00381770460510618\n",
      "│       │   ├── 2.1.2.3. -- Computing most probable explanation: p_gain=0.0, p_loss=0.00381770460510618\n",
      "│       │   ├── 2.1.2.4. -- Hypothesis testing and confidence interval computation: p_gain=0.0, p_loss=0.00381770460510618\n",
      "│       │   ├── 2.1.2.5. -- Density estimation: p_gain=0.0, p_loss=0.0007158196134574087\n",
      "│       │   │   └── 2.1.2.5.1. -- Quantile regression: p_gain=0.02720114531138153, p_loss=0.0\n",
      "│       │   └── 2.1.2.6. -- Max marginal computation: p_gain=0.0, p_loss=0.00381770460510618\n",
      "│       ├── 2.1.3. -- Probabilistic reasoning algorithms: p_gain=0.00023860653781913624, p_loss=0.00381770460510618\n",
      "│       │   ├── 2.1.3.1. -- Variable elimination: p_gain=0.0, p_loss=0.0054879503698401335\n",
      "│       │   ├── 2.1.3.2. -- Loopy belief propagation: p_gain=0.0, p_loss=0.0054879503698401335\n",
      "│       │   ├── 2.1.3.3. -- Variational methods: p_gain=0.0, p_loss=0.0054879503698401335\n",
      "│       │   ├── 2.1.3.4. -- Expectation maximization: p_gain=0.0190885230255309, p_loss=0.0054879503698401335\n",
      "│       │   ├── 2.1.3.5. -- Markov-chain Monte Carlo methods: p_gain=0.006919589596754951, p_loss=0.0007158196134574087\n",
      "│       │   │   ├── 2.1.3.5.1. -- Gibbs sampling: p_gain=0.09234073013600573, p_loss=0.004533524218563589\n",
      "│       │   │   ├── 2.1.3.5.2. -- Metropolis-Hastings algorithm: p_gain=0.1388690050107373, p_loss=0.00190885230255309\n",
      "│       │   │   ├── 2.1.3.5.3. -- Simulated annealing: p_gain=0.0, p_loss=0.011691720353137676\n",
      "│       │   │   └── 2.1.3.5.4. -- Markov-chain Monte Carlo convergence measures: p_gain=0.0007158196134574087, p_loss=0.009782868050584585\n",
      "│       │   ├── 2.1.3.6. -- Sequential Monte Carlo methods: p_gain=0.0035790980672870437, p_loss=0.0054879503698401335\n",
      "│       │   ├── 2.1.3.7. -- Kalman filters and hidden Markov models: p_gain=0.0, p_loss=0.005010737294201861\n",
      "│       │   │   └── 2.1.3.7.1 -- Factorial HMM: p_gain=0.18993080410403246, p_loss=0.0\n",
      "│       │   ├── 2.1.3.8. -- Resampling methods: p_gain=0.044142209496540206, p_loss=0.0035790980672870437\n",
      "│       │   │   ├── 2.1.3.8.1. -- Bootstrapping: p_gain=0.06370794559770938, p_loss=0.0016702457647339538\n",
      "│       │   │   └── 2.1.3.8.2. -- Jackknifing: p_gain=0.12001908852302554, p_loss=0.0\n",
      "│       │   └── 2.1.3.9. -- Random number generation: p_gain=0.0, p_loss=0.0054879503698401335\n",
      "│       ├── 2.1.4. -- Probabilistic algorithms: p_gain=0.12001908852302554, p_loss=0.006680983058935815\n",
      "│       ├── 2.1.5. -- Statistical paradigms: p_gain=0.04891434025292293, p_loss=0.00381770460510618\n",
      "│       │   ├── 2.1.5.1. -- Queueing theory: p_gain=0.10856597470770699, p_loss=0.05153901216893343\n",
      "│       │   ├── 2.1.5.2. -- Contingency table analysis: p_gain=0.0, p_loss=0.00572655690765927\n",
      "│       │   ├── 2.1.5.3. -- Regression analysis: p_gain=0.0, p_loss=0.05082319255547602\n",
      "│       │   │   └── 2.1.5.3.1. -- Robust regression: p_gain=0.03364352183249821, p_loss=0.0\n",
      "│       │   ├── 2.1.5.4. -- Time series analysis: p_gain=0.02028155571462658, p_loss=0.005249343832020997\n",
      "│       │   ├── 2.1.5.5. -- Survival analysis: p_gain=0.031496062992125984, p_loss=0.005249343832020997\n",
      "│       │   ├── 2.1.5.6. -- Renewal theory: p_gain=0.14364113576712, p_loss=0.052970651395848244\n",
      "│       │   ├── 2.1.5.7. -- Dimensionality reduction: p_gain=0.0, p_loss=0.05416368408494393\n",
      "│       │   ├── 2.1.5.8. -- Cluster analysis: p_gain=0.046528274874731566, p_loss=0.005249343832020997\n",
      "│       │   ├── 2.1.5.9. -- Statistical graphics: p_gain=0.011691720353137676, p_loss=0.05273204485802911\n",
      "│       │   └── 2.1.5.10. -- Exploratory data analysis: p_gain=0.0, p_loss=0.009305654974946313\n",
      "│       ├── 2.1.6. -- Stochastic processes: p_gain=0.0, p_loss=0.004533524218563589\n",
      "│       │   └── 2.1.6.1. -- Markov processes: p_gain=0.11930326890956812, p_loss=0.0\n",
      "│       ├── 2.1.7. -- Nonparametric statistics: p_gain=0.025530899546647577, p_loss=0.005249343832020997\n",
      "│       ├── 2.1.8. -- Distribution functions: p_gain=0.013600572655690766, p_loss=0.0054879503698401335\n",
      "│       └── 2.1.9. -- Multivariate statistics: p_gain=0.0011930326890956812, p_loss=0.006680983058935815\n",
      "├── 3. + 0 -- Information systems: p_gain=0.020042949176807445, p_loss=0.0\n",
      "│   ├── 3.1. -- Data management systems: p_gain=0.01956573610116917, p_loss=0.0\n",
      "│   │   ├── 3.1.1. -- Database design and models: p_gain=0.0004772130756382725, p_loss=0.007874015748031496\n",
      "│   │   │   ├── 3.1.1.1. -- Relational database model: p_gain=0.0, p_loss=0.03626819374850871\n",
      "│   │   │   ├── 3.1.1.2. -- Entity relationship models: p_gain=0.0, p_loss=0.0381770460510618\n",
      "│   │   │   ├── 3.1.1.3. -- Graph-based database models: p_gain=0.0, p_loss=0.02863278453829635\n",
      "│   │   │   │   ├── 3.1.1.3.1. -- Hierarchical data models: p_gain=0.0164638511095204, p_loss=0.00023860653781913624\n",
      "│   │   │   │   └── 3.1.1.3.2. -- Network data models: p_gain=0.06871868289191124, p_loss=0.009305654974946313\n",
      "│   │   │   ├── 3.1.1.4. -- Physical data models: p_gain=0.03435934144595562, p_loss=0.032689095681221664\n",
      "│   │   │   └── 3.1.1.5. -- Data model extensions: p_gain=0.012407539966595084, p_loss=0.0021474588403722263\n",
      "│   │   │       ├── 3.1.1.5.1. -- Semi-structured data: p_gain=0.07945597709377238, p_loss=0.013600572655690766\n",
      "│   │   │       ├── 3.1.1.5.2. -- Data streams: p_gain=0.02600811262228585, p_loss=0.00763540921021236\n",
      "│   │   │       ├── 3.1.1.5.3. -- Data provenance: p_gain=0.0, p_loss=0.048437127177284656\n",
      "│   │   │       ├── 3.1.1.5.4. -- Incomplete data: p_gain=0.0, p_loss=0.047721307563827246\n",
      "│   │   │       ├── 3.1.1.5.5. -- Temporal data: p_gain=0.013600572655690766, p_loss=0.011691720353137676\n",
      "│   │   │       ├── 3.1.1.5.6. -- Uncertainty: p_gain=0.01789549033643522, p_loss=0.046051061799093296\n",
      "│   │   │       └── 3.1.1.5.7. -- Inconsistent data: p_gain=0.0, p_loss=0.048437127177284656\n",
      "│   │   ├── 3.1.2. -- Data structures: p_gain=0.0004772130756382725, p_loss=0.006919589596754951\n",
      "│   │   │   ├── 3.1.2.1. -- Data access methods: p_gain=0.010021474588403722, p_loss=0.0\n",
      "│   │   │   │   ├── 3.1.2.1.1. -- Multidimensional range search: p_gain=0.0004772130756382725, p_loss=0.0355523741350513\n",
      "│   │   │   │   ├── 3.1.2.1.2. -- Data scans: p_gain=0.05416368408494393, p_loss=0.022667621092817943\n",
      "│   │   │   │   ├── 3.1.2.1.3. -- Point lookups: p_gain=0.11476974469100454, p_loss=0.04581245526127416\n",
      "│   │   │   │   ├── 3.1.2.1.4. -- Unidimensional range search: p_gain=0.005965163445478406, p_loss=0.0329277022190408\n",
      "│   │   │   │   └── 3.1.2.1.5. -- Proximity search: p_gain=0.04127893104271057, p_loss=0.02672393223574326\n",
      "│   │   │   └── 3.1.2.2. -- Data layout: p_gain=0.0, p_loss=0.03722261989978525\n",
      "│   │   │       ├── 3.1.2.2.1. -- Data compression: p_gain=0.0, p_loss=0.00190885230255309\n",
      "│   │   │       ├── 3.1.2.2.2. -- Data encryption: p_gain=0.0, p_loss=0.00190885230255309\n",
      "│   │   │       └── 3.1.2.2.3. -- Record and block layout: p_gain=0.1579575280362682, p_loss=0.0\n",
      "│   │   ├── 3.1.3. -- Database management system engines: p_gain=0.0004772130756382725, p_loss=0.0021474588403722263\n",
      "│   │   │   ├── 3.1.3.1. -- DBMS engine architectures: p_gain=0.03698401336196612, p_loss=0.033882128370317344\n",
      "│   │   │   ├── 3.1.3.2. -- Database query processing: p_gain=0.08303507516105942, p_loss=0.02219040801717967\n",
      "│   │   │   │   ├── 3.1.3.2.1. -- Query optimization: p_gain=0.02982581722739203, p_loss=0.04700548795036984\n",
      "│   │   │   │   ├── 3.1.3.2.2. -- Query operators: p_gain=0.004294917680744453, p_loss=0.07277499403483656\n",
      "│   │   │   │   ├── 3.1.3.2.3. -- Query planning: p_gain=0.016225244571701266, p_loss=0.05392507754712479\n",
      "│   │   │   │   └── 3.1.3.2.3. -- Join algorithms: p_gain=0.12097351467430208, p_loss=0.03483655452159389\n",
      "│   │   │   ├── 3.1.3.3. -- Database transaction processing: p_gain=0.004533524218563589, p_loss=0.013839179193509903\n",
      "│   │   │   │   ├── 3.1.3.3.1. -- Data locking: p_gain=0.10713433548079217, p_loss=0.005010737294201861\n",
      "│   │   │   │   ├── 3.1.3.3.2. -- Transaction logging: p_gain=0.0, p_loss=0.03459794798377475\n",
      "│   │   │   │   └── 3.1.3.3.3. -- Database recovery: p_gain=0.02910999761393462, p_loss=0.00572655690765927\n",
      "│   │   │   ├── 3.1.3.4. -- Record and buffer management: p_gain=0.0, p_loss=0.04390360295872107\n",
      "│   │   │   ├── 3.1.3.5. -- Parallel and distributed DBMSs: p_gain=0.006919589596754951, p_loss=0.04247196373180625\n",
      "│   │   │   │   ├── 3.1.3.5.1. -- Key-value stores: p_gain=0.0190885230255309, p_loss=0.0\n",
      "│   │   │   │   ├── 3.1.3.5.2. -- MapReduce-based systems: p_gain=0.0004772130756382725, p_loss=0.008351228823669768\n",
      "│   │   │   │   └── 3.1.3.5.3. -- Relational parallel and distributed DBMSs: p_gain=0.0014316392269148174, p_loss=0.0014316392269148174\n",
      "│   │   │   ├── 3.1.3.6. -- Triggers and rules: p_gain=0.1362443330947268, p_loss=0.04271057026962539\n",
      "│   │   │   ├── 3.1.3.7. -- Database views: p_gain=0.07563827248866618, p_loss=0.015509424958243857\n",
      "│   │   │   ├── 3.1.3.8. -- Integrity checking: p_gain=0.07874015748031496, p_loss=0.03841565258888094\n",
      "│   │   │   ├── 3.1.3.9. -- Distributed database transactions: p_gain=0.0164638511095204, p_loss=0.018134096874254355\n",
      "│   │   │   │   ├── 3.1.3.9.1. -- Distributed data locking: p_gain=0.0217131949415414, p_loss=0.0071581961345740875\n",
      "│   │   │   │   ├── 3.1.3.9.2. -- Deadlocks: p_gain=0.12192794082557862, p_loss=0.03435934144595562\n",
      "│   │   │   │   └── 3.1.3.9.3. -- Distributed database recovery: p_gain=0.015986638033882127, p_loss=0.009305654974946313\n",
      "│   │   │   ├── 3.1.3.10. -- Main memory engines: p_gain=0.000954426151276545, p_loss=0.043664996420901936\n",
      "│   │   │   ├── 3.1.3.11. -- Online analytical processing engines: p_gain=0.008112622285850633, p_loss=0.04271057026962539\n",
      "│   │   │   └── 3.1.3.12. -- Stream management: p_gain=0.00023860653781913624, p_loss=0.04390360295872107\n",
      "│   │   ├── 3.1.4. -- Query languages: p_gain=0.07826294440467668, p_loss=0.01527081842042472\n",
      "│   │   │   ├── 3.1.4.1. -- Relational database query languages: p_gain=0.0, p_loss=0.028871391076115485\n",
      "│   │   │   │   └── 3.1.4.1.1. -- Structured Query Language: p_gain=0.05416368408494393, p_loss=0.0\n",
      "│   │   │   ├── 3.1.4.2. -- XML query languages: p_gain=0.04223335719398712, p_loss=0.02672393223574326\n",
      "│   │   │   │   ├── 3.1.4.2.1 -- XPath: p_gain=0.12407539966595085, p_loss=0.06824146981627296\n",
      "│   │   │   │   └── 3.1.4.2.2. -- XQuery: p_gain=0.1224051539012169, p_loss=0.0071581961345740875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "│   │   │   ├── 3.1.4.3. -- Query languages for non-relational engines: p_gain=0.0, p_loss=0.03435934144595562\n",
      "│   │   │   │   └── 3.1.4.3.1. -- MapReduce languages: p_gain=0.12813171080887617, p_loss=0.0\n",
      "│   │   │   └── 3.1.4.4. -- Call level interfaces: p_gain=0.0007158196134574087, p_loss=0.10832736816988786\n",
      "│   │   └── 3.1.5. -- Information integration: p_gain=0.0004772130756382725, p_loss=0.015748031496062992\n",
      "│   │       ├── 3.1.5.1. -- Deduplication: p_gain=0.0, p_loss=0.030303030303030304\n",
      "│   │       ├── 3.1.5.2. -- Extraction, transformation and loading: p_gain=0.0, p_loss=0.030303030303030304\n",
      "│   │       ├── 3.1.5.3. -- Data exchange: p_gain=0.10880458124552612, p_loss=0.0021474588403722263\n",
      "│   │       ├── 3.1.5.4. -- Data cleaning: p_gain=0.00023860653781913624, p_loss=0.015748031496062992\n",
      "│   │       ├── 3.1.5.5. -- Wrappers (data mining): p_gain=0.00190885230255309, p_loss=0.014793605344786447\n",
      "│   │       ├── 3.1.5.6. -- Mediators and data integration: p_gain=0.0, p_loss=0.030303030303030304\n",
      "│   │       ├── 3.1.5.7. -- Entity resolution: p_gain=0.0, p_loss=0.030303030303030304\n",
      "│   │       ├── 3.1.5.8. -- Data warehouses: p_gain=0.0004772130756382725, p_loss=0.0217131949415414\n",
      "│   │       └── 3.1.5.9. -- Federated databases: p_gain=0.03507516105941303, p_loss=0.0026246719160104987\n",
      "│   ├── 3.2. -- Information systems applications: p_gain=0.0, p_loss=0.0026246719160104987\n",
      "│   │   └── 3.2.1. -- Data mining: p_gain=0.0, p_loss=0.0\n",
      "│   │       ├── 3.2.1.1. -- Data cleaning: p_gain=0.008351228823669768, p_loss=0.016941064185158672\n",
      "│   │       ├── 3.2.1.2. -- Collaborative filtering: p_gain=0.0, p_loss=0.02290622763063708\n",
      "│   │       │   ├── 3.2.1.2.1 -- Item-based: p_gain=0.011214507277499404, p_loss=0.0\n",
      "│   │       │   └── 3.2.1.2.2 -- Scalable: p_gain=0.0, p_loss=0.0004772130756382725\n",
      "│   │       ├── 3.2.1.3. -- Association rules: p_gain=0.0, p_loss=0.023383440706275353\n",
      "│   │       │   ├── 3.2.1.3.1 -- Types of association rules: p_gain=0.0, p_loss=0.0\n",
      "│   │       │   ├── 3.2.1.3.2 -- Interestingness: p_gain=0.0, p_loss=0.0\n",
      "│   │       │   └── 3.2.1.3.3 -- Parallel computation: p_gain=0.0, p_loss=0.0\n",
      "│   │       ├── 3.2.1.4. -- Clustering: p_gain=0.03579098067287044, p_loss=0.02290622763063708\n",
      "│   │       │   ├── 3.2.1.4.1 -- Massive data clustering: p_gain=0.0, p_loss=0.0004772130756382725\n",
      "│   │       │   ├── 3.2.1.4.2 -- Consensus clustering: p_gain=0.0, p_loss=0.0004772130756382725\n",
      "│   │       │   ├── 3.2.1.4.3 -- Fuzzy clustering: p_gain=0.00023860653781913624, p_loss=0.0004772130756382725\n",
      "│   │       │   ├── 3.2.1.4.4 -- Additive clustering: p_gain=0.0, p_loss=0.0004772130756382725\n",
      "│   │       │   ├── 3.2.1.4.5 -- Feature weight clustering: p_gain=0.010975900739680267, p_loss=0.0\n",
      "│   │       │   ├── 3.2.1.4.6 -- Conceptual clustering: p_gain=0.0, p_loss=0.0004772130756382725\n",
      "│   │       │   └── 3.2.1.4.7 -- Biclustering: p_gain=0.00023860653781913624, p_loss=0.0004772130756382725\n",
      "│   │       ├── 3.2.1.5. -- Nearest-neighbor search: p_gain=0.06609401097590074, p_loss=0.010021474588403722\n",
      "│   │       ├── 3.2.1.6. -- Data stream mining: p_gain=0.008589835361488905, p_loss=0.01717967072297781\n",
      "│   │       ├── 3.2.1.7 -- Graph mining: p_gain=0.0, p_loss=0.021474588403722263\n",
      "│   │       │   ├── 3.2.1.7.1 -- Graph partitioning: p_gain=0.0, p_loss=0.00190885230255309\n",
      "│   │       │   ├── 3.2.1.7.2 -- Frequent graph mining: p_gain=0.032689095681221664, p_loss=0.0\n",
      "│   │       │   ├── 3.2.1.7.3 -- Graph based conceptual clustering: p_gain=0.03579098067287044, p_loss=0.00190885230255309\n",
      "│   │       │   ├── 3.2.1.7.4 -- Anomaly detection: p_gain=0.0, p_loss=0.00190885230255309\n",
      "│   │       │   └── 3.2.1.7.5 -- Critical nodes detection: p_gain=0.0, p_loss=0.00190885230255309\n",
      "│   │       ├── 3.2.1.8. -- Process mining: p_gain=0.05654974946313529, p_loss=0.015509424958243857\n",
      "│   │       ├── 3.2.1.11 -- Text mining: p_gain=0.0, p_loss=0.02028155571462658\n",
      "│   │       │   ├── 3.2.1.11.1 -- Text categorization: p_gain=0.0434263898830828, p_loss=0.0\n",
      "│   │       │   └── 3.2.1.11.2 -- Key-phrase indexing: p_gain=0.02028155571462658, p_loss=0.003101884991648771\n",
      "│   │       ├── 3.2.1.10. -- Data mining tools: p_gain=0.02052016225244572, p_loss=0.015032211882605583\n",
      "│   │       ├── 3.2.1.9 -- Sequence mining: p_gain=0.00572655690765927, p_loss=0.021474588403722263\n",
      "│   │       │   ├── 3.2.1.9.1. -- Rule and pattern discovery: p_gain=0.013839179193509903, p_loss=0.0035790980672870437\n",
      "│   │       │   ├── 3.2.1.9.2. -- Trajectory clustering: p_gain=0.0434263898830828, p_loss=0.00572655690765927\n",
      "│   │       │   └── 3.2.1.9.3 -- Market graph: p_gain=0.2230971128608924, p_loss=0.00023860653781913624\n",
      "│   │       └── 3.2.1.12 -- Formal concept analysis: p_gain=0.04008589835361489, p_loss=0.023144834168456217\n",
      "│   ├── 3.3. -- World Wide Web: p_gain=0.010021474588403722, p_loss=0.015986638033882127\n",
      "│   │   └── 3.3.1. -- Web mining: p_gain=0.0, p_loss=0.0\n",
      "│   │       ├── 3.3.1.2. -- Site wrapping: p_gain=0.12598425196850394, p_loss=0.010975900739680267\n",
      "│   │       ├── 3.3.1.3. -- Data extraction and integration: p_gain=0.01765688379861608, p_loss=0.0071581961345740875\n",
      "│   │       │   ├── 3.3.1.3.1 -- Deep web: p_gain=0.06967310904318778, p_loss=0.006680983058935815\n",
      "│   │       │   ├── 3.3.1.3.2. -- Surfacing: p_gain=0.1362443330947268, p_loss=0.0011930326890956812\n",
      "│   │       │   └── 3.3.1.3.3. -- Search results deduplication: p_gain=0.0, p_loss=0.03054163684084944\n",
      "│   │       ├── 3.3.1.4. -- Web log analysis: p_gain=0.09401097590073967, p_loss=0.01980434263898831\n",
      "│   │       ├── 3.3.1.5. -- Traffic analysis: p_gain=0.07062753519446432, p_loss=0.01980434263898831\n",
      "│   │       └── 3.3.1.6 -- Knowledge discovery: p_gain=0.10450966356478167, p_loss=0.0033404915294679076\n",
      "│   └── 3.4. -- Information retrieval: p_gain=0.06824146981627296, p_loss=0.00190885230255309\n",
      "│       ├── 3.4.1. -- Document representation: p_gain=0.0011930326890956812, p_loss=0.03698401336196612\n",
      "│       │   ├── 3.4.1.1. -- Document structure: p_gain=0.09257933667382487, p_loss=0.008351228823669768\n",
      "│       │   ├── 3.4.1.2. -- Document topic models: p_gain=0.00882844189930804, p_loss=0.020042949176807445\n",
      "│       │   ├── 3.4.1.3. -- Content analysis and feature selection: p_gain=0.03579098067287044, p_loss=0.05654974946313529\n",
      "│       │   ├── 3.4.1.4. -- Data encoding and canonicalization: p_gain=0.015986638033882127, p_loss=0.05320925793366738\n",
      "│       │   ├── 3.4.1.5. -- Document collection models: p_gain=0.0, p_loss=0.05320925793366738\n",
      "│       │   ├── 3.4.1.6. -- Ontologies: p_gain=0.05153901216893343, p_loss=0.04390360295872107\n",
      "│       │   ├── 3.4.1.7. -- Dictionaries: p_gain=0.0, p_loss=0.05654974946313529\n",
      "│       │   └── 3.4.1.8. -- Thesauri: p_gain=0.15771892149844904, p_loss=0.05511811023622047\n",
      "│       ├── 3.4.2. -- Information retrieval query processing: p_gain=0.005965163445478406, p_loss=0.05893581484132665\n",
      "│       │   ├── 3.4.2.1. -- Query representation: p_gain=0.029587210689572895, p_loss=0.03769983297542353\n",
      "│       │   ├── 3.4.2.2. -- Query intent: p_gain=0.028871391076115485, p_loss=0.0011930326890956812\n",
      "│       │   ├── 3.4.2.3. -- Query log analysis: p_gain=0.07730851825340014, p_loss=0.0329277022190408\n",
      "│       │   ├── 3.4.2.4. -- Query suggestion: p_gain=0.007396802672393224, p_loss=0.03340491529467907\n",
      "│       │   └── 3.4.2.5. -- Query reformulation: p_gain=0.0, p_loss=0.03937007874015748\n",
      "│       ├── 3.4.3. -- Users and interactive retrieval: p_gain=0.030064423765211165, p_loss=0.0434263898830828\n",
      "│       │   ├── 3.4.3.1. -- Personalization: p_gain=0.022429014554998808, p_loss=0.0632307325220711\n",
      "│       │   ├── 3.4.3.2. -- Task models: p_gain=0.049868766404199474, p_loss=0.045573848723455025\n",
      "│       │   ├── 3.4.3.3. -- Search interfaces: p_gain=0.006680983058935815, p_loss=0.021951801479360534\n",
      "│       │   └── 3.4.3.4. -- Collaborative search: p_gain=0.0016702457647339538, p_loss=0.03937007874015748\n",
      "│       ├── 3.4.4. -- Retrieval models and ranking: p_gain=0.0, p_loss=0.01527081842042472\n",
      "│       │   ├── 3.4.4.1. -- Rank aggregation: p_gain=0.0, p_loss=0.077069911715581\n",
      "│       │   ├── 3.4.4.2. -- Probabilistic retrieval models: p_gain=0.006680983058935815, p_loss=0.01837270341207349\n",
      "│       │   ├── 3.4.4.3. -- Language models: p_gain=0.148890479599141, p_loss=0.0329277022190408\n",
      "│       │   ├── 3.4.4.4. -- Similarity measures: p_gain=0.0035790980672870437, p_loss=0.07516105941302792\n",
      "│       │   ├── 3.4.4.5. -- Learning to rank: p_gain=0.03579098067287044, p_loss=0.077069911715581\n",
      "│       │   ├── 3.4.4.6. -- Combination, fusion and federated search: p_gain=0.0, p_loss=0.077069911715581\n",
      "│       │   ├── 3.4.4.7. -- Information retrieval diversity: p_gain=0.00023860653781913624, p_loss=0.021235981865903125\n",
      "│       │   ├── 3.4.4.8. -- Top-k retrieval in databases: p_gain=0.047721307563827246, p_loss=0.009782868050584585\n",
      "│       │   └── 3.4.4.9. -- Novelty in information retrieval: p_gain=0.0004772130756382725, p_loss=0.021235981865903125\n",
      "│       ├── 3.4.5. -- Retrieval tasks and goals: p_gain=0.0, p_loss=0.02099737532808399\n",
      "│       │   ├── 3.4.5.1. -- Question answering: p_gain=0.0004772130756382725, p_loss=0.0711047482701026\n",
      "│       │   ├── 3.4.5.2. -- Document filtering: p_gain=0.0, p_loss=0.03674540682414698\n",
      "│       │   ├── 3.4.5.3. -- Recommender systems: p_gain=0.0026246719160104987, p_loss=0.07134335480792174\n",
      "│       │   ├── 3.4.5.4. -- Information extraction: p_gain=0.0, p_loss=0.0711047482701026\n",
      "│       │   ├── 3.4.5.5. -- Sentiment analysis: p_gain=0.046051061799093296, p_loss=0.0711047482701026\n",
      "│       │   ├── 3.4.5.6. -- Expert search: p_gain=0.049868766404199474, p_loss=0.03435934144595562\n",
      "│       │   ├── 3.4.5.7. -- Near-duplicate and plagiarism detection: p_gain=0.0, p_loss=0.07134335480792174\n",
      "│       │   ├── 3.4.5.8. -- Clustering and classification: p_gain=0.04271057026962539, p_loss=0.0691958959675495\n",
      "│       │   ├── 3.4.5.9. -- Summarization: p_gain=0.043664996420901936, p_loss=0.05082319255547602\n",
      "│       │   └── 3.4.5.10. -- Business intelligence: p_gain=0.0071581961345740875, p_loss=0.06943450250536864\n",
      "│       ├── 3.4.6. -- Evaluation of retrieval results: p_gain=0.0, p_loss=0.02791696492483894\n",
      "│       │   ├── 3.4.6.1. -- Test collections: p_gain=0.0, p_loss=0.06442376521116679\n",
      "│       │   ├── 3.4.6.2. -- Relevance assessment: p_gain=0.0, p_loss=0.06442376521116679\n",
      "│       │   ├── 3.4.6.3. -- Retrieval effectiveness: p_gain=0.000954426151276545, p_loss=0.0016702457647339538\n",
      "│       │   ├── 3.4.6.4. -- Retrieval efficiency: p_gain=0.01145311381531854, p_loss=0.0\n",
      "│       │   └── 3.4.6.5. -- Presentation of retrieval results: p_gain=0.003101884991648771, p_loss=0.024337866857551897\n",
      "│       └── 3.4.7. -- Specialized information retrieval: p_gain=0.06824146981627296, p_loss=0.005010737294201861\n",
      "│           ├── 3.4.7.1. -- Structure and multilingual text search: p_gain=0.000954426151276545, p_loss=0.03173466952994512\n",
      "│           │   ├── 3.4.7.1.1. -- Structured text search: p_gain=0.08279646862324028, p_loss=0.03674540682414698\n",
      "│           │   ├── 3.4.7.1.2. -- Mathematics retrieval: p_gain=0.0026246719160104987, p_loss=0.05273204485802911\n",
      "│           │   ├── 3.4.7.1.3. -- Chemical and biochemical retrieval: p_gain=0.009782868050584585, p_loss=0.051777618706752564\n",
      "│           │   └── 3.4.7.1.4. -- Multilingual and cross-lingual retrieval: p_gain=0.0004772130756382725, p_loss=0.06561679790026247\n",
      "│           ├── 3.4.7.2. -- Multimedia and multimodal retrieval: p_gain=0.0, p_loss=0.0040563111429253165\n",
      "│           │   ├── 3.4.7.2.1. -- Image search: p_gain=0.01145311381531854, p_loss=0.04294917680744453\n",
      "│           │   ├── 3.4.7.2.2. -- Video search: p_gain=0.00023860653781913624, p_loss=0.04891434025292293\n",
      "│           │   ├── 3.4.7.2.3. -- Speech / audio search: p_gain=0.0007158196134574087, p_loss=0.06442376521116679\n",
      "│           │   └── 3.4.7.2.4. -- Music retrieval: p_gain=0.03435934144595562, p_loss=0.07826294440467668\n",
      "│           └── 3.4.7.3. -- Environment-specific retrieval: p_gain=0.0014316392269148174, p_loss=0.03841565258888094\n",
      "│               ├── 3.4.7.3.1. -- Enterprise search: p_gain=0.0, p_loss=0.0434263898830828\n",
      "│               ├── 3.4.7.3.2. -- Desktop search: p_gain=0.031257456454306845, p_loss=0.0\n",
      "│               └── 3.4.7.3.3. -- Web and social media search: p_gain=0.00023860653781913624, p_loss=0.03340491529467907\n",
      "├── 4.+0 -- Human-centered computing: p_gain=0.05869720830350752, p_loss=0.0026246719160104987\n",
      "│   └── 4.1. -- Visualization: p_gain=0.0, p_loss=0.0\n",
      "│       ├── 4.1.2. -- Visualization techniques: p_gain=0.1717967072297781, p_loss=0.024576473395371033\n",
      "│       │   ├── 4.1.2.1. -- Treemaps: p_gain=0.05559532331185874, p_loss=0.1381531853972799\n",
      "│       │   ├── 4.1.2.2. -- Hyperbolic trees: p_gain=0.01980434263898831, p_loss=0.18635170603674542\n",
      "│       │   ├── 4.1.2.3. -- Heat maps: p_gain=0.06609401097590074, p_loss=0.1317108088761632\n",
      "│       │   ├── 4.1.2.4. -- Graph drawings: p_gain=0.08112622285850632, p_loss=0.08637556669052732\n",
      "│       │   ├── 4.1.2.5. -- Dendrograms: p_gain=0.09854450011930327, p_loss=0.0849439274636125\n",
      "│       │   ├── 4.1.2.6. -- Cladograms: p_gain=0.13314244810307801, p_loss=0.07158196134574088\n",
      "│       │   └── 4.1.2.7 -- Elastic maps: p_gain=0.05010737294201861, p_loss=0.12455261274158912\n",
      "│       ├── 4.1.3. -- Visualization application domains: p_gain=0.008112622285850633, p_loss=0.0016702457647339538\n",
      "│       │   ├── 4.1.3.1. -- Scientific visualization: p_gain=0.0, p_loss=0.02863278453829635\n",
      "│       │   ├── 4.1.3.2. -- Visual analytics: p_gain=0.04151753758052971, p_loss=0.04438081603435934\n",
      "│       │   ├── 4.1.3.3. -- Geographic visualization: p_gain=0.028871391076115485, p_loss=0.00023860653781913624\n",
      "│       │   └── 4.1.3.4. -- Information visualization: p_gain=0.0, p_loss=0.06370794559770938\n",
      "│       ├── 4.1.4. -- Visualization systems and tools: p_gain=0.0, p_loss=0.0016702457647339538\n",
      "│       │   └── 4.1.4.1. -- Visualization toolkits: p_gain=0.046051061799093296, p_loss=0.0\n",
      "│       ├── 4.1.5. -- Visualization theory, concepts and paradigms: p_gain=0.0011930326890956812, p_loss=0.015032211882605583\n",
      "│       ├── 4.1.6. -- Empirical studies in visualization: p_gain=0.0004772130756382725, p_loss=0.018134096874254355\n",
      "│       └── 4.1.7. -- Visualization design and evaluation methods: p_gain=0.0, p_loss=0.06132188021951802\n",
      "└── 5.+0 -- Computing methodologies: p_gain=0.00190885230255309, p_loss=0.0\n",
      "    ├── 5.1. -- Artificial intelligence: p_gain=0.006680983058935815, p_loss=0.00023860653781913624\n",
      "    │   ├── 5.1.1. -- Natural language processing: p_gain=0.005965163445478406, p_loss=0.002863278453829635\n",
      "    │   │   ├── 5.1.1.2. -- Information extraction: p_gain=0.00023860653781913624, p_loss=0.017418277260796946\n",
      "    │   │   ├── 5.1.1.3. -- Machine translation: p_gain=0.027678358387019805, p_loss=0.015748031496062992\n",
      "    │   │   ├── 5.1.1.4. -- Discourse, dialogue and pragmatics: p_gain=0.0, p_loss=0.017418277260796946\n",
      "    │   │   ├── 5.1.1.5. -- Natural language generation: p_gain=0.018611309949892626, p_loss=0.009782868050584585\n",
      "    │   │   ├── 5.1.1.6. -- Speech recognition: p_gain=0.0789787640181341, p_loss=0.010498687664041995\n",
      "    │   │   ├── 5.1.1.7. -- Lexical semantics: p_gain=0.0, p_loss=0.016941064185158672\n",
      "    │   │   │   └── 5.1.1.7.1 -- Wikipedia based semantics: p_gain=0.0040563111429253165, p_loss=0.0\n",
      "    │   │   ├── 5.1.1.8. -- Phonology / morphology: p_gain=0.1474588403722262, p_loss=0.0035790980672870437\n",
      "    │   │   └── 5.1.1.9. -- Language resources: p_gain=0.21426867096158433, p_loss=0.00190885230255309\n",
      "    │   ├── 5.1.2. -- Knowledge representation and reasoning: p_gain=0.0016702457647339538, p_loss=0.0011930326890956812\n",
      "    │   │   ├── 5.1.2.1. -- Description logics: p_gain=0.0004772130756382725, p_loss=0.014316392269148175\n",
      "    │   │   ├── 5.1.2.2. -- Semantic networks: p_gain=0.07158196134574088, p_loss=0.013600572655690766\n",
      "    │   │   ├── 5.1.2.3. -- Nonmonotonic, default reasoning and belief revision: p_gain=0.0004772130756382725, p_loss=0.01455499880696731\n",
      "    │   │   ├── 5.1.2.4. -- Probabilistic reasoning: p_gain=0.023622047244094488, p_loss=0.011214507277499404\n",
      "    │   │   ├── 5.1.2.5. -- Vagueness and fuzzy logic: p_gain=0.21355285134812693, p_loss=0.005249343832020997\n",
      "    │   │   ├── 5.1.2.6. -- Causal reasoning and diagnostics: p_gain=0.010021474588403722, p_loss=0.012168933428775949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    │   │   ├── 5.1.2.7. -- Temporal reasoning: p_gain=0.000954426151276545, p_loss=0.014316392269148175\n",
      "    │   │   ├── 5.1.2.8. -- Cognitive robotics: p_gain=0.07945597709377238, p_loss=0.00954426151276545\n",
      "    │   │   ├── 5.1.2.9. -- Ontology engineering: p_gain=0.030064423765211165, p_loss=0.008351228823669768\n",
      "    │   │   ├── 5.1.2.10. -- Logic programming and answer set programming: p_gain=0.08351228823669768, p_loss=0.008589835361488905\n",
      "    │   │   ├── 5.1.2.11. -- Spatial and physical reasoning: p_gain=0.0, p_loss=0.014793605344786447\n",
      "    │   │   └── 5.1.2.12. -- Reasoning about belief and knowledge: p_gain=0.12383679312813171, p_loss=0.002863278453829635\n",
      "    │   └── 5.1.3. -- Computer vision: p_gain=0.021235981865903125, p_loss=0.0023860653781913625\n",
      "    │       ├── 5.1.3.1. -- Computer vision problems: p_gain=0.0, p_loss=0.0004772130756382725\n",
      "    │       │   ├── 5.1.3.1.1. -- Interest point and salient region detections: p_gain=0.0, p_loss=0.032689095681221664\n",
      "    │       │   ├── 5.1.3.1.2. -- Image segmentation: p_gain=0.00023860653781913624, p_loss=0.02099737532808399\n",
      "    │       │   ├── 5.1.3.1.3. -- Video segmentation: p_gain=0.0, p_loss=0.03173466952994512\n",
      "    │       │   ├── 5.1.3.1.4. -- Shape inference: p_gain=0.0035790980672870437, p_loss=0.03221188260558339\n",
      "    │       │   ├── 5.1.3.1.5. -- Object detection: p_gain=0.0004772130756382725, p_loss=0.032689095681221664\n",
      "    │       │   ├── 5.1.3.1.6. -- Object recognition: p_gain=0.047482701026008114, p_loss=0.007396802672393224\n",
      "    │       │   ├── 5.1.3.1.7. -- Object identification: p_gain=0.0004772130756382725, p_loss=0.032689095681221664\n",
      "    │       │   ├── 5.1.3.1.8. -- Tracking: p_gain=0.15032211882605584, p_loss=0.026246719160104987\n",
      "    │       │   ├── 5.1.3.1.9. -- Reconstruction: p_gain=0.033882128370317344, p_loss=0.011691720353137676\n",
      "    │       │   └── 5.1.3.1.10. -- Matching: p_gain=0.03531376759723216, p_loss=0.03221188260558339\n",
      "    │       └── 5.1.3.2. -- Computer vision representations: p_gain=0.0071581961345740875, p_loss=0.006442376521116679\n",
      "    │           ├── 5.1.3.2.1. -- Image representations: p_gain=0.0, p_loss=0.015509424958243857\n",
      "    │           │   └── 5.1.3.2.1.1 -- 2D PCA: p_gain=0.09353376282510141, p_loss=0.0\n",
      "    │           ├── 5.1.3.2.2. -- Shape representations: p_gain=0.0021474588403722263, p_loss=0.0033404915294679076\n",
      "    │           ├── 5.1.3.2.3. -- Appearance and texture representations: p_gain=0.0, p_loss=0.004772130756382725\n",
      "    │           └── 5.1.3.2.4. -- Hierarchical representations: p_gain=0.0004772130756382725, p_loss=0.0071581961345740875\n",
      "    └── 5.2. -- Machine learning: p_gain=0.03579098067287044, p_loss=0.0\n",
      "        ├── 5.2.1. -- Learning paradigms: p_gain=0.0, p_loss=0.00190885230255309\n",
      "        │   ├── 5.2.1.1. -- Supervised learning: p_gain=0.0, p_loss=0.00023860653781913624\n",
      "        │   │   ├── 5.2.1.1.1. -- Ranking: p_gain=0.2001908852302553, p_loss=0.03674540682414698\n",
      "        │   │   ├── 5.2.1.1.2. -- Learning to rank: p_gain=0.0, p_loss=0.00572655690765927\n",
      "        │   │   ├── 5.2.1.1.3. -- Supervised learning by classification: p_gain=0.009305654974946313, p_loss=0.005010737294201861\n",
      "        │   │   ├── 5.2.1.1.4. -- Supervised learning by regression: p_gain=0.0, p_loss=0.00572655690765927\n",
      "        │   │   ├── 5.2.1.1.5. -- Structured outputs: p_gain=0.1395848246241947, p_loss=0.03865425912670007\n",
      "        │   │   └── 5.2.1.1.6. -- Cost-sensitive learning: p_gain=0.0, p_loss=0.00572655690765927\n",
      "        │   ├── 5.2.1.2. -- Unsupervised learning: p_gain=0.023144834168456217, p_loss=0.0408017179670723\n",
      "        │   │   ├── 5.2.1.2.1. -- Cluster analysis: p_gain=0.07229778095919828, p_loss=0.000954426151276545\n",
      "        │   │   ├── 5.2.1.2.2. -- Anomaly detection: p_gain=0.0, p_loss=0.024099260319732762\n",
      "        │   │   ├── 5.2.1.2.3. -- Mixture modeling: p_gain=0.03746122643760439, p_loss=0.0007158196134574087\n",
      "        │   │   ├── 5.2.1.2.4. -- Topic modeling: p_gain=0.0520162252445717, p_loss=0.000954426151276545\n",
      "        │   │   ├── 5.2.1.2.5. -- Source separation: p_gain=0.0, p_loss=0.024099260319732762\n",
      "        │   │   ├── 5.2.1.2.6. -- Motif discovery: p_gain=0.04509663564781675, p_loss=0.023622047244094488\n",
      "        │   │   └── 5.2.1.2.7. -- Dimensionality reduction and manifold learning: p_gain=0.0, p_loss=0.023860653781913623\n",
      "        │   │       ├── 5.2.1.2.7.1 -- Graph embedding: p_gain=0.05821999522786924, p_loss=0.0\n",
      "        │   │       └── 5.2.1.2.7.2 -- Supervised dimesionality reduction: p_gain=0.0, p_loss=0.00023860653781913624\n",
      "        │   ├── 5.2.1.3. -- Reinforcement learning: p_gain=0.0, p_loss=0.003101884991648771\n",
      "        │   │   ├── 5.2.1.3.1. -- Sequential decision making: p_gain=0.03531376759723216, p_loss=0.03579098067287044\n",
      "        │   │   ├── 5.2.1.3.2. -- Inverse reinforcement learning: p_gain=0.0, p_loss=0.00381770460510618\n",
      "        │   │   ├── 5.2.1.3.3. -- Apprenticeship learning: p_gain=0.0, p_loss=0.002863278453829635\n",
      "        │   │   ├── 5.2.1.3.4. -- Multi-agent reinforcement learning: p_gain=0.0, p_loss=0.03793843951324266\n",
      "        │   │   └── 5.2.1.3.5. -- Adversarial learning: p_gain=0.0, p_loss=0.002863278453829635\n",
      "        │   └── 5.2.1.4. -- Multi-task learning: p_gain=0.0, p_loss=0.005965163445478406\n",
      "        │       ├── 5.2.1.4.1. -- Transfer learning: p_gain=0.0, p_loss=0.0\n",
      "        │       ├── 5.2.1.4.2. -- Lifelong machine learning: p_gain=0.015509424958243857, p_loss=0.0\n",
      "        │       └── 5.2.1.4.3. -- Learning under covariate shift: p_gain=0.0, p_loss=0.0\n",
      "        ├── 5.2.2. -- Learning settings: p_gain=0.0, p_loss=0.007874015748031496\n",
      "        │   ├── 5.2.2.1. -- Batch learning: p_gain=0.0, p_loss=0.0\n",
      "        │   ├── 5.2.2.2. -- Online learning settings: p_gain=0.0, p_loss=0.0\n",
      "        │   ├── 5.2.2.3. -- Learning from demonstrations: p_gain=0.0, p_loss=0.00190885230255309\n",
      "        │   ├── 5.2.2.4. -- Learning from critiques: p_gain=0.0, p_loss=0.0\n",
      "        │   ├── 5.2.2.5. -- Learning from implicit feedback: p_gain=0.0, p_loss=0.0\n",
      "        │   ├── 5.2.2.6. -- Active learning settings: p_gain=0.0, p_loss=0.0\n",
      "        │   └── 5.2.2.7. -- Semi-supervised learning settings: p_gain=0.0, p_loss=0.03579098067287044\n",
      "        │       └── 5.2.2.7.1 -- Kernel approach: p_gain=0.08613696015270818, p_loss=0.0\n",
      "        ├── 5.2.3. -- Machine learning approaches: p_gain=0.000954426151276545, p_loss=0.0004772130756382725\n",
      "        │   ├── 5.2.3.1. -- Classification and regression trees: p_gain=0.0, p_loss=0.044142209496540206\n",
      "        │   │   ├── 5.2.3.1.1 -- Parallel implementation: p_gain=0.0, p_loss=0.0\n",
      "        │   │   ├── 5.2.3.1.2 -- Splittting criteria: p_gain=0.0, p_loss=0.0\n",
      "        │   │   └── 5.2.3.1.3 -- Model trees: p_gain=0.0572655690765927, p_loss=0.0\n",
      "        │   ├── 5.2.3.2. -- Kernel methods: p_gain=0.0033404915294679076, p_loss=0.03674540682414698\n",
      "        │   │   ├── 5.2.3.2.1. -- Kernel support vector machines: p_gain=0.0, p_loss=0.0016702457647339538\n",
      "        │   │   │   └── 5.2.3.2.1.1 -- Dynamic: p_gain=0.21760916249105225, p_loss=0.0\n",
      "        │   │   ├── 5.2.3.2.2. -- Gaussian processes: p_gain=0.08446671438797423, p_loss=0.0011930326890956812\n",
      "        │   │   ├── 5.2.3.2.3 -- Kernel Matrix: p_gain=0.15175375805297064, p_loss=0.007396802672393224\n",
      "        │   │   ├── 5.2.3.2.4 -- Kernel Independent components: p_gain=0.0004772130756382725, p_loss=0.010737294201861132\n",
      "        │   │   └── 5.2.3.2.5 -- Kernel-based clustering: p_gain=0.03579098067287044, p_loss=0.010737294201861132\n",
      "        │   ├── 5.2.3.3. -- Neural networks: p_gain=0.004294917680744453, p_loss=0.04056311142925316\n",
      "        │   │   ├── 5.2.3.3.1 -- Self organized map: p_gain=0.010021474588403722, p_loss=0.007396802672393224\n",
      "        │   │   ├── 5.2.3.3.2 -- Training approaches: p_gain=0.0, p_loss=0.0040563111429253165\n",
      "        │   │   │   └── 5.2.3.3.2.1 -- Evolutionary approach: p_gain=0.03769983297542353, p_loss=0.0\n",
      "        │   │   ├── 5.2.3.3.3 -- Representation: p_gain=0.0, p_loss=0.005965163445478406\n",
      "        │   │   │   ├── 5.2.3.3.3.1 -- Rule-based netwok archirtecture: p_gain=0.03364352183249821, p_loss=0.00190885230255309\n",
      "        │   │   │   └── 5.2.3.3.3.2 -- Fuzzy representation: p_gain=0.03722261989978525, p_loss=0.0\n",
      "        │   │   ├── 5.2.3.3.4 -- Evolving NN: p_gain=0.1152469577666428, p_loss=0.0016702457647339538\n",
      "        │   │   └── 5.2.3.3.5 -- Ensembling: p_gain=0.022667621092817943, p_loss=0.0035790980672870437\n",
      "        │   ├── 5.2.3.4. -- Logical and relational learning: p_gain=0.0, p_loss=0.008351228823669768\n",
      "        │   │   ├── 5.2.3.4.1. -- Inductive logic learning: p_gain=0.0, p_loss=0.0\n",
      "        │   │   └── 5.2.3.4.2. -- Statistical relational learning: p_gain=0.0, p_loss=0.03579098067287044\n",
      "        │   ├── 5.2.3.5. -- Learning in probabilistic graphical models: p_gain=0.03602958721068957, p_loss=0.0434263898830828\n",
      "        │   │   ├── 5.2.3.5.1. -- Maximum likelihood modeling: p_gain=0.03173466952994512, p_loss=0.000954426151276545\n",
      "        │   │   ├── 5.2.3.5.2. -- Maximum entropy modeling: p_gain=0.0, p_loss=0.03602958721068957\n",
      "        │   │   ├── 5.2.3.5.3. -- Maximum a posteriori modeling: p_gain=0.0, p_loss=0.03602958721068957\n",
      "        │   │   ├── 5.2.3.5.4. -- Mixture models: p_gain=0.04223335719398712, p_loss=0.0\n",
      "        │   │   ├── 5.2.3.5.5. -- Latent variable models: p_gain=0.0, p_loss=0.0026246719160104987\n",
      "        │   │   ├── 5.2.3.5.6. -- Bayesian network models: p_gain=0.03364352183249821, p_loss=0.0007158196134574087\n",
      "        │   │   └── 5.2.3.5.7. -- Markov network models: p_gain=0.03316630875685994, p_loss=0.0007158196134574087\n",
      "        │   ├── 5.2.3.6. -- Learning linear models: p_gain=0.00190885230255309, p_loss=0.03793843951324266\n",
      "        │   │   ├── 5.2.3.6.1. -- Perceptron algorithm: p_gain=0.15079933190169412, p_loss=0.000954426151276545\n",
      "        │   │   └── 5.2.3.6.2 -- Linear Discriminant Analysis: p_gain=0.0, p_loss=0.004772130756382725\n",
      "        │   │       └── 5.2.3.6.2.1 -- Tensor representation: p_gain=0.02982581722739203, p_loss=0.0\n",
      "        │   ├── 5.2.3.7. -- Factorization methods: p_gain=0.004294917680744453, p_loss=0.04247196373180625\n",
      "        │   │   ├── 5.2.3.7.1. -- Non-negative matrix factorization: p_gain=0.014793605344786447, p_loss=0.005965163445478406\n",
      "        │   │   ├── 5.2.3.7.2. -- Factor analysis: p_gain=0.07730851825340014, p_loss=0.0016702457647339538\n",
      "        │   │   ├── 5.2.3.7.3. -- Principal component analysis: p_gain=0.007874015748031496, p_loss=0.004533524218563589\n",
      "        │   │   │   ├── 5.2.3.7.3.1 -- 2D PCA: p_gain=0.11834884275829158, p_loss=0.0\n",
      "        │   │   │   └── 5.2.3.7.3.2 -- Sparse PCA: p_gain=0.09735146743020759, p_loss=0.0011930326890956812\n",
      "        │   │   ├── 5.2.3.7.4. -- Canonical correlation analysis: p_gain=0.03316630875685994, p_loss=0.0016702457647339538\n",
      "        │   │   ├── 5.2.3.7.6. -- Latent Dirichlet allocation: p_gain=0.0, p_loss=0.005965163445478406\n",
      "        │   │   ├── 5.2.3.7.8. -- Independent Component Analysis: p_gain=0.05273204485802911, p_loss=0.0016702457647339538\n",
      "        │   │   ├── 5.2.3.7.9 -- Nonlinear Principal Components: p_gain=0.00023860653781913624, p_loss=0.005965163445478406\n",
      "        │   │   └── 5.2.3.7.10 -- Multidimentional scaling: p_gain=0.0, p_loss=0.00572655690765927\n",
      "        │   │       └── 5.2.3.7.10.1 -- Least moduli: p_gain=0.027678358387019805, p_loss=0.0\n",
      "        │   ├── 5.2.3.8. -- Rule learning: p_gain=0.0, p_loss=0.04294917680744453\n",
      "        │   │   └── 5.2.3.8.1 -- Neuro-fuzzy approach: p_gain=0.12884753042233357, p_loss=0.0\n",
      "        │   ├── 5.2.3.9. -- Instance-based learning: p_gain=0.0, p_loss=0.008351228823669768\n",
      "        │   ├── 5.2.3.10. -- Markov decision processes: p_gain=0.08589835361488905, p_loss=0.03841565258888094\n",
      "        │   ├── 5.2.3.11. -- Partially-observable Markov decision processes: p_gain=0.048675733715103794, p_loss=0.04127893104271057\n",
      "        │   ├── 5.2.3.12. -- Stochastic games: p_gain=0.11882605583392986, p_loss=0.04199475065616798\n",
      "        │   ├── 5.2.3.13. -- Learning latent representations: p_gain=0.0, p_loss=0.044142209496540206\n",
      "        │   │   └── 5.2.3.13.1. -- Deep belief networks: p_gain=0.07659269863994274, p_loss=0.0\n",
      "        │   ├── 5.2.3.14 -- Multiresolution: p_gain=0.0, p_loss=0.044142209496540206\n",
      "        │   └── 5.2.3.15 -- Support vector machines: p_gain=0.1126222858506323, p_loss=0.04271057026962539\n",
      "        ├── 5.2.4. -- Machine learning algorithms: p_gain=0.0, p_loss=0.0035790980672870437\n",
      "        │   ├── 5.2.4.1. -- Dynamic programming for Markov decision processes: p_gain=0.0, p_loss=0.004294917680744453\n",
      "        │   │   ├── 5.2.4.1.1. -- Value iteration: p_gain=0.0, p_loss=0.03579098067287044\n",
      "        │   │   ├── 5.2.4.1.2. -- Q-learning: p_gain=0.0, p_loss=0.0\n",
      "        │   │   ├── 5.2.4.1.3. -- Policy iteration: p_gain=0.0, p_loss=0.03579098067287044\n",
      "        │   │   ├── 5.2.4.1.4. -- Temporal difference learning: p_gain=0.0, p_loss=0.0\n",
      "        │   │   └── 5.2.4.1.5. -- Approximate dynamic programming methods: p_gain=0.03602958721068957, p_loss=0.03579098067287044\n",
      "        │   ├── 5.2.4.2. -- Ensemble methods: p_gain=0.022429014554998808, p_loss=0.03602958721068957\n",
      "        │   │   ├── 5.2.4.2.1. -- Boosting: p_gain=0.011214507277499404, p_loss=0.004294917680744453\n",
      "        │   │   ├── 5.2.4.2.2. -- Bagging: p_gain=0.06012884753042233, p_loss=0.003101884991648771\n",
      "        │   │   └── 5.2.4.2.3. -- Fusion of classifiers: p_gain=0.07754712479121928, p_loss=0.02028155571462658\n",
      "        │   ├── 5.2.4.3. -- Spectral methods: p_gain=0.0, p_loss=0.04008589835361489\n",
      "        │   │   └── 5.2.4.3.1 -- Spectral clustering: p_gain=0.03579098067287044, p_loss=0.0\n",
      "        │   ├── 5.2.4.4. -- Feature selection: p_gain=0.01336196611787163, p_loss=0.03937007874015748\n",
      "        │   └── 5.2.4.5. -- Regularization: p_gain=0.0, p_loss=0.03984729181579575\n",
      "        │       └── 5.2.4.5.1 -- Generalized eigenvalue: p_gain=0.0054879503698401335, p_loss=0.0\n",
      "        └── 5.2.5. -- Cross-validation: p_gain=0.0, p_loss=0.043664996420901936\n"
     ]
    }
   ],
   "source": [
    "print_tree(lifter.taxonomy, only_name=False, attrs=['p_gain', 'p_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/lifter.pkl']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(lifter, 'data/lifter.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lifter = joblib.load( 'data/lifter.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "ML lift - hard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from anytree import LevelOrderIter\n",
    "from pgfs import EPS, logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipe import as_list, sort, as_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = pd.read_excel('data/cluster_topics_u_nonzero.xlsx', index_col=[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_memberships = (\n",
    "    clusters.loc[0, 'u']\n",
    "    .loc[lambda x: x > 0.1] # filter noise\n",
    "    .pipe(lambda x:  x / np.sqrt((x ** 2).sum())) # normalize\n",
    "    .apply(lambda x: 1) ## hard clusters!\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "lifted = lifter.lift_ml(cluster_memberships)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_elements = cluster_memberships | as_list | sort | as_set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1.1.1.10. -- Models of learning',\n",
       " '1.1.1.11. -- Query learning',\n",
       " '1.1.1.13.2. -- Inverse reinforcement learning',\n",
       " '1.1.1.13.3. -- Apprenticeship learning',\n",
       " '1.1.1.13.5. -- Adversarial learning',\n",
       " '1.1.1.14. -- Active learning',\n",
       " '1.1.1.15. -- Semi-supervised learning',\n",
       " '1.1.1.2. -- Boolean function learning',\n",
       " '1.1.1.3. -- Unsupervised learning and clustering',\n",
       " '1.1.1.8. -- Online learning theory',\n",
       " '1.1.1.9. -- Multi-agent learning',\n",
       " '3.4.4.5. -- Learning to rank',\n",
       " '5.2.1.1.2. -- Learning to rank',\n",
       " '5.2.1.1.3. -- Supervised learning by classification',\n",
       " '5.2.1.1.4. -- Supervised learning by regression',\n",
       " '5.2.1.1.6. -- Cost-sensitive learning',\n",
       " '5.2.1.3.2. -- Inverse reinforcement learning',\n",
       " '5.2.1.3.3. -- Apprenticeship learning',\n",
       " '5.2.1.3.5. -- Adversarial learning',\n",
       " '5.2.1.4.1. -- Transfer learning',\n",
       " '5.2.1.4.2. -- Lifelong machine learning',\n",
       " '5.2.1.4.3. -- Learning under covariate shift',\n",
       " '5.2.2.1. -- Batch learning',\n",
       " '5.2.2.2. -- Online learning settings',\n",
       " '5.2.2.3. -- Learning from demonstrations',\n",
       " '5.2.2.4. -- Learning from critiques',\n",
       " '5.2.2.5. -- Learning from implicit feedback',\n",
       " '5.2.2.6. -- Active learning settings',\n",
       " '5.2.3.4.1. -- Inductive logic learning',\n",
       " '5.2.3.9. -- Instance-based learning',\n",
       " '5.2.4.1.2. -- Q-learning',\n",
       " '5.2.4.1.4. -- Temporal difference learning'}"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = [i for i, j in lifted.events_n if j == 'loss']\n",
    "gains = [i for i, j in lifted.events_n if j == 'gain'] | as_set\n",
    "\n",
    "gaps = losses | as_list | sort\n",
    "heads = gains - cluster_elements | as_list | sort \n",
    "offs = cluster_elements & gains | as_list | sort "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['1.1.1.1. -- Sample complexity and generalization bounds',\n",
       "  '1.1.1.12. -- Structured prediction',\n",
       "  '1.1.1.13.1. -- Sequential decision making',\n",
       "  '1.1.1.13.4. -- Multi-agent reinforcement learning',\n",
       "  '1.1.1.16. -- Markov decision processes',\n",
       "  '1.1.1.17. -- Regret bounds',\n",
       "  '1.1.1.4. -- Kernel methods',\n",
       "  '1.1.1.5. -- Boosting',\n",
       "  '1.1.1.6. -- Bayesian analysis',\n",
       "  '1.1.1.7. -- Inductive inference',\n",
       "  '5.2.1.1.1. -- Ranking',\n",
       "  '5.2.1.1.5. -- Structured outputs',\n",
       "  '5.2.1.2. -- Unsupervised learning',\n",
       "  '5.2.1.3.1. -- Sequential decision making',\n",
       "  '5.2.1.3.4. -- Multi-agent reinforcement learning',\n",
       "  '5.2.2.7. -- Semi-supervised learning settings',\n",
       "  '5.2.3.1. -- Classification and regression trees',\n",
       "  '5.2.3.10. -- Markov decision processes',\n",
       "  '5.2.3.11. -- Partially-observable Markov decision processes',\n",
       "  '5.2.3.12. -- Stochastic games',\n",
       "  '5.2.3.13. -- Learning latent representations',\n",
       "  '5.2.3.14 -- Multiresolution',\n",
       "  '5.2.3.15 -- Support vector machines',\n",
       "  '5.2.3.2. -- Kernel methods',\n",
       "  '5.2.3.3. -- Neural networks',\n",
       "  '5.2.3.4.2. -- Statistical relational learning',\n",
       "  '5.2.3.5. -- Learning in probabilistic graphical models',\n",
       "  '5.2.3.6. -- Learning linear models',\n",
       "  '5.2.3.7. -- Factorization methods',\n",
       "  '5.2.3.8. -- Rule learning',\n",
       "  '5.2.4.1.1. -- Value iteration',\n",
       "  '5.2.4.1.3. -- Policy iteration',\n",
       "  '5.2.4.1.5. -- Approximate dynamic programming methods',\n",
       "  '5.2.4.2. -- Ensemble methods',\n",
       "  '5.2.4.3. -- Spectral methods',\n",
       "  '5.2.4.4. -- Feature selection',\n",
       "  '5.2.4.5. -- Regularization',\n",
       "  '5.2.5. -- Cross-validation'],\n",
       " ['1.1.1. -- Machine learning theory', '5.2. -- Machine learning'],\n",
       " ['3.4.4.5. -- Learning to rank'])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gaps, heads, offs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnyNode(events_i=[('2. + 0 -- Mathematics of computing', 'loss')], events_n=[], gain='', log_p_inherited=-7.242082359256822, log_p_not_inherited=-0.003825010640115581, lost='', n_gains=16, n_losses=3, name='2. + 0 -- Mathematics of computing', p_gain=0.00381770460510618, p_inherited=0.0007158196134575086, p_inherited_all=(0, 0), p_loss=0.0007158196134574087, p_not_inherited=0.9961822953948939, p_not_inherited_all=(0, 0), u=0.0)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lifted.children[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "lifting_df = []\n",
    "\n",
    "for cluster_id in clusters.index.levels[0]:\n",
    "    \n",
    "    cluster_memberships = (\n",
    "        clusters.loc[cluster_id, 'u']\n",
    "        .loc[lambda x: x > 0.1] # filter noise\n",
    "        .pipe(lambda x: x / np.sqrt((x ** 2).sum())) # normalize\n",
    "        .apply(lambda x: 1) ## hard clusters!\n",
    "        .to_dict()\n",
    "    )\n",
    "    lifted = lifter.lift_ml(cluster_memberships)\n",
    "    \n",
    "    cluster_elements = cluster_memberships | as_list | sort | as_set \n",
    "\n",
    "    losses = [i for i, j in lifted.events_n if j == 'loss']\n",
    "    gains = [i for i, j in lifted.events_n if j == 'gain'] | as_set\n",
    "\n",
    "    gaps = losses | as_list | sort\n",
    "    heads = gains - cluster_elements | as_list | sort \n",
    "    offs = cluster_elements & gains | as_list | sort \n",
    "    clust =  [(i, round(j, 3)) for i, j in cluster_memberships.items()]\n",
    "\n",
    "    df = (\n",
    "        pd.concat((\n",
    "            pd.Series(clust, name='topics_and_memberships'), \n",
    "            pd.Series(heads, name='head_subjects'), \n",
    "            pd.Series(gaps, name='gaps'), \n",
    "            pd.Series(offs, name='offshoots')), axis=1)\n",
    "        .assign(cluster_id=cluster_id)\n",
    "        .set_index('cluster_id', append=True)\n",
    "        .reorder_levels([1,0])\n",
    "    )\n",
    "\n",
    "    lifting_df.append(df)\n",
    "    \n",
    "lifting_df = pd.concat(lifting_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "lifting_df.to_excel('lifting_ml_crisp_results.xlsx', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "compare with parsimonious lifting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "lifting_pars_df = pd.read_excel('lifting_results.xlsx', index_col=[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "lifting_compare_df = lifting_pars_df\\\n",
    "    .join(lifting_df, rsuffix='_ml')\\\n",
    "    .drop('topics_and_memberships_ml', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "lifting_compare_df.to_excel('lifting_pars+ml_crisp_results.xlsx', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
