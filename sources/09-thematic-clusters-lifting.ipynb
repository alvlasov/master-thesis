{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run lib.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipe import *\n",
    "from taxonomy import print_tree\n",
    "import pgfs\n",
    "from anytree.importer import JsonImporter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "importer =  JsonImporter()\n",
    "\n",
    "with open('ds_taxonomy.json', 'r') as f:\n",
    "    tax = importer.read(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_tree(tax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = pd.read_excel('data/cluster_topics_u_nonzero.xlsx', index_col=[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_memberships = (\n",
    "    clusters.loc[5, 'u']\n",
    "    .loc[lambda x: x > 0.1] # filter noise\n",
    "    .pipe(lambda x: x / np.sqrt((x ** 2).sum())) # normalize\n",
    "    .to_dict()\n",
    ")\n",
    "cluster_elements = set(list(cluster_memberships.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lifter = pgfs.PGFS(tax, lmbda=0.1, gamma=0.9)\n",
    "\n",
    "lifted = lifter.lift(cluster_memberships)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'2.1.1.1. -- Bayesian networks',\n",
       " '2.1.1.2. -- Markov networks',\n",
       " '2.1.1.6. -- Causal networks',\n",
       " '3.1.1.3.2. -- Network data models',\n",
       " '3.1.3.9.2. -- Deadlocks',\n",
       " '3.4.7.2.1. -- Image search',\n",
       " '3.4.7.3.2. -- Desktop search',\n",
       " '5.1.2.2. -- Semantic networks',\n",
       " '5.2.3.13.1. -- Deep belief networks',\n",
       " '5.2.3.3.3.1 -- Rule-based netwok archirtecture',\n",
       " '5.2.3.5.6. -- Bayesian network models',\n",
       " '5.2.3.5.7. -- Markov network models'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster elements: \n",
      "('2.1.1.2. -- Markov networks', 0.456)\n",
      "('2.1.1.6. -- Causal networks', 0.454)\n",
      "('5.2.3.13.1. -- Deep belief networks', 0.418)\n",
      "('5.1.2.2. -- Semantic networks', 0.365)\n",
      "('2.1.1.1. -- Bayesian networks', 0.347)\n",
      "('5.2.3.5.7. -- Markov network models', 0.176)\n",
      "('5.2.3.5.6. -- Bayesian network models', 0.172)\n",
      "('5.2.3.3.3.1 -- Rule-based netwok archirtecture', 0.168)\n",
      "('3.4.7.2.1. -- Image search', 0.14)\n",
      "('3.1.1.3.2. -- Network data models', 0.137)\n",
      "('3.4.7.3.2. -- Desktop search', 0.134)\n",
      "('3.1.3.9.2. -- Deadlocks', 0.113)\n",
      "\n",
      "Head subjects: \n",
      "2.1.1. -- Probabilistic representations\n",
      "\n",
      "Offshoots: \n",
      "3.1.1.3.2. -- Network data models\n",
      "3.1.3.9.2. -- Deadlocks\n",
      "3.4.7.2.1. -- Image search\n",
      "3.4.7.3.2. -- Desktop search\n",
      "5.1.2.2. -- Semantic networks\n",
      "5.2.3.13.1. -- Deep belief networks\n",
      "5.2.3.3.3.1 -- Rule-based netwok archirtecture\n",
      "5.2.3.5.6. -- Bayesian network models\n",
      "5.2.3.5.7. -- Markov network models\n",
      "\n",
      "Gaps: \n",
      "2.1.1.3. -- Factor graphs\n",
      "2.1.1.4. -- Decision diagrams\n",
      "2.1.1.5. -- Equational models\n",
      "2.1.1.7. -- Stochastic differential equations\n",
      "2.1.1.8. -- Nonparametric representations\n",
      "\n",
      "p = 2.7367910534330084\n"
     ]
    }
   ],
   "source": [
    "gaps = lifted.L | as_list | sort\n",
    "heads = lifted.H - cluster_elements | as_list | sort \n",
    "offs = cluster_elements & lifted.H | as_list | sort \n",
    "clust =  [(i, round(j, 3)) for i, j in cluster_memberships.items()]\n",
    "\n",
    "gaps_str = gaps | concat('\\n')\n",
    "heads_str = heads | concat('\\n')\n",
    "offs_str = offs | concat('\\n')\n",
    "clust_str =  clust | concat('\\n') \n",
    "\n",
    "\n",
    "print(f'Cluster elements: \\n{clust_str}\\n\\n'\n",
    "      f'Head subjects: \\n{heads_str}\\n\\n'\n",
    "      f'Offshoots: \\n{offs_str}\\n\\n'\n",
    "      f'Gaps: \\n{gaps_str}\\n\\n'\n",
    "      f'p = {lifted.p}')\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lifter = pgfs.PGFS(tax, lmbda=0.1, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lifting_df = []\n",
    "lifted_trees = []\n",
    "\n",
    "for cluster_id in clusters.index.levels[0]:\n",
    "    \n",
    "    cluster_memberships = (\n",
    "        clusters.loc[cluster_id, 'u']\n",
    "        .loc[lambda x: x > 0.1] # filter noise\n",
    "        .pipe(lambda x: x / np.sqrt((x ** 2).sum())) # normalize\n",
    "        .to_dict()\n",
    "    )\n",
    "    cluster_elements = set(list(cluster_memberships.keys()))\n",
    "\n",
    "    lifted = lifter.lift(cluster_memberships)\n",
    "\n",
    "    gaps = lifted.L | as_list | sort\n",
    "    heads = lifted.H - cluster_elements | as_list | sort \n",
    "    offs = cluster_elements & lifted.H | as_list | sort \n",
    "    clust =  [(i, round(j, 3)) for i, j in cluster_memberships.items()]\n",
    "\n",
    "    df = (\n",
    "        pd.concat((\n",
    "            pd.Series(clust, name='topics_and_memberships'), \n",
    "            pd.Series(heads, name='head_subjects'), \n",
    "            pd.Series(gaps, name='gaps'), \n",
    "            pd.Series(offs, name='offshoots')), axis=1)\n",
    "        .assign(cluster_id=cluster_id)\n",
    "        .set_index('cluster_id', append=True)\n",
    "        .reorder_levels([1,0])\n",
    "    )\n",
    "\n",
    "    lifting_df.append(df)\n",
    "    lifted_trees.append(lifted)\n",
    "\n",
    "lifting_df = pd.concat(lifting_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "lifting_df.to_excel('data/lifting_results.xlsx', index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/lifted_trees.pkl']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(lifted_trees, 'data/lifted_trees.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
